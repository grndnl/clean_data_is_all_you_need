{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pymupdf #Required to run the code below\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg2xTicgONXB",
        "outputId": "fc2fb20f-11c0-4fc8-8573-a487a62c1cc6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.23.6-cp310-none-manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.23.6 (from pymupdf)\n",
            "  Downloading PyMuPDFb-1.23.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.23.6 pymupdf-1.23.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUi2H3FbXLc8",
        "outputId": "1e68e92a-9133-41ba-b819-6b397e840491"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#File Location and name\n",
        "\n",
        "file_name = '1603.09631.pdf'\n",
        "file_base = file_name.replace('.pdf', '')\n",
        "pdf_file_path = '/content/drive/MyDrive/W210/Extracting_Text/' + file_name  #Where PDF is Located\n",
        "Mask_csv_path = '/content/drive/MyDrive/W210/Extracting_Text/' #Where the csv mask output from the DLA is located\n"
      ],
      "metadata": {
        "id": "9FUv1WD8483a"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#this is to generate the combined df\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "file_pattern = Mask_csv_path + file_base + '*_mask_mask_summary.csv'\n",
        "print(file_pattern)\n",
        "\n",
        "# Use glob to find all files that match the pattern\n",
        "csv_files = glob.glob(file_pattern)\n",
        "\n",
        "# Initialize an empty DataFrame to concatenate all the individual DataFrames\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "\n",
        "\n",
        "# Loop through each file path and read the CSV file into a DataFrame\n",
        "for file in csv_files:\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Extract specific columns\n",
        "    coordinates_df = df[['x0f', 'x1f', 'y0f', 'y1f','mask_id','mask_shape','category_lbl']].copy()\n",
        "\n",
        "    # Extract the page number from the file name\n",
        "    # Assuming the format is always like '..._page_XXXX_...'\n",
        "    page_number = os.path.basename(file).split('_')[2]\n",
        "\n",
        "    # Add a new column for the page number\n",
        "    coordinates_df['page_number'] = int(page_number)\n",
        "\n",
        "    # Concatenate the current DataFrame with the combined DataFrame\n",
        "    combined_df = pd.concat([combined_df, coordinates_df], ignore_index=True)\n",
        "\n",
        "# Display the combined data\n",
        "print(combined_df)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxfOvJb4QIoQ",
        "outputId": "418e39f7-cd83-4003-ba32-7e1658f0cd33"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/W210/Extracting_Text/1603.09631*_mask_mask_summary.csv\n",
            "       x0f    x1f    y0f    y1f  mask_id  mask_shape category_lbl  page_number\n",
            "0     47.0  528.0   36.0   55.0        0  (819, 579)      unknown            3\n",
            "1     61.0  267.0   65.0  270.0        1  (819, 579)         list            3\n",
            "2     63.0  261.0   68.0  109.0        2  (819, 579)         text            3\n",
            "3     62.0  264.0  106.0  136.0        3  (819, 579)         text            3\n",
            "4     62.0  262.0  182.0  211.0        4  (819, 579)         text            3\n",
            "..     ...    ...    ...    ...      ...         ...          ...          ...\n",
            "98   294.0  432.0  555.0  573.0       19  (819, 579)        title            2\n",
            "99   294.0  529.0  571.0  636.0       20  (819, 579)         text            2\n",
            "100  293.0  425.0  641.0  659.0       21  (819, 579)        title            2\n",
            "101  294.0  527.0  658.0  744.0       22  (819, 579)         text            2\n",
            "102  305.0  434.0  757.0  772.0       23  (819, 579)         text            2\n",
            "\n",
            "[103 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#page_data = combined_df[combined_df['page_number'] == 1]\n",
        "#print(page_data)"
      ],
      "metadata": {
        "id": "38akJdNCTBGC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import fitz  # PyMuPDF\n",
        "import os\n",
        "import textwrap\n",
        "\n",
        "\n",
        "def extract_text_from_scaled_pdf(pdf_path, page_number, coords, new_dimensions):\n",
        "    \"\"\"Extract text from specified coordinates in a scaled PDF.\"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    scaled_doc = fitz.open()  # Create a new empty PDF for scaled pages\n",
        "\n",
        "    # Scale the specific page\n",
        "    page = doc.load_page(page_number)\n",
        "    new_page = scaled_doc.new_page(width=int(new_dimensions[1]), height=int(new_dimensions[0]))\n",
        "    new_page.show_pdf_page(new_page.rect, doc, page.number)\n",
        "\n",
        "    # Extract text from the scaled page\n",
        "    scaled_page = scaled_doc.load_page(0)  # As we have only one page in scaled_doc\n",
        "    extracted_text = scaled_page.get_text(\"text\", clip=fitz.Rect(coords))\n",
        "    #extracted_text = ' '.join(extracted_text.split())\n",
        "\n",
        "\n",
        "\n",
        "    # Clean up\n",
        "    doc.close()\n",
        "    scaled_doc.close()\n",
        "    return extracted_text\n",
        "\n",
        "# Path to your CSV and PDF files\n",
        "\n",
        "pdf_file_path = pdf_file_path\n",
        "# Extract the base name of the PDF file (e.g., '1909.11687' from '1909.11687.pdf')\n",
        "document_name = os.path.splitext(os.path.basename(pdf_file_path))[0]\n",
        "\n",
        "# Reading the CSV file\n",
        "df = combined_df\n",
        "df_sorted = df.sort_values(by=[df.columns[7], df.columns[4]])\n",
        "print(df_sorted)\n",
        "\n",
        "# Initialize an empty string to concatenate text\n",
        "concatenated_text = \"\"\n",
        "\n",
        "\n",
        "# Extract text from each row's coordinates\n",
        "for index, row in df_sorted.iterrows():\n",
        "    coords = (row['x0f'], row['y0f'], row['x1f'], row['y1f'])\n",
        "    page_number = int(row['page_number']) - 1  # Convert to zero-based index for PDF pages\n",
        "    mask_shape = row['mask_shape']\n",
        "    category = row['category_lbl']\n",
        "    numbers = mask_shape.strip(\"()\").split(\", \")\n",
        "    extracted_text = extract_text_from_scaled_pdf(pdf_file_path, page_number, coords, numbers)\n",
        "\n",
        "    # Split the extracted text into lines\n",
        "    lines = extracted_text.split('\\n')\n",
        "\n",
        "    # Process each line\n",
        "    processed_lines = []\n",
        "    for line in lines:\n",
        "        # Check if the line ends with a hyphen\n",
        "        #if line.endswith('-  '):\n",
        "        #    processed_lines.append(line[:-3])\n",
        "        #if line.endswith('- '):\n",
        "            # Remove the hyphen and do not add a space after this line\n",
        "        #    processed_lines.append(line[:-2])\n",
        "        if line.endswith('-'):\n",
        "            processed_lines.append(line[:-1])\n",
        "        else:\n",
        "            # Add a space at the end of the line for normal concatenation\n",
        "            processed_lines.append(line)\n",
        "    print(f\"Extracted Text from Page {page_number+1}, Line {index}:\", processed_lines)\n",
        "    # Join all lines into a single line\n",
        "    single_line_text = ''.join(processed_lines).strip()\n",
        "    single_line_text = textwrap.fill(single_line_text, width=80)\n",
        "\n",
        "    # Format the text for titles\n",
        "    if category == 'title':\n",
        "        single_line_text = \"\\n## \" + single_line_text + \" ##\\n\"\n",
        "\n",
        "    concatenated_text += \"\\n\" + single_line_text\n",
        "\n",
        "\n",
        "# Print the concatenated text\n",
        "print(\"\\nConcatenated Text:\\n\", concatenated_text)\n",
        "\n",
        "# Save the concatenated text into a .txt file\n",
        "output_file_path = f'/content/drive/MyDrive/W210/Extracting_Text/{document_name}_extracted_text.txt'\n",
        "with open(output_file_path, 'w') as file:\n",
        "    file.write(concatenated_text)\n",
        "\n",
        "print(f\"Concatenated text saved to {output_file_path}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X09RQoh1dP1q",
        "outputId": "7a038277-ecb4-48ea-e18a-ad31c41b9ce0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      x0f    x1f    y0f    y1f  mask_id  mask_shape category_lbl  page_number\n",
            "32  108.0  472.0   80.0  105.0        0  (819, 579)        title            1\n",
            "33  206.0  372.0  110.0  129.0        1  (819, 579)         text            1\n",
            "34  158.0  419.0  122.0  176.0        2  (819, 579)         text            1\n",
            "35  266.0  309.0  185.0  201.0        3  (819, 579)        title            1\n",
            "36   48.0  529.0  198.0  255.0        4  (819, 579)         text            1\n",
            "..    ...    ...    ...    ...      ...         ...          ...          ...\n",
            "78  293.0  530.0  682.0  718.0       23  (819, 579)         text            4\n",
            "51   47.0  528.0   36.0   55.0        0  (819, 579)      unknown            5\n",
            "52   46.0  286.0   64.0  282.0        1  (819, 579)         text            5\n",
            "53   48.0  280.0  173.0  197.0        2  (819, 579)         text            5\n",
            "54   48.0  282.0  195.0  212.0        3  (819, 579)         text            5\n",
            "\n",
            "[103 rows x 8 columns]\n",
            "Extracted Text from Page 1, Line 32: ['Data Collection for Interactive Learning through the Dialog', '']\n",
            "Extracted Text from Page 1, Line 33: ['Miroslav Vodol´an, Filip Jurˇc´ıˇcek', 'i', 'i P', 'F', 'l', 'f M h', 'i', '']\n",
            "Extracted Text from Page 1, Line 34: ['Miroslav Vodolan, Filip Jurcıcek', 'Charles University in Prague, Faculty of Mathematics and Physics', 'Institute of Formal and Applied Linguistics', 'Malostransk´e n´amˇest´ı 25, 11800 Praha 1, Czech Republic', '{vodolan, jurcicek}@ufal.mff.cuni.cz', '']\n",
            "Extracted Text from Page 1, Line 35: ['Abstract', '']\n",
            "Extracted Text from Page 1, Line 36: ['This paper presents a dataset collected from natural dialogs which enables to test the ability of dialog systems to learn new facts from', 'user utterances throughout the dialog. This interactive learning will help with one of the most prevailing problems of open domain', 'dialog system, which is the sparsity of facts a dialog system can reason about. The proposed dataset, consisting of 1900 collected', 'dialogs, allows simulation of an interactive gaining of denotations and questions explanations from users which can be used for the', 'interactive learning.', '']\n",
            "Extracted Text from Page 1, Line 37: ['15 May 2016', '']\n",
            "Extracted Text from Page 1, Line 38: ['Keywords: dataset, data collection, dialog, knowledge graph, interactive learning', '']\n",
            "Extracted Text from Page 1, Line 39: ['1.', 'Introduction', '']\n",
            "Extracted Text from Page 1, Line 40: ['Nowadays, dialog systems are usually designed for a single', 'domain (Mrksic et al., 2015). They store data in a well', 'deﬁned format with a ﬁxed number of attributes for entities', 'that the system can provide. Because data in this format', 'can be stored as a two-dimensional table within a relational', 'database, we call the data ﬂat. This data representation al', 'lows the system to query the database in a simple and ef', 'ﬁcient way. It also allows to keep the dialog state in the', 'form of slots (which usually correspond to columns in the', 'table) and track it through the dialog using probabilistic be', 'lief tracking (Williams et al., 2013; Henderson et al., 2014).', 'However, the well-deﬁned structure of the database of a', 'typical dialog system comes with a high cost of extending it', 'as every piece of new information has to ﬁt the format. This', 'is especially a problem when we one is adapting the system', 'for a new domain because its entities could have different', 'attributes.', '']\n",
            "Extracted Text from Page 1, Line 41: ['arXiv:1603.09631v2  ', '']\n",
            "Extracted Text from Page 1, Line 42: ['A dialog system based on knowledge bases offers many ad', 'vantages. First, the knowledge base, which can be repre', 'sented as knowledge graph containing entities connected by', 'relations, is much more ﬂexible than the relational database.', 'Second, freely available knowledge bases, such as Free', 'base, Wikidata, etc. contain an enormous amount of struc', 'tured information, and are still growing. A dialog system', 'which is capable of working with this type of information', 'would be therefore very useful.', '']\n",
            "Extracted Text from Page 1, Line 43: ['y', 'In this paper we propose a dataset aiming to help develop', 'and evaluate dialog systems based on knowledge bases by', 'interactive learning motivated in Section 2. Section 3. de', 'scribes policies that can be used for retrieving information', 'from knowledge bases. In Section 4. is introduced a di', 'alog simulation from natural conversations which we use', 'for evaluation of interactive learning. The dataset collec', 'tion process allowing the dialog simulation is described in', 'Section 5. and is followed by properties of the resulting', 'dataset in Section 6. Evaluation guidelines with proposed', 'metrics can be found in Section 7. The planned future work', 'is summarized in Section 8. We conclude the paper with', 'Section 9.', '']\n",
            "Extracted Text from Page 1, Line 44: ['2.', 'Motivation', '']\n",
            "Extracted Text from Page 1, Line 45: ['From the point of view of dialog systems providing gen', 'eral information from a knowledge base, the most limiting', 'factor is that a large portion of the questions is understood', 'poorly.', '']\n",
            "Extracted Text from Page 1, Line 46: ['poorly.', 'Current approaches (Berant and Liang, 2015; Bordes et', 'al., 2014) can only achieve around 50% accuracy on some', 'question answering datasets. Therefore, we think that there', 'is a room for improvements which can be achieved by inter', 'actively asking for additional information in conversational', 'dialogs with users. This extra information can be used for', 'improving policies of dialog systems. We call this approach', 'the interactive learning from dialogs.', '']\n",
            "Extracted Text from Page 1, Line 47: ['the interactive learning from dialogs.', 'We can improve dialog systems in several aspects through', 'interactive learning in a direct interaction with users. First,', 'the most straightforward way obviously is getting the cor', 'rect answer for questions that the system does not know.', 'We can try to ask users for answers on questions that the', 'system encountered in a conversation with a different user', 'and did not understand it. Second, the system can ask the', 'user for a broader explanation of a question. This expla', 'nation could help the system to understand the question', 'and provide the correct answer. In addition, the system can', 'learn correct policy for the question which allows providing', 'answers without asking any extra information for similar', 'questions next time. We hypothesize that users are will', 'ing to give such explanations because it could help them', 'to ﬁnd answers for their own questions. The last source', 'of information that we consider for interactive learning is', 'rephrasing, which could help when the system does know', 'the concept but does not know the correct wording. This', 'area is extensively studied for the purposes of information', 'retrieval (Imielinski, 2009; France et al., 2003).', 'Th', 'i', 'f h', 'll', 'd d', 'i', 'bl i', '']\n",
            "Extracted Text from Page 1, Line 48: ['retrieval (Imielinski, 2009; France et al., 2003).', 'The main purpose of the collected dataset is to enable in', 'teractive learning using the steps proposed above and po', 'tentially to evaluate how different systems perform on this', 'task.', '']\n",
            "Extracted Text from Page 1, Line 49: ['3.', 'Dialog policies', '']\n",
            "Extracted Text from Page 1, Line 50: ['The obvious difﬁculty when developing a dialog system is', 'ﬁnding a way how to identify the piece of information that', '']\n",
            "Extracted Text from Page 2, Line 79: ['Accepted as a workshop paper at RE-WOCHAT 2016', '']\n",
            "Extracted Text from Page 2, Line 80: ['the user is interested in. This is especially a problem for di', 'alog systems based on knowledge graphs containing a large', 'amount of complex structured information. While a similar', 'problem is being solved in a task of question answering, di', 'alog systems have more possibilities of identifying the real', 'intention of the user. For example, a dialog system can ask', 'for additional information during the dialog.', '']\n",
            "Extracted Text from Page 2, Line 81: ['for additional information during the dialog.', 'We distinguish three different basic approaches to request', 'ing knowledge bases:', '']\n",
            "Extracted Text from Page 2, Line 82: ['ing knowledge bases:', 'handcrafted policy – the policy consists of ﬁxed set of', 'rules implemented by system developers,', 'ofﬂine policy – the policy is learned from some kind of', 'ofﬂine training data (usually annotated) without inter', 'action with system users (Bordes et al., 2015),', 'interactively learned policy – the system learns the pol', 'icy through the dialog from its users by interactively', 'asking them for additional information.', '']\n",
            "Extracted Text from Page 2, Line 83: ['handcrafted policy – the policy consists of ﬁxed set of', 'rules implemented by system developers,', '']\n",
            "Extracted Text from Page 2, Line 84: ['A combination of the above approaches is also possible.', 'For example, we can imagine scenarios where the dia', 'log system starts with hand-crafted rules, which are sub', 'sequently interactively improved through dialogs with its', 'users.', 'With a growing demand for open domain dialog', 'systems, it shows that creating hand-crafted policies does', 'not scale well - therefore, machine learning approaches are', 'gaining on popularity.', 'Many public datasets for ofﬂine', 'learning have been published (Berant et al., 2013; Bor', 'des et al., 2015). However, to our knowledge, no public', 'datasets for interactive learning are available. To ﬁll this', 'gap, we collected a dataset which enables to train interac', 'tively learned policies through a simulated interaction with', 'users.', '']\n",
            "Extracted Text from Page 2, Line 85: ['4.', 'Dialog Simulation', '']\n",
            "Extracted Text from Page 2, Line 86: ['Ofﬂine evaluation of interactive dialogs on real data is difﬁ', 'cult because different policies can lead to different variants', 'of the dialog. Our solution to this issue is to collect data in', 'a way that allows us to simulate all dialog variants possible', 'according to any policy.', 'Th', 'di l', 'i', 'id i', 'f', 'i', 'i', '']\n",
            "Extracted Text from Page 2, Line 87: ['according to any policy.', 'The dialog variants we are considering for interactive', 'learning differ only in presence of several parts of the di', 'alog. Therefore, we can collect dialogs containing all in', 'formation used for interactive learning and omit those parts', 'that were not requested by the policy.', 'W', 'll', 'd h d', '(', 'S', 'i', '5 ) h', 'bl', 'i', '']\n",
            "Extracted Text from Page 2, Line 88: ['that were not requested by the policy.', 'We collected the dataset (see Section 5.) that enables sim', 'ulation where the policy can decide how much extra infor', 'mation to the question it requests. If the question is clear', 'to the system it can attempt to answer the question without', 'any other information. It can also ask for a broader explana', 'tion with a possibility to answer the question afterwards. If', 'the system decides not to answer the question, we can sim', 'ulate rerouting the question to another user, to try to obtain', 'the answer from them. The principle of simulated user’s', 'answer is shown in the Figure 1.', '']\n",
            "Extracted Text from Page 2, Line 89: ['answer is shown in the Figure 1.', 'Note that the simulated user’s answer can be incorrect be', 'cause human users naturally made mistakes. We intention', 'ally keep these mistakes in the dataset because real systems', 'must address them as well.', '']\n",
            "Extracted Text from Page 2, Line 90: ['1', '//', '']\n",
            "Extracted Text from Page 2, Line 91: ['Dialog 2', '...', 'U: How are you?', 'S : I have trouble with ', '      Question1', 'U: That is Answer1', 'Dialog 1', '...', 'U: Question1', 'S : I don’t know', 'Dialog 3', '...', 'U: Question1', 'S : Answer1', '']\n",
            "Extracted Text from Page 2, Line 92: ['Figure 1: Unknown questions can be rerouted between', 'users.', 'We can, for example, use chitchat to get correct', 'answers.', 'The challenge is in generalizing the collected', 'question-answer pairs using the knowledge base in order', 'to apply them to previously unseen questions.', '']\n",
            "Extracted Text from Page 2, Line 93: ['5.', 'Dataset Collection Process', '']\n",
            "Extracted Text from Page 2, Line 94: ['A perfect data collection scenario for our dataset would use', 'real running dialog system providing general information', 'from the knowledge base to real users. This system could', 'then ask for explanations and answers for questions which', 'it is not able to answer.', '']\n",
            "Extracted Text from Page 2, Line 95: ['it is not able to answer.', 'However, getting access to systems with real users is usu', 'ally hard. Therefore, we used the crowdsourcing platform', 'CrowdFlower1 (CF) for our data collection.', '']\n",
            "Extracted Text from Page 2, Line 96: ['(', ')', 'A CF worker gets a task instructing them to use our chat', 'like interface to help the system with a question which', 'is randomly selected from training examples of Simple', 'questions (Bordes et al., 2015) dataset. To complete the', 'task user has to communicate with the system through the', 'three phase dialog discussing question paraphrase (see Sec', 'tion 5.1.), explanation (see Section 5.2.) and answer of the', 'question (see Section 5.3.). To avoid poor English level', 'of dialogs we involved CF workers from English speaking', 'countries only. The collected dialogs has been annotated', '(see Section 5.4.) by expert annotators afterwards.', '']\n",
            "Extracted Text from Page 2, Line 97: ['(see Section 5.4.) by expert annotators afterwards.', 'The described procedure leads to dialogs like the one shown', 'in the Figure 2.', '']\n",
            "Extracted Text from Page 2, Line 98: ['5.1.', 'Question Paraphrasing', '']\n",
            "Extracted Text from Page 2, Line 99: ['At beginning of the dialog, the system is requesting the user', 'to paraphrase question that the system does not understand.', 'The main goal of this ﬁrst phase is to let the user get familiar', 'with the presented question and to get alternative wordings', 'of the posed question.', '']\n",
            "Extracted Text from Page 2, Line 100: ['5.2.', 'Question Explanation', '']\n",
            "Extracted Text from Page 2, Line 101: ['In the second phase, the user is asked for an explanation', 'of the question. We expect the explanation to be different', 'enough from the original question (in terms of the number', 'of common words between the question and the explana', 'tion). If the explanation is too similar to the question, the', 'user is notiﬁed that their explanation is not broad enough', 'and they must provide a better one.', '']\n",
            "Extracted Text from Page 2, Line 102: ['1http://crowdflower.com', '']\n",
            "Extracted Text from Page 3, Line 0: ['Accepted as a workshop paper at RE-WOCHAT 2016', '']\n",
            "Extracted Text from Page 3, Line 1: ['S1: Hello, I need help with this question: ', \"'what country is pilar mateos from?'. \", 'Can you put this question in a different way?', 'U1: We are wondering what is the nationality ', 'of a specific person.', 'S2: This seems like an interesting question. ', 'Could you explain in detail what it is really ', 'asking for?', 'U2: The nationality is the origine, the country ', 'where someone is born.', 'S3: It sounds reasonable, however, I still ', 'cannot find the answer. Could you give me ', 'the correct answer for the question please?', 'U3: The answer is Spain, Pilar Mateos is', '']\n",
            "Extracted Text from Page 3, Line 2: ['S1: Hello, I need help with this question: ', \"'what country is pilar mateos from?'. \", 'Can you put this question in a different way?', '']\n",
            "Extracted Text from Page 3, Line 3: ['y', 'p', 'q', 'y', 'U1: We are wondering what is the nationality ', 'of a specific person.', '']\n",
            "Extracted Text from Page 3, Line 4: ['g', 'U2: The nationality is the origine, the country ', 'where someone is born.', '']\n",
            "Extracted Text from Page 3, Line 5: ['S3: It sounds reasonable, however, I still ', 'cannot find the answer. Could you give me ', 'the correct answer for the question please?', '']\n",
            "Extracted Text from Page 3, Line 6: ['q', 'p', 'U3: The answer is Spain, Pilar Mateos is ', 'Spanish.', '']\n",
            "Extracted Text from Page 3, Line 7: ['Figure 2: An Example of a short dialog collected on the', 'crowdsourcing platform. We can see that the user provides', 'the question paraphrase (S1), the explanation (S2) and the', 'correct answer for the question (S3).', '']\n",
            "Extracted Text from Page 3, Line 8: ['5.3.', 'Question Answer', '']\n",
            "Extracted Text from Page 3, Line 9: ['5.3.', 'Question Answer', 'With the valid explanation the dialog turns into the last', 'phase where the user is asked for a correct answer to the', 'original question.', 'The system requires the user to an', 'swer with a full sentence. In practical experiments this has', 'shown as a useful decision because it improves system’s', 'ability to reveal cheaters. We can simply measure the con', 'nection (in terms of common words ) between question and', 'the answer sentence. This allows to reject completely irrel', 'evant answers.', '']\n",
            "Extracted Text from Page 3, Line 10: ['5.4.', 'Annotation', '']\n",
            "Extracted Text from Page 3, Line 11: ['The correct answer for question in each dialog is avail', 'able from Simple questions dataset. Answers are in form', 'of Freebase2 entities identiﬁed by unique id. For evalua', 'tion purposes we need information whether dialog contains', 'the answer which is consistent with the entity from Simple', 'questions, the answer with another entity or whether the', 'dialog does not contain any answer. While the annotation', 'process is quite simple, we did not need crowdsourcing for', 'the process.', '']\n",
            "Extracted Text from Page 3, Line 12: ['5.5.', 'Natural Language Understanding (NLU)', '']\n",
            "Extracted Text from Page 3, Line 13: ['g', 'g', 'g (', ')', 'The collection system needs to recognize following dialog', 'acts from user utterances during all phases of the dialog:', '']\n",
            "Extracted Text from Page 3, Line 14: ['Negate – user does not want to provide requested informa', 'tion,', '']\n",
            "Extracted Text from Page 3, Line 15: ['Afﬁrm – user agrees to provide requested information,', '']\n",
            "Extracted Text from Page 3, Line 16: ['DontKnow – user does not know the requested informa', 'tion,', '']\n",
            "Extracted Text from Page 3, Line 17: ['2https://www.freebase.com/', 'l', 'i', '']\n",
            "Extracted Text from Page 3, Line 18: ['ChitChat – user tries chit chat with the system (hello, bye,', 'who are you...),', '']\n",
            "Extracted Text from Page 3, Line 19: ['Inform – none of the above, interpreted as user is giving', 'information requested by the system.', '']\n",
            "Extracted Text from Page 3, Line 20: ['Parsing of the dialog acts is made by hand written rules', 'using templates and keyword spotting. The templates and', 'keywords were manually collected from frequent expres', 'sions used by CF workers during preparation runs of the', 'dataset collection process (google it, check wikipedia, I', 'would need... → Negate).', '']\n",
            "Extracted Text from Page 3, Line 21: ['6.', 'Dataset Properties', '']\n",
            "Extracted Text from Page 3, Line 22: ['p', 'We collected the dataset with 1900 dialogs and 8533 turns.', 'Topics discussed in dialogs are questions randomly chosen', 'from training examples of Simple questions (Bordes et al.,', '2015) dataset. From this dataset we also took the correct', 'answers in form of Freebase entities.', '']\n",
            "Extracted Text from Page 3, Line 23: ['answers in form of Freebase entities.', 'Our dataset consists of standard data split into training, de', 'velopment and test ﬁles. The basic properties of those ﬁles', 'are as follows:', '']\n",
            "Extracted Text from Page 3, Line 24: ['dialog count dialog turns', '']\n",
            "Extracted Text from Page 3, Line 25: ['dialog count dialog turns', 'Training dialogs', '950', '4249', 'Development dialogs', '285', '1258', 'Testing dialogs', '665', '3026', '']\n",
            "Extracted Text from Page 3, Line 26: ['Table 1: Table of turn and dialog counts for dataset splits.', '']\n",
            "Extracted Text from Page 3, Line 27: ['Each ﬁle contains complete dialogs enriched by outputs of', 'NLU (see Section 5.5.) that were used during the data col', 'lection. On top of that, each dialog is labeled by the correct', 'answer for the question and expert annotation of the user', 'answer hint which tells whether the hint points to the cor', 'rect answer, incorrect answer, or no answer at all.', '351 f ll', 'll', 'd di l', 'i', 'id d', '']\n",
            "Extracted Text from Page 3, Line 28: ['rect answer, incorrect answer, or no answer at all.', '351 of all collected dialogs contain correct answer provided', 'by users and 702 dialogs have incorrect answer. In the re', 'maining 847 dialogs users did not want to answer the ques', 'tion. The collected dialogs also contain 1828 paraphrases', 'and 1539 explanations for 1870 questions.', 'A', 'f', 'i', 'l b l d', 'b', '']\n",
            "Extracted Text from Page 3, Line 29: ['p', 'q', 'An answer for a question was labeled as correct by anno', 'tators only when it was evident to them that the answer', 'points to the same Freebase entity that was present in Sim', 'ple questions dataset for that particular question. However,', 'a large amount of questions from that dataset is quite gen', 'eral - with many possible answers. Therefore lot of answers', 'from users were labeled as incorrect even though those an', 'swers perfectly ﬁt the question. Our annotators identiﬁed', 'that 285 of the incorrect answers were answers for such', 'general questions. Example of this situation can be demon', 'strated by question ’Name an actor’ which was correctly', 'answered by ’Brad Pitt is an actor’, however, to be con', 'sistent with Simple questions annotation, which is ’Kelly', 'Atwood’, annotators were forced to mark it as an incorrect', 'answer.', '']\n",
            "Extracted Text from Page 3, Line 30: ['7.', 'Interactive Learning Evaluation', '']\n",
            "Extracted Text from Page 3, Line 31: ['g', 'A perfect interactive learning model would be able to', 'learn anything interactively from test dialogs during test', 'ing, which would allow us to measure progress of the model', '']\n",
            "Extracted Text from Page 4, Line 55: ['Accepted as a workshop paper at RE-WOCHAT 2016', '']\n",
            "Extracted Text from Page 4, Line 56: ['from scratch over the course of time. However, a develop', 'ment of such model would be unnecessarily hard, there', 'fore we provide training dialogs which can be used for fea', 'ture extraction and other engineering related to interactive', 'learning from dialogs in natural language. Model develop', 'ment is further supported with labeled validation data for', 'parameter tuning.', '']\n",
            "Extracted Text from Page 4, Line 57: ['parameter tuning.', 'We propose two evaluation metrics for comparing interac', 'tive learning models. First metric (see Section 7.1.) scores', 'amount of information required by the model, second met', 'ric (see Section 7.2.) is accuracy of answer extraction from', 'user utterances. All models must base their answers only', 'on information gained from training dialogs and testing di', 'alogs seen during the simulation so far, to ensure that the', 'score will reﬂect the interactive learning of the model in', 'stead of general question answering.', '']\n",
            "Extracted Text from Page 4, Line 58: ['7.1.', 'Efﬁciency Score', '']\n",
            "Extracted Text from Page 4, Line 59: ['y', 'The simulation of dialogs from our dataset allows to eval', 'uate how efﬁcient a dialog system is in using information', 'gained from users. The dialog system should maximize the', 'number of correctly answered questions without requesting', 'too many explanations and answers from users. To evaluate', 'different systems using the collected data, we propose the', 'following evaluation measure:', '']\n",
            "Extracted Text from Page 4, Line 60: ['SD = nc − wini − wene − wana', '|D|', '(1)', 'i', 'F', 'v', '']\n",
            "Extracted Text from Page 4, Line 61: ['Here, nc denotes the number of correctly answered ques', 'tions, ni denotes the number of incorrectly answered ques', 'tions, ne denotes the number of requested explanations, na', 'denotes the number of requested answers and |D| denotes', 'the number of simulated dialogs in the dataset. wi, we, wa', 'are penalization weights.', 'h', 'li', 'i', 'i h', 'd', 'f', 'diff', '']\n",
            "Extracted Text from Page 4, Line 62: ['are penalization weights.', 'The penalization weights are used to compensate for differ', 'ent costs of obtaining different types of information from', 'the user. For example, gaining broader explanation from', 'the user is relatively simple because it is in their favor to', 'cooperate with the system on a question they are interested', 'in. However, obtaining correct answers from users is sig', 'niﬁcantly more difﬁcult because the system does not always', 'have the chance to ask the question and the user does not', 'have to know the correct answer for it.', '']\n",
            "Extracted Text from Page 4, Line 63: ['have to know the correct answer for it.', 'To make the evaluations comparable between different sys', 'tems we recommend using our evaluation scripts included', 'with the dataset with following penalization weights that', 'reﬂect our intuition for gaining information from users:', '']\n",
            "Extracted Text from Page 4, Line 64: ['• wi = 5 – incorrect answers are penalized signiﬁcantly,', '• we = 0.2 – explanations are quite cheap; therefore, we', 'will penalize them just slightly,', '• wa = 1 – gaining question’s answer from users is harder', 'than gaining explanations.', '']\n",
            "Extracted Text from Page 4, Line 65: ['7.2.', 'Answer Extraction Accuracy', '']\n",
            "Extracted Text from Page 4, Line 66: ['y', 'It is quite challenging to ﬁnd appropriate entity in the', 'knowledge base even though the user provided the correct', 'answer. Therefore, we propose another metric relevant to', 'our dataset. This metric is the accuracy of entity extraction', '']\n",
            "Extracted Text from Page 4, Line 67: ['o', 'n', '3hdl.handle.net/11234/1-1670', '']\n",
            "Extracted Text from Page 4, Line 68: ['which measures how many times was extracted a correct', 'answer from answer hints provided by the user in dialogs', 'annotated as correctly answered.', '']\n",
            "Extracted Text from Page 4, Line 69: ['8.', 'Future Work', '']\n",
            "Extracted Text from Page 4, Line 70: ['Our future work will be mainly focused on providing a', 'baseline system for interactive learning which will be eval', 'uated on the dataset. We are also planning improvements', 'for dialog management that is used to gain explanations', 'during the data collection. We believe that with conver', 'sation about speciﬁc aspects of the discussed question it', 'will be possible to gain even more interesting information', 'from users. The other area of our interest is in possibilities', 'to improve question answering accuracy on test questions', 'of Simple question dataset with the extra information con', 'tained in the collected dialogs.', '']\n",
            "Extracted Text from Page 4, Line 71: ['9.', 'Conclusion', '']\n",
            "Extracted Text from Page 4, Line 72: ['In this paper, we presented a novel way how to evaluate', 'different interactive learning approaches for dialog models.', 'The evaluation covers two challenging aspects of interac', 'tive learning. First, it scores efﬁciency of using information', 'gained from users in simulated question answering dialogs.', 'Second, it measures accuracy on answer hints understand', 'ing.', '']\n",
            "Extracted Text from Page 4, Line 73: ['For purposes of evaluation we collected a dataset from con', 'versational dialogs with workers on crowdsourcing plat', 'form CrowdFlower. Those dialogs were annotated with ex', 'pert annotators and published under Creative Commons 4.0', 'BY-SA license on lindat3.', 'We also provide evaluation', 'scripts with the dataset that should ensure comparable eval', 'uation of different interactive learning approaches.', '']\n",
            "Extracted Text from Page 4, Line 74: ['10.', 'Acknowledgments', '']\n",
            "Extracted Text from Page 4, Line 75: ['This work was funded by the Ministry of Education, Youth', 'and Sports of the Czech Republic under the grant agree', 'ment LK11221 and core research funding, SVV project 260', '224, and GAUK grant 1170516 of Charles University in', 'Prague. It used language resources stored and distributed', 'by the LINDAT/CLARIN project of the Ministry of Edu', 'cation, Youth and Sports of the Czech Republic (project', 'LM2015071).', '']\n",
            "Extracted Text from Page 4, Line 76: ['11.', 'Bibliographical References', '']\n",
            "Extracted Text from Page 4, Line 77: ['Berant, J. and Liang, P. (2015). Imitation learning of', 'agenda-based semantic parsers. Transactions of the As', 'sociation for Computational Linguistics (TACL), 3:545–', '558.', 'Berant, J., Chou, A., Frostig, R., and Liang, P. (2013).', 'Semantic Parsing on Freebase from Question-Answer', 'Pairs. Proceedings of EMNLP, (October):1533–1544.', 'Bordes, A., Chopra, S., and Weston, J. (2014). Question', 'Answering with Subgraph Embeddings.', 'Bordes, A., Usunier, N., Chopra, S., and Weston, J. (2015).', 'Large-scale Simple Question Answering with Memory', 'Networks.', '']\n",
            "Extracted Text from Page 4, Line 78: ['Pairs. Proceedings of EMNLP, (October):1533–1544.', 'Bordes, A., Chopra, S., and Weston, J. (2014). Question', 'Answering with Subgraph Embeddings.', 'Bordes A Usunier N Chopra S and Weston J (2015)', '']\n",
            "Extracted Text from Page 5, Line 51: ['Accepted as a workshop paper at RE-WOCHAT 2016', '']\n",
            "Extracted Text from Page 5, Line 52: ['France, F. D., Yvon, F., and Collin, O. (2003). Learn', 'ing paraphrases to improve a question-answering sys', 'tem. In In Proceedings of the 10th Conference of EACL', 'Workshop Natural Language Processing for Question', 'Answering.', 'Henderson, M., Thomson, B., and Williams, J. (2014). The', 'second dialog state tracking challenge. In 15th Annual', 'Meeting of the Special Interest Group on Discourse and', 'Dialogue, volume 263.', 'Imielinski, T. (2009). If you ask nicely , I will answer :', 'Semantic Search and Today’s Search Engines. Search.', 'Mrksic, N., S´eaghdha, D. ´O., Thomson, B., Gasic, M., Su,', 'P., Vandyke, D., Wen, T., and Young, S. J. (2015). Multi', 'domain dialog state tracking using recurrent neural net', 'works. CoRR, abs/1506.07190.', 'Williams, J., Raux, A., Ramachandran, D., and Black, A.', '(2013). The Dialog State Tracking Challenge. Sigdial,', '(August):404–413.', '']\n",
            "Extracted Text from Page 5, Line 53: ['Imielinski, T. (2009). If you ask nicely , I will answer :', 'Semantic Search and Today’s Search Engines. Search.', '']\n",
            "Extracted Text from Page 5, Line 54: ['y', 'g', 'Mrksic, N., S´eaghdha, D. ´O., Thomson, B., Gasic, M., Su,', 'P V', 'd k', 'D W', 'T', 'd Y', 'S J (2015) M l i', '']\n",
            "\n",
            "Concatenated Text:\n",
            " \n",
            "\n",
            "## Data Collection for Interactive Learning through the Dialog ##\n",
            "\n",
            "Miroslav Vodol´an, Filip Jurˇc´ıˇcekii PFlf M hi\n",
            "Miroslav Vodolan, Filip JurcıcekCharles University in Prague, Faculty of\n",
            "Mathematics and PhysicsInstitute of Formal and Applied LinguisticsMalostransk´e\n",
            "n´amˇest´ı 25, 11800 Praha 1, Czech Republic{vodolan, jurcicek}@ufal.mff.cuni.cz\n",
            "\n",
            "## Abstract ##\n",
            "\n",
            "This paper presents a dataset collected from natural dialogs which enables to\n",
            "test the ability of dialog systems to learn new facts fromuser utterances\n",
            "throughout the dialog. This interactive learning will help with one of the most\n",
            "prevailing problems of open domaindialog system, which is the sparsity of facts\n",
            "a dialog system can reason about. The proposed dataset, consisting of 1900\n",
            "collecteddialogs, allows simulation of an interactive gaining of denotations and\n",
            "questions explanations from users which can be used for theinteractive learning.\n",
            "15 May 2016\n",
            "Keywords: dataset, data collection, dialog, knowledge graph, interactive\n",
            "learning\n",
            "\n",
            "## 1.Introduction ##\n",
            "\n",
            "Nowadays, dialog systems are usually designed for a singledomain (Mrksic et al.,\n",
            "2015). They store data in a welldeﬁned format with a ﬁxed number of attributes\n",
            "for entitiesthat the system can provide. Because data in this formatcan be\n",
            "stored as a two-dimensional table within a relationaldatabase, we call the data\n",
            "ﬂat. This data representation allows the system to query the database in a\n",
            "simple and efﬁcient way. It also allows to keep the dialog state in theform of\n",
            "slots (which usually correspond to columns in thetable) and track it through the\n",
            "dialog using probabilistic belief tracking (Williams et al., 2013; Henderson et\n",
            "al., 2014).However, the well-deﬁned structure of the database of atypical dialog\n",
            "system comes with a high cost of extending itas every piece of new information\n",
            "has to ﬁt the format. Thisis especially a problem when we one is adapting the\n",
            "systemfor a new domain because its entities could have differentattributes.\n",
            "arXiv:1603.09631v2\n",
            "A dialog system based on knowledge bases offers many advantages. First, the\n",
            "knowledge base, which can be represented as knowledge graph containing entities\n",
            "connected byrelations, is much more ﬂexible than the relational database.Second,\n",
            "freely available knowledge bases, such as Freebase, Wikidata, etc. contain an\n",
            "enormous amount of structured information, and are still growing. A dialog\n",
            "systemwhich is capable of working with this type of informationwould be\n",
            "therefore very useful.\n",
            "yIn this paper we propose a dataset aiming to help developand evaluate dialog\n",
            "systems based on knowledge bases byinteractive learning motivated in Section 2.\n",
            "Section 3. describes policies that can be used for retrieving informationfrom\n",
            "knowledge bases. In Section 4. is introduced a dialog simulation from natural\n",
            "conversations which we usefor evaluation of interactive learning. The dataset\n",
            "collection process allowing the dialog simulation is described inSection 5. and\n",
            "is followed by properties of the resultingdataset in Section 6. Evaluation\n",
            "guidelines with proposedmetrics can be found in Section 7. The planned future\n",
            "workis summarized in Section 8. We conclude the paper withSection 9.\n",
            "\n",
            "## 2.Motivation ##\n",
            "\n",
            "From the point of view of dialog systems providing general information from a\n",
            "knowledge base, the most limitingfactor is that a large portion of the questions\n",
            "is understoodpoorly.\n",
            "poorly.Current approaches (Berant and Liang, 2015; Bordes etal., 2014) can only\n",
            "achieve around 50% accuracy on somequestion answering datasets. Therefore, we\n",
            "think that thereis a room for improvements which can be achieved by\n",
            "interactively asking for additional information in conversationaldialogs with\n",
            "users. This extra information can be used forimproving policies of dialog\n",
            "systems. We call this approachthe interactive learning from dialogs.\n",
            "the interactive learning from dialogs.We can improve dialog systems in several\n",
            "aspects throughinteractive learning in a direct interaction with users.\n",
            "First,the most straightforward way obviously is getting the correct answer for\n",
            "questions that the system does not know.We can try to ask users for answers on\n",
            "questions that thesystem encountered in a conversation with a different userand\n",
            "did not understand it. Second, the system can ask theuser for a broader\n",
            "explanation of a question. This explanation could help the system to understand\n",
            "the questionand provide the correct answer. In addition, the system canlearn\n",
            "correct policy for the question which allows providinganswers without asking any\n",
            "extra information for similarquestions next time. We hypothesize that users are\n",
            "willing to give such explanations because it could help themto ﬁnd answers for\n",
            "their own questions. The last sourceof information that we consider for\n",
            "interactive learning isrephrasing, which could help when the system does knowthe\n",
            "concept but does not know the correct wording. Thisarea is extensively studied\n",
            "for the purposes of informationretrieval (Imielinski, 2009; France et al.,\n",
            "2003).Thif hlld dibl i\n",
            "retrieval (Imielinski, 2009; France et al., 2003).The main purpose of the\n",
            "collected dataset is to enable interactive learning using the steps proposed\n",
            "above and potentially to evaluate how different systems perform on thistask.\n",
            "\n",
            "## 3.Dialog policies ##\n",
            "\n",
            "The obvious difﬁculty when developing a dialog system isﬁnding a way how to\n",
            "identify the piece of information that\n",
            "Accepted as a workshop paper at RE-WOCHAT 2016\n",
            "the user is interested in. This is especially a problem for dialog systems based\n",
            "on knowledge graphs containing a largeamount of complex structured information.\n",
            "While a similarproblem is being solved in a task of question answering, dialog\n",
            "systems have more possibilities of identifying the realintention of the user.\n",
            "For example, a dialog system can askfor additional information during the\n",
            "dialog.\n",
            "for additional information during the dialog.We distinguish three different\n",
            "basic approaches to requesting knowledge bases:\n",
            "ing knowledge bases:handcrafted policy – the policy consists of ﬁxed set ofrules\n",
            "implemented by system developers,ofﬂine policy – the policy is learned from some\n",
            "kind ofofﬂine training data (usually annotated) without interaction with system\n",
            "users (Bordes et al., 2015),interactively learned policy – the system learns the\n",
            "policy through the dialog from its users by interactivelyasking them for\n",
            "additional information.\n",
            "handcrafted policy – the policy consists of ﬁxed set ofrules implemented by\n",
            "system developers,\n",
            "A combination of the above approaches is also possible.For example, we can\n",
            "imagine scenarios where the dialog system starts with hand-crafted rules, which\n",
            "are subsequently interactively improved through dialogs with itsusers.With a\n",
            "growing demand for open domain dialogsystems, it shows that creating hand-\n",
            "crafted policies doesnot scale well - therefore, machine learning approaches\n",
            "aregaining on popularity.Many public datasets for ofﬂinelearning have been\n",
            "published (Berant et al., 2013; Bordes et al., 2015). However, to our knowledge,\n",
            "no publicdatasets for interactive learning are available. To ﬁll thisgap, we\n",
            "collected a dataset which enables to train interactively learned policies\n",
            "through a simulated interaction withusers.\n",
            "\n",
            "## 4.Dialog Simulation ##\n",
            "\n",
            "Ofﬂine evaluation of interactive dialogs on real data is difﬁcult because\n",
            "different policies can lead to different variantsof the dialog. Our solution to\n",
            "this issue is to collect data ina way that allows us to simulate all dialog\n",
            "variants possibleaccording to any policy.Thdi liid ifii\n",
            "according to any policy.The dialog variants we are considering for\n",
            "interactivelearning differ only in presence of several parts of the dialog.\n",
            "Therefore, we can collect dialogs containing all information used for\n",
            "interactive learning and omit those partsthat were not requested by the\n",
            "policy.Wlld h d(Si5 ) hbli\n",
            "that were not requested by the policy.We collected the dataset (see Section 5.)\n",
            "that enables simulation where the policy can decide how much extra information\n",
            "to the question it requests. If the question is clearto the system it can\n",
            "attempt to answer the question withoutany other information. It can also ask for\n",
            "a broader explanation with a possibility to answer the question afterwards.\n",
            "Ifthe system decides not to answer the question, we can simulate rerouting the\n",
            "question to another user, to try to obtainthe answer from them. The principle of\n",
            "simulated user’sanswer is shown in the Figure 1.\n",
            "answer is shown in the Figure 1.Note that the simulated user’s answer can be\n",
            "incorrect because human users naturally made mistakes. We intentionally keep\n",
            "these mistakes in the dataset because real systemsmust address them as well.\n",
            "1//\n",
            "Dialog 2...U: How are you?S : I have trouble with       Question1U: That is\n",
            "Answer1Dialog 1...U: Question1S : I don’t knowDialog 3...U: Question1S : Answer1\n",
            "Figure 1: Unknown questions can be rerouted betweenusers.We can, for example,\n",
            "use chitchat to get correctanswers.The challenge is in generalizing the\n",
            "collectedquestion-answer pairs using the knowledge base in orderto apply them to\n",
            "previously unseen questions.\n",
            "\n",
            "## 5.Dataset Collection Process ##\n",
            "\n",
            "A perfect data collection scenario for our dataset would usereal running dialog\n",
            "system providing general informationfrom the knowledge base to real users. This\n",
            "system couldthen ask for explanations and answers for questions whichit is not\n",
            "able to answer.\n",
            "it is not able to answer.However, getting access to systems with real users is\n",
            "usually hard. Therefore, we used the crowdsourcing platformCrowdFlower1 (CF) for\n",
            "our data collection.\n",
            "()A CF worker gets a task instructing them to use our chatlike interface to help\n",
            "the system with a question whichis randomly selected from training examples of\n",
            "Simplequestions (Bordes et al., 2015) dataset. To complete thetask user has to\n",
            "communicate with the system through thethree phase dialog discussing question\n",
            "paraphrase (see Section 5.1.), explanation (see Section 5.2.) and answer of\n",
            "thequestion (see Section 5.3.). To avoid poor English levelof dialogs we\n",
            "involved CF workers from English speakingcountries only. The collected dialogs\n",
            "has been annotated(see Section 5.4.) by expert annotators afterwards.\n",
            "(see Section 5.4.) by expert annotators afterwards.The described procedure leads\n",
            "to dialogs like the one shownin the Figure 2.\n",
            "\n",
            "## 5.1.Question Paraphrasing ##\n",
            "\n",
            "At beginning of the dialog, the system is requesting the userto paraphrase\n",
            "question that the system does not understand.The main goal of this ﬁrst phase is\n",
            "to let the user get familiarwith the presented question and to get alternative\n",
            "wordingsof the posed question.\n",
            "\n",
            "## 5.2.Question Explanation ##\n",
            "\n",
            "In the second phase, the user is asked for an explanationof the question. We\n",
            "expect the explanation to be differentenough from the original question (in\n",
            "terms of the numberof common words between the question and the explanation). If\n",
            "the explanation is too similar to the question, theuser is notiﬁed that their\n",
            "explanation is not broad enoughand they must provide a better one.\n",
            "1http://crowdflower.com\n",
            "Accepted as a workshop paper at RE-WOCHAT 2016\n",
            "S1: Hello, I need help with this question: 'what country is pilar mateos from?'.\n",
            "Can you put this question in a different way?U1: We are wondering what is the\n",
            "nationality of a specific person.S2: This seems like an interesting question.\n",
            "Could you explain in detail what it is really asking for?U2: The nationality is\n",
            "the origine, the country where someone is born.S3: It sounds reasonable,\n",
            "however, I still cannot find the answer. Could you give me the correct answer\n",
            "for the question please?U3: The answer is Spain, Pilar Mateos is\n",
            "S1: Hello, I need help with this question: 'what country is pilar mateos from?'.\n",
            "Can you put this question in a different way?\n",
            "ypqyU1: We are wondering what is the nationality of a specific person.\n",
            "gU2: The nationality is the origine, the country where someone is born.\n",
            "S3: It sounds reasonable, however, I still cannot find the answer. Could you\n",
            "give me the correct answer for the question please?\n",
            "qpU3: The answer is Spain, Pilar Mateos is Spanish.\n",
            "Figure 2: An Example of a short dialog collected on thecrowdsourcing platform.\n",
            "We can see that the user providesthe question paraphrase (S1), the explanation\n",
            "(S2) and thecorrect answer for the question (S3).\n",
            "\n",
            "## 5.3.Question Answer ##\n",
            "\n",
            "5.3.Question AnswerWith the valid explanation the dialog turns into the\n",
            "lastphase where the user is asked for a correct answer to theoriginal\n",
            "question.The system requires the user to answer with a full sentence. In\n",
            "practical experiments this hasshown as a useful decision because it improves\n",
            "system’sability to reveal cheaters. We can simply measure the connection (in\n",
            "terms of common words ) between question andthe answer sentence. This allows to\n",
            "reject completely irrelevant answers.\n",
            "\n",
            "## 5.4.Annotation ##\n",
            "\n",
            "The correct answer for question in each dialog is available from Simple\n",
            "questions dataset. Answers are in formof Freebase2 entities identiﬁed by unique\n",
            "id. For evaluation purposes we need information whether dialog containsthe\n",
            "answer which is consistent with the entity from Simplequestions, the answer with\n",
            "another entity or whether thedialog does not contain any answer. While the\n",
            "annotationprocess is quite simple, we did not need crowdsourcing forthe process.\n",
            "\n",
            "## 5.5.Natural Language Understanding (NLU) ##\n",
            "\n",
            "ggg ()The collection system needs to recognize following dialogacts from user\n",
            "utterances during all phases of the dialog:\n",
            "Negate – user does not want to provide requested information,\n",
            "Afﬁrm – user agrees to provide requested information,\n",
            "DontKnow – user does not know the requested information,\n",
            "2https://www.freebase.com/li\n",
            "ChitChat – user tries chit chat with the system (hello, bye,who are you...),\n",
            "Inform – none of the above, interpreted as user is givinginformation requested\n",
            "by the system.\n",
            "Parsing of the dialog acts is made by hand written rulesusing templates and\n",
            "keyword spotting. The templates andkeywords were manually collected from\n",
            "frequent expressions used by CF workers during preparation runs of thedataset\n",
            "collection process (google it, check wikipedia, Iwould need... → Negate).\n",
            "\n",
            "## 6.Dataset Properties ##\n",
            "\n",
            "pWe collected the dataset with 1900 dialogs and 8533 turns.Topics discussed in\n",
            "dialogs are questions randomly chosenfrom training examples of Simple questions\n",
            "(Bordes et al.,2015) dataset. From this dataset we also took the correctanswers\n",
            "in form of Freebase entities.\n",
            "answers in form of Freebase entities.Our dataset consists of standard data split\n",
            "into training, development and test ﬁles. The basic properties of those ﬁlesare\n",
            "as follows:\n",
            "dialog count dialog turns\n",
            "dialog count dialog turnsTraining dialogs9504249Development\n",
            "dialogs2851258Testing dialogs6653026\n",
            "Table 1: Table of turn and dialog counts for dataset splits.\n",
            "Each ﬁle contains complete dialogs enriched by outputs ofNLU (see Section 5.5.)\n",
            "that were used during the data collection. On top of that, each dialog is\n",
            "labeled by the correctanswer for the question and expert annotation of the\n",
            "useranswer hint which tells whether the hint points to the correct answer,\n",
            "incorrect answer, or no answer at all.351 f lllld di liid d\n",
            "rect answer, incorrect answer, or no answer at all.351 of all collected dialogs\n",
            "contain correct answer providedby users and 702 dialogs have incorrect answer.\n",
            "In the remaining 847 dialogs users did not want to answer the question. The\n",
            "collected dialogs also contain 1828 paraphrasesand 1539 explanations for 1870\n",
            "questions.Afil b l db\n",
            "pqAn answer for a question was labeled as correct by annotators only when it was\n",
            "evident to them that the answerpoints to the same Freebase entity that was\n",
            "present in Simple questions dataset for that particular question. However,a\n",
            "large amount of questions from that dataset is quite general - with many\n",
            "possible answers. Therefore lot of answersfrom users were labeled as incorrect\n",
            "even though those answers perfectly ﬁt the question. Our annotators\n",
            "identiﬁedthat 285 of the incorrect answers were answers for suchgeneral\n",
            "questions. Example of this situation can be demonstrated by question ’Name an\n",
            "actor’ which was correctlyanswered by ’Brad Pitt is an actor’, however, to be\n",
            "consistent with Simple questions annotation, which is ’KellyAtwood’, annotators\n",
            "were forced to mark it as an incorrectanswer.\n",
            "\n",
            "## 7.Interactive Learning Evaluation ##\n",
            "\n",
            "gA perfect interactive learning model would be able tolearn anything\n",
            "interactively from test dialogs during testing, which would allow us to measure\n",
            "progress of the model\n",
            "Accepted as a workshop paper at RE-WOCHAT 2016\n",
            "from scratch over the course of time. However, a development of such model would\n",
            "be unnecessarily hard, therefore we provide training dialogs which can be used\n",
            "for feature extraction and other engineering related to interactivelearning from\n",
            "dialogs in natural language. Model development is further supported with labeled\n",
            "validation data forparameter tuning.\n",
            "parameter tuning.We propose two evaluation metrics for comparing interactive\n",
            "learning models. First metric (see Section 7.1.) scoresamount of information\n",
            "required by the model, second metric (see Section 7.2.) is accuracy of answer\n",
            "extraction fromuser utterances. All models must base their answers onlyon\n",
            "information gained from training dialogs and testing dialogs seen during the\n",
            "simulation so far, to ensure that thescore will reﬂect the interactive learning\n",
            "of the model instead of general question answering.\n",
            "\n",
            "## 7.1.Efﬁciency Score ##\n",
            "\n",
            "yThe simulation of dialogs from our dataset allows to evaluate how efﬁcient a\n",
            "dialog system is in using informationgained from users. The dialog system should\n",
            "maximize thenumber of correctly answered questions without requestingtoo many\n",
            "explanations and answers from users. To evaluatedifferent systems using the\n",
            "collected data, we propose thefollowing evaluation measure:\n",
            "SD = nc − wini − wene − wana|D|(1)iFv\n",
            "Here, nc denotes the number of correctly answered questions, ni denotes the\n",
            "number of incorrectly answered questions, ne denotes the number of requested\n",
            "explanations, nadenotes the number of requested answers and |D| denotesthe\n",
            "number of simulated dialogs in the dataset. wi, we, waare penalization\n",
            "weights.hliii hdfdiff\n",
            "are penalization weights.The penalization weights are used to compensate for\n",
            "different costs of obtaining different types of information fromthe user. For\n",
            "example, gaining broader explanation fromthe user is relatively simple because\n",
            "it is in their favor tocooperate with the system on a question they are\n",
            "interestedin. However, obtaining correct answers from users is signiﬁcantly more\n",
            "difﬁcult because the system does not alwayshave the chance to ask the question\n",
            "and the user does nothave to know the correct answer for it.\n",
            "have to know the correct answer for it.To make the evaluations comparable\n",
            "between different systems we recommend using our evaluation scripts includedwith\n",
            "the dataset with following penalization weights thatreﬂect our intuition for\n",
            "gaining information from users:\n",
            "• wi = 5 – incorrect answers are penalized signiﬁcantly,• we = 0.2 –\n",
            "explanations are quite cheap; therefore, wewill penalize them just slightly,• wa\n",
            "= 1 – gaining question’s answer from users is harderthan gaining explanations.\n",
            "\n",
            "## 7.2.Answer Extraction Accuracy ##\n",
            "\n",
            "yIt is quite challenging to ﬁnd appropriate entity in theknowledge base even\n",
            "though the user provided the correctanswer. Therefore, we propose another metric\n",
            "relevant toour dataset. This metric is the accuracy of entity extraction\n",
            "on3hdl.handle.net/11234/1-1670\n",
            "which measures how many times was extracted a correctanswer from answer hints\n",
            "provided by the user in dialogsannotated as correctly answered.\n",
            "\n",
            "## 8.Future Work ##\n",
            "\n",
            "Our future work will be mainly focused on providing abaseline system for\n",
            "interactive learning which will be evaluated on the dataset. We are also\n",
            "planning improvementsfor dialog management that is used to gain\n",
            "explanationsduring the data collection. We believe that with conversation about\n",
            "speciﬁc aspects of the discussed question itwill be possible to gain even more\n",
            "interesting informationfrom users. The other area of our interest is in\n",
            "possibilitiesto improve question answering accuracy on test questionsof Simple\n",
            "question dataset with the extra information contained in the collected dialogs.\n",
            "\n",
            "## 9.Conclusion ##\n",
            "\n",
            "In this paper, we presented a novel way how to evaluatedifferent interactive\n",
            "learning approaches for dialog models.The evaluation covers two challenging\n",
            "aspects of interactive learning. First, it scores efﬁciency of using\n",
            "informationgained from users in simulated question answering dialogs.Second, it\n",
            "measures accuracy on answer hints understanding.\n",
            "For purposes of evaluation we collected a dataset from conversational dialogs\n",
            "with workers on crowdsourcing platform CrowdFlower. Those dialogs were annotated\n",
            "with expert annotators and published under Creative Commons 4.0BY-SA license on\n",
            "lindat3.We also provide evaluationscripts with the dataset that should ensure\n",
            "comparable evaluation of different interactive learning approaches.\n",
            "\n",
            "## 10.Acknowledgments ##\n",
            "\n",
            "This work was funded by the Ministry of Education, Youthand Sports of the Czech\n",
            "Republic under the grant agreement LK11221 and core research funding, SVV\n",
            "project 260224, and GAUK grant 1170516 of Charles University inPrague. It used\n",
            "language resources stored and distributedby the LINDAT/CLARIN project of the\n",
            "Ministry of Education, Youth and Sports of the Czech Republic\n",
            "(projectLM2015071).\n",
            "\n",
            "## 11.Bibliographical References ##\n",
            "\n",
            "Berant, J. and Liang, P. (2015). Imitation learning ofagenda-based semantic\n",
            "parsers. Transactions of the Association for Computational Linguistics (TACL),\n",
            "3:545–558.Berant, J., Chou, A., Frostig, R., and Liang, P. (2013).Semantic\n",
            "Parsing on Freebase from Question-AnswerPairs. Proceedings of EMNLP,\n",
            "(October):1533–1544.Bordes, A., Chopra, S., and Weston, J. (2014).\n",
            "QuestionAnswering with Subgraph Embeddings.Bordes, A., Usunier, N., Chopra, S.,\n",
            "and Weston, J. (2015).Large-scale Simple Question Answering with MemoryNetworks.\n",
            "Pairs. Proceedings of EMNLP, (October):1533–1544.Bordes, A., Chopra, S., and\n",
            "Weston, J. (2014). QuestionAnswering with Subgraph Embeddings.Bordes A Usunier N\n",
            "Chopra S and Weston J (2015)\n",
            "Accepted as a workshop paper at RE-WOCHAT 2016\n",
            "France, F. D., Yvon, F., and Collin, O. (2003). Learning paraphrases to improve\n",
            "a question-answering system. In In Proceedings of the 10th Conference of\n",
            "EACLWorkshop Natural Language Processing for QuestionAnswering.Henderson, M.,\n",
            "Thomson, B., and Williams, J. (2014). Thesecond dialog state tracking challenge.\n",
            "In 15th AnnualMeeting of the Special Interest Group on Discourse andDialogue,\n",
            "volume 263.Imielinski, T. (2009). If you ask nicely , I will answer :Semantic\n",
            "Search and Today’s Search Engines. Search.Mrksic, N., S´eaghdha, D. ´O.,\n",
            "Thomson, B., Gasic, M., Su,P., Vandyke, D., Wen, T., and Young, S. J. (2015).\n",
            "Multidomain dialog state tracking using recurrent neural networks. CoRR,\n",
            "abs/1506.07190.Williams, J., Raux, A., Ramachandran, D., and Black, A.(2013).\n",
            "The Dialog State Tracking Challenge. Sigdial,(August):404–413.\n",
            "Imielinski, T. (2009). If you ask nicely , I will answer :Semantic Search and\n",
            "Today’s Search Engines. Search.\n",
            "ygMrksic, N., S´eaghdha, D. ´O., Thomson, B., Gasic, M., Su,P Vd kD WTd YS J\n",
            "(2015) M l i\n",
            "Concatenated text saved to /content/drive/MyDrive/W210/Extracting_Text/1603.09631_extracted_text.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Json Creation\n",
        "import json\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "def extract_text_from_scaled_pdf(pdf_path, page_number, coords, new_dimensions):\n",
        "    \"\"\"Extract text from specified coordinates in a scaled PDF.\"\"\"\n",
        "    doc = fitz.open(pdf_path)\n",
        "    scaled_doc = fitz.open()  # Create a new empty PDF for scaled pages\n",
        "\n",
        "    # Scale the specific page\n",
        "    page = doc.load_page(page_number)\n",
        "    new_page = scaled_doc.new_page(width=int(new_dimensions[1]), height=int(new_dimensions[0]))\n",
        "    new_page.show_pdf_page(new_page.rect, doc, page.number)\n",
        "\n",
        "    # Extract text from the scaled page\n",
        "    scaled_page = scaled_doc.load_page(0)  # As we have only one page in scaled_doc\n",
        "    extracted_text = scaled_page.get_text(\"text\", clip=fitz.Rect(coords))\n",
        "\n",
        "    # Clean up\n",
        "    doc.close()\n",
        "    scaled_doc.close()\n",
        "    return extracted_text\n",
        "\n",
        "# Path to your PDF file\n",
        "pdf_file_path = pdf_file_path\n",
        "document_name = os.path.splitext(os.path.basename(pdf_file_path))[0]\n",
        "\n",
        "# Reading the CSV file\n",
        "# Make sure you define 'combined_df' before this line\n",
        "df = combined_df\n",
        "df_sorted = df.sort_values(by=[df.columns[7], df.columns[4]])\n",
        "\n",
        "# Initialize the JSON structure outside the loop\n",
        "json_structure = {\n",
        "    \"paper_id\": document_name,\n",
        "    \"title\": \"\",  # Set this based on your data/logic\n",
        "    \"paper_text\": []\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Extract and process text\n",
        "for index, row in df_sorted.iterrows():\n",
        "    coords = (row['x0f'], row['y0f'], row['x1f'], row['y1f'])\n",
        "    page_number = int(row['page_number']) - 1\n",
        "    mask_shape = row['mask_shape']\n",
        "    category = row['category_lbl']\n",
        "    numbers = mask_shape.strip(\"()\").split(\", \")\n",
        "    extracted_text = extract_text_from_scaled_pdf(pdf_file_path, page_number, coords, numbers)\n",
        "\n",
        "    # Process the extracted text\n",
        "    lines = extracted_text.split('\\n')\n",
        "    processed_lines = [line[:-1] if line.endswith('-') else line for line in lines]\n",
        "    single_line_text = ''.join(processed_lines).strip()\n",
        "    single_line_text = textwrap.fill(single_line_text, width=80)\n",
        "\n",
        "    # Add to JSON structure\n",
        "    section_dict = {\n",
        "        \"section_name\": \"\",  # Set this based on your data/logic\n",
        "        \"section_text\": single_line_text,\n",
        "        \"section_annotation\": category,\n",
        "        \"section_page\": page_number + 1,\n",
        "        \"section_column\": 0,  # Set this based on your data/logic\n",
        "        \"section_location\": [coords]\n",
        "    }\n",
        "    json_structure[\"paper_text\"].append(section_dict)\n",
        "\n",
        "# Convert the structured dictionary to JSON\n",
        "json_output = json.dumps(json_structure, indent=4)\n",
        "\n",
        "# Save the JSON output to a file\n",
        "json_output_file_path = f'/content/drive/MyDrive/W210/Extracting_Text/{document_name}_extracted_text.json'\n",
        "with open(json_output_file_path, 'w') as json_file:\n",
        "    json_file.write(json_output)\n",
        "\n",
        "print(f\"JSON output saved to {json_output_file_path}\")"
      ],
      "metadata": {
        "id": "OLMCyiWUdPyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b21708c6-e973-4412-d934-6c3d9c3663f5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON output saved to /content/drive/MyDrive/W210/Extracting_Text/1603.09631_extracted_text.json\n"
          ]
        }
      ]
    }
  ]
}