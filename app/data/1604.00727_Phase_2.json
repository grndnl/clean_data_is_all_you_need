{
    "paper_id": "1604.00727.pdf",
    "title": "",
    "paper_text": [
        {
            "section_name": "Title_025",
            "section_text": "\n## Character-Level Question Answering with Attention ##\n",
            "section_annotation": "Title",
            "section_page": 1,
            "section_id": 0,
            "section_image": "1604.00727_page_0001_mask_img_0.jpg",
            "section_column": 0,
            "section_im_bbox": [
                136,
                71,
                458,
                98
            ]
        },
        {
            "section_name": "Picture_019",
            "section_text": "Wh e r e ...    b o r n ? … 𝑣<𝑠> Q: Where was Barack Obama born? O b a m a B a r a c k  O b a m a Obama 0.18 Barack Obama 0.60 ...  CNNE CNNE CNNE 𝑣𝐵𝑎𝑟𝑎𝑐𝑘 𝑂𝑏𝑎𝑚𝑎 … a) Question encoder (character-level LSTM) c) Entity & Predicate encoder (character-level CNNs) b) KB query decoder (LSTM with an attention mechanism)  … people/…/spouse 0.2 people/…/…birth 0.58 ... CNNP CNNP CNNP … h0 </s> Entity attentions  Predicate attentions p e o p l e/…/ s p o u s e  h1 h2 h3 p e o p l e/…/ p l a  c e _ o f _ b i r t h  s1 …   s4          …             sn {α1} {α2} 𝑣𝑒𝑛𝑡𝑖𝑡𝑦 𝑣𝑝𝑟𝑒𝑑𝑖𝑐𝑎𝑡𝑒 𝑐1 𝑐2 …",
            "section_annotation": "Picture",
            "section_page": 2,
            "section_id": 0,
            "section_image": "1604.00727_page_0002_mask_img_0.jpg",
            "section_column": 0,
            "section_im_bbox": [
                71,
                68,
                538,
                278
            ]
        },
        {
            "section_name": "Text_675",
            "section_text": "tion and determine the most likely KB query. M t f th h d l l",
            "section_annotation": "Text",
            "section_page": 3,
            "section_id": 0,
            "section_image": "1604.00727_page_0003_mask_img_0.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                66,
                268,
                88
            ]
        },
        {
            "section_name": "Text_676",
            "section_text": "2. We feed x1, ..., xn from left to right into a two layer gated-feedback LSTM, and keep the out puts at all time steps as the embeddings for the question, i.e., these are the vectors s1, ..., sn.",
            "section_annotation": "Text",
            "section_page": 4,
            "section_id": 0,
            "section_image": "1604.00727_page_0004_mask_img_0.jpg",
            "section_column": 0,
            "section_im_bbox": [
                71,
                66,
                293,
                128
            ]
        },
        {
            "section_name": "Title_026",
            "section_text": "RESULTS ON SIMPLEQUESTIONS DATASET",
            "section_annotation": "Title",
            "section_page": 5,
            "section_id": 0,
            "section_image": "1604.00727_page_0005_mask_img_0.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                66,
                538,
                75
            ]
        },
        {
            "section_name": "Text_677",
            "section_text": "aliases and 886 out of 1,034 unique predicates in the test set were present in the train set. For the proposed dataset, there are two evaluation settings, called FB2M and FB5M, respectively. The former uses a KB for candidate generation which is a sub set of Freebase and contains 2M entities, while the latter uses subset of Freebase with 5M entities.",
            "section_annotation": "Text",
            "section_page": 6,
            "section_id": 0,
            "section_image": "1604.00727_page_0006_mask_img_0.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                66,
                298,
                163
            ]
        },
        {
            "section_name": "Text_678",
            "section_text": "cate. However, previous semantic parsing-based QA approaches (Yih et al., 2015; Yih et al., 2014) as sume that there is a clear separation between the predicate and entity mentions in the question. In contrast, the proposed model does not need to make this hard categorization, and attends the word “uni versity” when predicting both the entity and predi cate.",
            "section_annotation": "Text",
            "section_page": 8,
            "section_id": 0,
            "section_image": "1604.00727_page_0008_mask_img_0.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                66,
                293,
                178
            ]
        },
        {
            "section_name": "Section-header_081",
            "section_text": "\n## References ##\n",
            "section_annotation": "Section-header",
            "section_page": 9,
            "section_id": 0,
            "section_image": "1604.00727_page_0009_mask_img_0.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                66,
                128,
                83
            ]
        },
        {
            "section_name": "Text_679",
            "section_text": "[Santos and Zadrozny2014] Cicero D Santos and Bianca Zadrozny. 2014. Learning character-level represen tations for part-of-speech tagging. In Proceedings of the 31st International Conference on Machine Learn ing (ICML-14), pages 1818–1826. [Shen et al.2014] Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Gregoire Mesnil. 2014. A latent semantic model with convolutional-pooling structure for information retrieval. CIKM, November. [Sukhbaatar et al.2015] Sainbayar Sukhbaatar, Jason We ston, Rob Fergus, et al. 2015. End-to-end memory networks. In Advances in Neural Information Process ing Systems, pages 2431–2439. [Sutskever et al.2014] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104–3112. [Tang and Mooney2001] Lappoon R Tang and Ray mond J Mooney. 2001. Using multiple clause con structors in inductive logic programming for semantic parsing. In Machine Learning: ECML 2001, pages 466–477. Springer. [Venugopalan et al.2015] Subhashini Venugopalan, Hui juan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, and Kate Saenko. 2015. Translating videos to natural language using deep recurrent neural net works. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Com putational Linguistics: Human Language Technolo gies, pages 1494–1504, Denver, Colorado, May–June. Association for Computational Linguistics. [Vinyals et al.2015] Oriol Vinyals, Ł ukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. 2015. Grammar as a foreign language. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Gar nett, editors, Advances in Neural Information Process ing Systems 28, pages 2773–2781. Curran Associates, Inc. [Xu et al.2015] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudi nov, Rich Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In David Blei and Francis Bach, ed itors, Proceedings of the 32nd International Confer ence on Machine Learning (ICML-15), pages 2048– 2057. JMLR Workshop and Conference Proceedings. [Yih et al.2014] Wen-tau Yih, Xiaodong He, and Christo pher Meek. 2014. Semantic parsing for single-relation question answering. In Proceedings of ACL. Associa tion for Computational Linguistics, June. [Yih et al.2015] Wen-tau Yih, Ming-Wei Chang, Xi aodong He, and Jianfeng Gao. 2015. Semantic pars ing via staged query graph generation: Question an swering with knowledge base. In Proceedings of the",
            "section_annotation": "Text",
            "section_page": 10,
            "section_id": 0,
            "section_image": "1604.00727_page_0010_mask_img_0.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                66,
                298,
                708
            ]
        },
        {
            "section_name": "Text_680",
            "section_text": "David Golub ersity of Washin",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 1,
            "section_image": "1604.00727_page_0001_mask_img_1.jpg",
            "section_column": 0,
            "section_im_bbox": [
                166,
                126,
                238,
                150
            ]
        },
        {
            "section_name": "Caption_052",
            "section_text": "Figure 1: Our encoder-decoder architecture that generates a query against a structured knowledge base. We encode our question via a long short-term memory (LSTM) network and an attention mechanism to produce our context vector. During decoding, at each time step, we feed the current context vector and an embedding of the English alias of the previously generated knowledge base entry into an attention-based decoding LSTM to generate the new candidate entity or predicate.",
            "section_annotation": "Caption",
            "section_page": 2,
            "section_id": 1,
            "section_image": "1604.00727_page_0002_mask_img_1.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                281,
                528,
                353
            ]
        },
        {
            "section_name": "Text_681",
            "section_text": "y q y Most of these approaches use word-level embed dings to encode entities and predicates, and there fore might suffer from the out-of-vocabulary (OOV) problem when they encounter unseen words during test time. Consequently, they often rely on signif icant data augmentation from sources such as Par alex (Fader et al., 2013), which contains 18 million question-paraphrase pairs scraped from WikiAn swers, to have sufﬁcient examples for each word they encounter (Bordes et al., 2014b; Yih et al., 2014; Bordes et al 2015)",
            "section_annotation": "Text",
            "section_page": 3,
            "section_id": 1,
            "section_image": "1604.00727_page_0003_mask_img_1.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                81,
                298,
                221
            ]
        },
        {
            "section_name": "Text_682",
            "section_text": "\n## 3.2 Encoding Entities and Predicates in the KB ##\n",
            "section_annotation": "Text",
            "section_page": 4,
            "section_id": 1,
            "section_image": "1604.00727_page_0004_mask_img_1.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                126,
                293,
                143
            ]
        },
        {
            "section_name": "Title_027",
            "section_text": "RESULTS ON SIMPLEQUESTIONS DATASET",
            "section_annotation": "Title",
            "section_page": 5,
            "section_id": 1,
            "section_image": "1604.00727_page_0005_mask_img_1.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                68,
                538,
                81
            ]
        },
        {
            "section_name": "Text_683",
            "section_text": "In our experiments, the Memory Neural Networks (MemNNs) proposed in Bordes et al. (2015) serve as the baselines. For training, in addition to the 76K questions in the training set, the MemNNs use 3K training questions from WebQuestions (Berant et al., 2013), 15M paraphrases from WikiAnswers (Fader et al., 2013), and 11M and 12M automatically gener ated questions from the KB for the FB2M and FB5M settings, respectively. In contrast, our models are trained only on the 76K questions in the training set. F d l b h l f h LSTM b d",
            "section_annotation": "Text",
            "section_page": 6,
            "section_id": 1,
            "section_image": "1604.00727_page_0006_mask_img_1.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                161,
                298,
                298
            ]
        },
        {
            "section_name": "Table_033",
            "section_text": "# of LSTM Layers # of CNN Layers Joint Accuracy Predicate Accuracy Entity Accuracy",
            "section_annotation": "Table",
            "section_page": 7,
            "section_id": 1,
            "section_image": "1604.00727_page_0007_mask_img_1.jpg",
            "section_column": 0,
            "section_im_bbox": [
                71,
                67,
                523,
                80
            ]
        },
        {
            "section_name": "Section-header_082",
            "section_text": "\n## 6 Error Analysis ##\n",
            "section_annotation": "Section-header",
            "section_page": 8,
            "section_id": 1,
            "section_image": "1604.00727_page_0008_mask_img_1.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                181,
                168,
                203
            ]
        },
        {
            "section_name": "Text_684",
            "section_text": "[Amodei et al.2015] Dario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan C. Catanzaro, Jingdong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel, Linxi Fan, Christopher Fougner, Tony Han, Awni Y. Han nun, Billy Jun, Patrick LeGresley, Libby Lin, Sharan Narang, Andrew Y. Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh, David Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yogatama, Jun Zhan, and Zhenyao Zhu. 2015. Deep speech 2: End-to-end speech recognition in english and mandarin. CoRR, abs/1512.02595. [Bahdanau et al.2015] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In ICLR. [Berant and Liang2014] J. Berant and P. Liang. 2014. Se mantic parsing via paraphrasing. In Association for Computational Linguistics (ACL). [Berant et al.2013] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Empirical Methods in Natural Language Processing (EMNLP). [Bordes et al.2014a] Antoine Bordes, Sumit Chopra, and Jason Weston. 2014a. Question answering with sub graph embeddings. In Proceedings of the 2014 Con ference on Empirical Methods in Natural Language Processing (EMNLP), pages 615–620, Doha, Qatar, October. Association for Computational Linguistics. [Bordes et al.2014b] Antoine Bordes, Jason Weston, and Nicolas Usunier. 2014b. Open question answer ing with weakly supervised embedding models. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases  Volume 8724, ECML PKDD 2014, pages 165–180, New York, NY, USA. Springer-Verlag New York, Inc. [Bordes et al.2015] Antoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. 2015. Large-scale simple question answering with memory networks. In Proc. NIPS. [Chung et al.2015] Junyoung Chung, C¸ aglar G¨ulc¸ehre, Kyunghyun Cho, and Yoshua Bengio. 2015. Gated feedback recurrent neural networks. In Proceedings of the 32nd International Conference on Machine Learn ing, ICML 2015, Lille, France, 6-11 July 2015, pages 2067–2075. [Chung et al.2016] Junyoung Chung, Kyunghyun Cho, and Yoshua Bengio. 2016. A character-level de coder without explicit segmentation for neural ma chine translation. arXiv preprint arXiv:1603.06147.",
            "section_annotation": "Text",
            "section_page": 9,
            "section_id": 1,
            "section_image": "1604.00727_page_0009_mask_img_1.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                86,
                296,
                690
            ]
        },
        {
            "section_name": "Text_685",
            "section_text": "Joint Conference of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing of the AFNLP. ACL Association for Computational Linguistics, July. [Zaremba and Sutskever2014] Wojciech Zaremba and Ilya Sutskever. 2014. Learning to execute. arXiv preprint arXiv:1410.4615. [Zhang et al.2015] Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classiﬁcation. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Ad vances in Neural Information Processing Systems 28, pages 649–657. Curran Associates, Inc.",
            "section_annotation": "Text",
            "section_page": 10,
            "section_id": 1,
            "section_image": "1604.00727_page_0010_mask_img_1.jpg",
            "section_column": 1,
            "section_im_bbox": [
                296,
                66,
                528,
                228
            ]
        },
        {
            "section_name": "Text_686",
            "section_text": "David Golub University of Washington bd@ hi t",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 2,
            "section_image": "1604.00727_page_0001_mask_img_2.jpg",
            "section_column": 0,
            "section_im_bbox": [
                136,
                141,
                263,
                163
            ]
        },
        {
            "section_name": "Text_687",
            "section_text": "for the KB entries that are not present in our vo cabulary, a challenging task for standard encoder decoder frameworks.",
            "section_annotation": "Text",
            "section_page": 2,
            "section_id": 2,
            "section_image": "1604.00727_page_0002_mask_img_2.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                366,
                293,
                408
            ]
        },
        {
            "section_name": "Text_688",
            "section_text": "y ( 2014; Bordes et al., 2015). As opposed to word-level modeling, character level modeling can be used to handle the OOV is sue. While character-level modeling has not been applied to factoid question answering before, it has been successfully applied to information retrieval, machine translation, sentiment analysis, classiﬁca tion, and named entity recognition (Huang et al., 2013; Shen et al., 2014; Chung et al., 2016; Zhang et al., 2015; Santos and Zadrozny, 2014; dos Santos and Gatti, 2014; Klein et al., 2003; dos Santos, 2014; dos Santos et al., 2015). Moreover, Chung et al. (2015) demonstrate that gated-feedback LSTMs on top of character-level embeddings can capture long term dependencies in language modeling.",
            "section_annotation": "Text",
            "section_page": 3,
            "section_id": 2,
            "section_image": "1604.00727_page_0003_mask_img_2.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                214,
                298,
                415
            ]
        },
        {
            "section_name": "Text_689",
            "section_text": "To encode an entity or predicate in the KB, we take two steps:",
            "section_annotation": "Text",
            "section_page": 4,
            "section_id": 2,
            "section_image": "1604.00727_page_0004_mask_img_2.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                141,
                293,
                177
            ]
        },
        {
            "section_name": "Title_028",
            "section_text": "RESULTS ON SIMPLEQUESTIONS DATASET KB TRAIN SOURCES AUTOGEN. EMBED MODEL ENSEMBLE SQ ACCURACY # TRAIN WQ SIQ PRP QUESTIONS TYPE EXAMPLES FB2M no yes no no Char Ours 1 model 70.9 76K FB2M no yes no no Word Ours 1 model 53.9 76K FB2M yes yes yes yes Word MemNN 1 model 62.7 26M FB5M no yes no no Char Ours 1 model 70.3 76K FB5M no yes no no Word Ours 1 model 53.1 76K FB5M yes yes yes yes Word MemNN 5 models 63.9 27M FB5M yes yes yes yes Word MemNN Subgraph 62.9 27M FB5M yes yes yes yes Word MemNN 1 model 62.2 27M T bl 1 E i t l lt th Si l Q ti d t t M NN lt f B d t l",
            "section_annotation": "Title",
            "section_page": 5,
            "section_id": 2,
            "section_image": "1604.00727_page_0005_mask_img_2.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                74,
                538,
                207
            ]
        },
        {
            "section_name": "Text_690",
            "section_text": "y q g For our model, both layers of the LSTM-based question encoder have size 200. The hidden layers of the LSTM-based decoder have size 100, and the CNNs for entity and predicate embeddings have a hidden layer of size 200 and an output layer of size 100. The CNNs for entity and predicate embeddings use a receptive ﬁeld of size 4, λ = 5, and m = 100. We train the models using RMSProp with a learning rate of 1e−4.",
            "section_annotation": "Text",
            "section_page": 6,
            "section_id": 2,
            "section_image": "1604.00727_page_0006_mask_img_2.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                293,
                293,
                413
            ]
        },
        {
            "section_name": "Table_034",
            "section_text": "# of LSTM Layers # of CNN Layers Joint Accuracy Predicate Accuracy Entity Accuracy 2 2 78.3 80.0 96.6 2 1 77.7 79.4 96.8 1 2 71.5 73.9 95.0 1 1 72.2 74.7 94.9",
            "section_annotation": "Table",
            "section_page": 7,
            "section_id": 2,
            "section_image": "1604.00727_page_0007_mask_img_2.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                73,
                528,
                133
            ]
        },
        {
            "section_name": "Text_691",
            "section_text": "We randomly sampled 50 questions where the best-performing model generated the wrong KB query and categorized the errors. For 46 out of the 50 examples, the model predicted a predicate with a very similar alias to the true predicate, i.e. “/mu sic/release/track” vs. “/music/release/track list”. For 21 out of the 50 examples, the model predicted the wrong entity, e.g., “Album” vs. “Still Here” for the question “What type of album is still here?”. Finally, for 18 of the 50 examples, the model pre dicted the wrong entity and predicate, i.e. (“Play”, “/freebase/equivalent topic/equivalent type”) for the question “which instrument does amapola cabase play?” Training on more data, augment ing the negative sample set with words from the question that are not an entity mention, and having more examples that disambiguate between similar predicates may ameliorate many of these errors.",
            "section_annotation": "Text",
            "section_page": 8,
            "section_id": 2,
            "section_image": "1604.00727_page_0008_mask_img_2.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                201,
                298,
                448
            ]
        },
        {
            "section_name": "Text_692",
            "section_text": "y g golubd@cs.washington.edu",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 3,
            "section_image": "1604.00727_page_0001_mask_img_3.jpg",
            "section_column": 0,
            "section_im_bbox": [
                111,
                156,
                288,
                173
            ]
        },
        {
            "section_name": "Text_693",
            "section_text": "decode a ewo s. Our novel, character-level encoder-decoder model is compact, requires signiﬁcantly less data to train than previous work, and is able to generalize well to unseen entities in test time. In particular, without use of ensembles, we achieve 70.9% accu racy in the Freebase2M setting and 70.3% accuracy in the Freebase5M setting on the SimpleQuestions dataset, outperforming the previous state-of-arts of 62.7% and 63.9% (Bordes et al., 2015) by 8.2% and 6.4% respectively. Moreover, we only use the training questions provided in SimpleQuestions to train our model, which cover about 24% of words in entity aliases on the test set. This demonstrates the robustness of the character-level model to unseen entities. In contrast, data augmentation is usually necessary to provide more coverage for unseen entities and predicates, as done in previous work (Bordes et al., 2015; Yih et al., 2014).",
            "section_annotation": "Text",
            "section_page": 2,
            "section_id": 3,
            "section_image": "1604.00727_page_0002_mask_img_3.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                406,
                298,
                652
            ]
        },
        {
            "section_name": "Text_694",
            "section_text": "p g g g Lastly, encoder-decoder networks have been ap plied to many structured machine learning tasks. First introduced in Sutskever et al. (2014), in an encoder-decoder network, a source sequence is ﬁrst encoded with a recurrent neural network (RNN) into a ﬁxed-length vector which intuitively captures its “meaning”, and then decoded into a desired tar get sequence. This approach and related memory based or attention-based approaches have been suc cessfully applied in diverse domains such as speech recognition, machine translation, image captioning, parsing, executing programs, and conversational di alogues (Amodei et al., 2015; Venugopalan et al., 2015; Bahdanau et al., 2015; Vinyals et al., 2015; Zaremba and Sutskever, 2014; Xu et al., 2015; Sukhbaatar et al., 2015).",
            "section_annotation": "Text",
            "section_page": 3,
            "section_id": 3,
            "section_image": "1604.00727_page_0003_mask_img_3.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                411,
                298,
                625
            ]
        },
        {
            "section_name": "List-item_133",
            "section_text": "1. We ﬁrst extract one-hot encoding vectors for characters in its English alias, x1, ..., xn, where xi represents the one-hot encoding vector for the ith character in the alias. 2. We then feed x1, ..., xn into a temporal CNN with two alternating convolutional and fully connected layers, followed by one fully connected layer: f(x1, ..., xn) = tanh(W3 × max(tanh(W2× conv(tanh(W1 × conv(x1, ..., xn)))))) where f(x1...n) is an embedding vector of size N, W3 has size RN×h, conv represents a tem poral convolutional neural network, and max represents a max pooling layer in the temporal direction.",
            "section_annotation": "List-item",
            "section_page": 4,
            "section_id": 3,
            "section_image": "1604.00727_page_0004_mask_img_3.jpg",
            "section_column": 0,
            "section_im_bbox": [
                71,
                171,
                298,
                398
            ]
        },
        {
            "section_name": "Caption_053",
            "section_text": "5 y y y y 6 Table 1: Experimental results on the SimpleQuestions dataset. MemNN results are from Bordes et al. (2015). WQ, SIQ and PRP stand for WebQuestions, SimpleQuestions and extracted paraphrases from WikiAnswers, respectively.",
            "section_annotation": "Caption",
            "section_page": 5,
            "section_id": 3,
            "section_image": "1604.00727_page_0005_mask_img_3.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                197,
                533,
                248
            ]
        },
        {
            "section_name": "Text_695",
            "section_text": "In order to make the input character sequence long enough to ﬁll up the receptive ﬁelds of mul tiple CNN layers, we pad each predicate or entity using three padding symbols P, a special start sym bol, and a special end symbol. For instance, Obama would become SstartPPPObamaPPPSend. For consistency, we apply the same padding to the ques tions.",
            "section_annotation": "Text",
            "section_page": 6,
            "section_id": 3,
            "section_image": "1604.00727_page_0006_mask_img_3.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                411,
                298,
                518
            ]
        },
        {
            "section_name": "Caption_054",
            "section_text": "Table 2: Results for a random sampling experiment where we varied the number of layers used for convolutions and the question-encoding LSTM. We terminated training models after 14 epochs and 3 days on a GPU.",
            "section_annotation": "Caption",
            "section_page": 7,
            "section_id": 3,
            "section_image": "1604.00727_page_0007_mask_img_3.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                132,
                528,
                163
            ]
        },
        {
            "section_name": "Section-header_083",
            "section_text": "\n## 7 Conclusion ##\n",
            "section_annotation": "Section-header",
            "section_page": 8,
            "section_id": 3,
            "section_image": "1604.00727_page_0008_mask_img_3.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                451,
                148,
                468
            ]
        },
        {
            "section_name": "Text_696",
            "section_text": "[dos Santos and Gatti2014] Cicero dos Santos and Maira Gatti. 2014. Deep convolutional neural networks for sentiment analysis of short texts. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 69–78, Dublin, Ireland, August. Dublin City Univer sity and Association for Computational Linguistics. [dos Santos et al.2015] Cıcero dos Santos, Victor Guimaraes, RJ Niter´oi, and Rio de Janeiro. 2015. Boosting named entity recognition with neural char acter embeddings. In Proceedings of NEWS 2015 The Fifth Named Entities Workshop, page 25. [dos Santos2014] Cicero dos Santos. 2014. Think posi tive: Towards twitter sentiment analysis from scratch. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 647–651, Dublin, Ireland, August. Association for Computa tional Linguistics and Dublin City University. [Fader et al.2013] Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. 2013. Paraphrase-driven learning for open question answering. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1608– 1618, Soﬁa, Bulgaria, August. Association for Com putational Linguistics. [Fader et al.2014] Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. 2014. Open question answering over curated and extracted knowledge bases. In The 20th ACM SIGKDD International Conference on Knowl edge Discovery and Data Mining, KDD ’14, New York, NY, USA - August 24 - 27, 2014, pages 1156–1165. [Hochreiter and Schmidhuber1997] Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural Comput., 9(8):1735–1780, November. [Huang et al.2013] Po-Sen Huang, Xiaodong He, Jian feng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Confer ence on information & knowledge management, pages 2333–2338. ACM. [Klein et al.2003] Dan Klein, Joseph Smarr, Huy Nguyen, and Christopher D Manning. 2003. Named entity recognition with character-level models. In Proceed ings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume 4, pages 180– 183. Association for Computational Linguistics. [Ljubeˇsi´c et al.2014] Nikola Ljubeˇsi´c, Tomaˇz Erjavec, and Darja Fiˇser. 2014. Standardizing tweets with character-level machine translation. In Proceedings of the 15th International Conference on Computational Linguistics and Intelligent Text Processing - Volume 8404, CICLing 2014, pages 164–175, New York, NY, USA. Springer-Verlag New York, Inc.",
            "section_annotation": "Text",
            "section_page": 9,
            "section_id": 3,
            "section_image": "1604.00727_page_0009_mask_img_3.jpg",
            "section_column": 1,
            "section_im_bbox": [
                296,
                69,
                533,
                708
            ]
        },
        {
            "section_name": "Text_697",
            "section_text": "\n## Abstract ##\n",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 4,
            "section_image": "1604.00727_page_0001_mask_img_4.jpg",
            "section_column": 0,
            "section_im_bbox": [
                151,
                241,
                208,
                258
            ]
        },
        {
            "section_name": "Section-header_084",
            "section_text": "\n## 2 Related Work ##\n",
            "section_annotation": "Section-header",
            "section_page": 2,
            "section_id": 4,
            "section_image": "1604.00727_page_0002_mask_img_4.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                651,
                163,
                673
            ]
        },
        {
            "section_name": "Text_698",
            "section_text": "Sukhbaatar et al., 2015). Unlike previous work, we formulate question an swering as a problem of decoding the KB query given the question and KB entries which are en coded in embedding spaces. We therefore inte grate the learning of question and KB embeddings in a uniﬁed encoder-decoder framework, where the",
            "section_annotation": "Text",
            "section_page": 3,
            "section_id": 4,
            "section_image": "1604.00727_page_0003_mask_img_4.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                618,
                293,
                703
            ]
        },
        {
            "section_name": "Text_699",
            "section_text": "We use a CNN as opposed to an LSTM to embed KB entries primarily for computational efﬁciency. Also, we use two different CNNs to encode enti ties and predicates because they typically have sig niﬁcantly different styles (e.g., “Barack Obama” vs. “/people/person/place of birth”).",
            "section_annotation": "Text",
            "section_page": 4,
            "section_id": 4,
            "section_image": "1604.00727_page_0004_mask_img_4.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                396,
                293,
                483
            ]
        },
        {
            "section_name": "Text_700",
            "section_text": "each other. In current experiments we use a simple cosine-similarity metric: cos(x1, x2).",
            "section_annotation": "Text",
            "section_page": 5,
            "section_id": 4,
            "section_image": "1604.00727_page_0005_mask_img_4.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                261,
                293,
                291
            ]
        },
        {
            "section_name": "Section-header_085",
            "section_text": "\n## 5 Results ##\n",
            "section_annotation": "Section-header",
            "section_page": 6,
            "section_id": 4,
            "section_image": "1604.00727_page_0006_mask_img_4.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                526,
                128,
                543
            ]
        },
        {
            "section_name": "Table_035",
            "section_text": "Embedding Type Joint Accuracy Predicate Accuracy Entity Accuracy Character 78.3 80.0 96.6 Word 37.6 78.8 45.5",
            "section_annotation": "Table",
            "section_page": 7,
            "section_id": 4,
            "section_image": "1604.00727_page_0007_mask_img_4.jpg",
            "section_column": 0,
            "section_im_bbox": [
                126,
                171,
                468,
                213
            ]
        },
        {
            "section_name": "Text_701",
            "section_text": "In this paper, we proposed a new character-level, attention-based encoder-decoder model for question answering. In our approach, embeddings of ques tions, entities, and predicates are all jointly learned to directly optimize the likelihood of generating the correct KB query. Our approach improved the state of-the-art accuracy on the SimpleQuestions bench mark signiﬁcantly, using much less data than pre vious work. Furthermore, thanks to character-level modeling, we have a compact model that is robust to unseen entities. Visualizations of the attention distribution reveal that our model, although built on character-level inputs, can learn higher-level se mantic concepts required to answer a natural lan guage question with a structured KB. In the future we would like to extend our system to handle multi relation questions.",
            "section_annotation": "Text",
            "section_page": 8,
            "section_id": 4,
            "section_image": "1604.00727_page_0008_mask_img_4.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                471,
                293,
                703
            ]
        },
        {
            "section_name": "Text_702",
            "section_text": "We show that a character-level encoder decoder framework can be successfully ap plied to question answering with a structured knowledge base. We use our model for single relation question answering and demonstrate the effectiveness of our approach on the Sim pleQuestions dataset (Bordes et al., 2015), where we improve state-of-the-art accuracy from 63.9% to 70.9%, without use of ensem bles. Importantly, our character-level model has 16x fewer parameters than an equivalent word-level model, can be learned with signif icantly less data compared to previous work, which relies on data augmentation, and is ro bust to new entities in testing.",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 5,
            "section_image": "1604.00727_page_0001_mask_img_5.jpg",
            "section_column": 0,
            "section_im_bbox": [
                86,
                271,
                273,
                452
            ]
        },
        {
            "section_name": "Text_703",
            "section_text": "Our work is motivated by three major threads of research in machine learning and natural lan",
            "section_annotation": "Text",
            "section_page": 2,
            "section_id": 5,
            "section_image": "1604.00727_page_0002_mask_img_5.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                671,
                293,
                708
            ]
        },
        {
            "section_name": "Text_704",
            "section_text": "whole system is optimized end-to-end.",
            "section_annotation": "Text",
            "section_page": 3,
            "section_id": 5,
            "section_image": "1604.00727_page_0003_mask_img_5.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                66,
                473,
                88
            ]
        },
        {
            "section_name": "Section-header_086",
            "section_text": "\n## 3.3 Decoding the KB Query ##\n",
            "section_annotation": "Section-header",
            "section_page": 4,
            "section_id": 5,
            "section_image": "1604.00727_page_0004_mask_img_5.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                486,
                208,
                503
            ]
        },
        {
            "section_name": "Text_705",
            "section_text": "cosine similarity metric: cos(x1, x2). Using this similarity metric, the likelihoods of generating entity ej and predicate pk are:",
            "section_annotation": "Text",
            "section_page": 5,
            "section_id": 5,
            "section_image": "1604.00727_page_0005_mask_img_5.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                286,
                298,
                322
            ]
        },
        {
            "section_name": "Section-header_087",
            "section_text": "\n## 5.1 End-to-end Results on SimpleQuestions ##\n",
            "section_annotation": "Section-header",
            "section_page": 6,
            "section_id": 5,
            "section_image": "1604.00727_page_0006_mask_img_5.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                546,
                278,
                563
            ]
        },
        {
            "section_name": "Caption_055",
            "section_text": "Table 3: Results for a random sampling experiment where we varied the embedding type (word vs. character-level). We used 2 layered-LSTMs and CNNs for all our experiments. Our models were trained for 14 epochs and 3 days on a GPU.",
            "section_annotation": "Caption",
            "section_page": 7,
            "section_id": 5,
            "section_image": "1604.00727_page_0007_mask_img_5.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                211,
                528,
                253
            ]
        },
        {
            "section_name": "Picture_020",
            "section_text": "a) b) c) d)",
            "section_annotation": "Picture",
            "section_page": 8,
            "section_id": 5,
            "section_image": "1604.00727_page_0008_mask_img_5.jpg",
            "section_column": 1,
            "section_im_bbox": [
                306,
                71,
                523,
                690
            ]
        },
        {
            "section_name": "Section-header_088",
            "section_text": "\n## 1 Introduction ##\n",
            "section_annotation": "Section-header",
            "section_page": 1,
            "section_id": 6,
            "section_image": "1604.00727_page_0001_mask_img_6.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                466,
                153,
                488
            ]
        },
        {
            "section_name": "Text_706",
            "section_text": "guage processing: semantic-parsing for open domain question answering, character-level lan guage modeling, and encoder-decoder methods. S i i f d i i",
            "section_annotation": "Text",
            "section_page": 2,
            "section_id": 6,
            "section_image": "1604.00727_page_0002_mask_img_6.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                366,
                528,
                413
            ]
        },
        {
            "section_name": "Section-header_089",
            "section_text": "\n## 3 Model ##\n",
            "section_annotation": "Section-header",
            "section_page": 3,
            "section_id": 6,
            "section_image": "1604.00727_page_0003_mask_img_6.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                91,
                358,
                108
            ]
        },
        {
            "section_name": "Text_707",
            "section_text": "g y To generate the single topic entity and predicate to form the KB query, we use a decoder with two key components:",
            "section_annotation": "Text",
            "section_page": 4,
            "section_id": 6,
            "section_image": "1604.00727_page_0004_mask_img_6.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                501,
                293,
                548
            ]
        },
        {
            "section_name": "Text_708",
            "section_text": "Following Bordes et al. (2015), we report results on the SimpleQuestions dataset in terms of SQ ac curacy, for both FB2M and FB5M settings in Ta ble 1. SQ accuracy is deﬁned as the percentage of questions for which the model generates a cor rect KB query (i.e., both the topic entity and predi cate are correct). Our single character-level model achieves SQ accuracies of 70.9% and 70.3% on the FB2M and FB5M settings, outperforming the previous state-of-art results by 8.2% and 6.4%, re",
            "section_annotation": "Text",
            "section_page": 6,
            "section_id": 6,
            "section_image": "1604.00727_page_0006_mask_img_6.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                561,
                293,
                703
            ]
        },
        {
            "section_name": "Text_709",
            "section_text": "We ﬁnd that a two-layer LSTM boosts joint accu racy by over 6%. The majority of accuracy gains are a result of improved predicate predictions, possibly because entity accuracy is already saturated in this experimental setup.",
            "section_annotation": "Text",
            "section_page": 7,
            "section_id": 6,
            "section_image": "1604.00727_page_0007_mask_img_6.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                276,
                293,
                348
            ]
        },
        {
            "section_name": "Caption_056",
            "section_text": "Figure 2: Attention distribution over outputs of a left-to-right LSTM on question characters.",
            "section_annotation": "Caption",
            "section_page": 8,
            "section_id": 6,
            "section_image": "1604.00727_page_0008_mask_img_6.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                691,
                528,
                723
            ]
        },
        {
            "section_name": "Text_710",
            "section_text": "Single-relation factoid questions are the most com mon form of questions found in search query logs and community question answering websites (Yih et al., 2014; Fader et al., 2013). A knowledge-base (KB) such as Freebase, DBpedia, or Wikidata can help answer such questions after users reformulate them as queries. For instance, the question “Where was Barack Obama born?” can be answered by is suing the following KB query:",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 7,
            "section_image": "1604.00727_page_0001_mask_img_7.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                491,
                298,
                618
            ]
        },
        {
            "section_name": "Text_711",
            "section_text": "g g g, Semantic parsing for open-domain question an swering, which translates a question into a struc tured KB query, is a key component in question an swering with a KB. While early approaches relied on building high-quality lexicons for domain-speciﬁc databases such as GeoQuery (Tang and Mooney, 2001), recent work has focused on building seman tic parsing frameworks for general knowledge bases such as Freebase (Yih et al., 2014; Bordes et al., 2014a; Bordes et al., 2015; Berant and Liang, 2014; Fader et al., 2013).",
            "section_annotation": "Text",
            "section_page": 2,
            "section_id": 7,
            "section_image": "1604.00727_page_0002_mask_img_7.jpg",
            "section_column": 1,
            "section_im_bbox": [
                296,
                407,
                533,
                556
            ]
        },
        {
            "section_name": "Text_712",
            "section_text": "Since we focus on single-relation question answer ing in this work, our model decodes every ques tion into a KB query that consists of exactly two elements–the topic entity, and the predicate. More formally, our model is a function f(q, {e}, {p}) that takes as input a question q, a set of candidate enti ties {e} = e1, ..., en, a set of candidate predicates {p} = p1, ..., pm, and produces a likelihood score p(ei, pj|q) of generating entity ei and predicate pj given question q for all i ∈ 1...n, j ∈ 1...m. As illustrated in Figure 1 our model consists of",
            "section_annotation": "Text",
            "section_page": 3,
            "section_id": 7,
            "section_image": "1604.00727_page_0003_mask_img_7.jpg",
            "section_column": 1,
            "section_im_bbox": [
                296,
                111,
                528,
                252
            ]
        },
        {
            "section_name": "List-item_134",
            "section_text": "1. An LSTM-based decoder with attention. Its hidden states at each time step i, hi, have the same dimensionality N as the embeddings of entities/predicates. The initial hidden state h0 is set to the zero vector: ⃗0. 2. A pairwise semantic relevance function that measures the similarity between the hidden units of the LSTM and the embedding of an en tity or predicate candidate. It then returns the mostly likely entity or predicate based on the similarity score.",
            "section_annotation": "List-item",
            "section_page": 4,
            "section_id": 7,
            "section_image": "1604.00727_page_0004_mask_img_7.jpg",
            "section_column": 0,
            "section_im_bbox": [
                71,
                547,
                298,
                707
            ]
        },
        {
            "section_name": "Formula_019",
            "section_text": "P(ej) = exp(λcos(h1, ej)) �n i=1 exp(λcos(h1, ei)) P(pk) = exp(λcos(h2, pk)) �m i=1 exp(λcos(h2, pi))",
            "section_annotation": "Formula",
            "section_page": 5,
            "section_id": 7,
            "section_image": "1604.00727_page_0005_mask_img_7.jpg",
            "section_column": 0,
            "section_im_bbox": [
                101,
                331,
                258,
                393
            ]
        },
        {
            "section_name": "Text_713",
            "section_text": "spectively. Compared to the character-level model, which only has 1.2M parameters, our word-level model has 19.9M parameters, and only achieves a best SQ accuracy of 53.9%. In addition, in contrast to previous work, the OOV issue is much more se vere for our word-level model, since we use no data augmentation to cover entities unseen in the train set.",
            "section_annotation": "Text",
            "section_page": 6,
            "section_id": 7,
            "section_image": "1604.00727_page_0006_mask_img_7.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                66,
                528,
                178
            ]
        },
        {
            "section_name": "Section-header_090",
            "section_text": "\n## 5.3 Attention Mechanisms ##\n",
            "section_annotation": "Section-header",
            "section_page": 7,
            "section_id": 7,
            "section_image": "1604.00727_page_0007_mask_img_7.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                346,
                203,
                368
            ]
        },
        {
            "section_name": "Footnote_093",
            "section_text": "\n## λ(x).place of birth(Barack Obama, x) ##\n",
            "section_annotation": "Footnote",
            "section_page": 1,
            "section_id": 8,
            "section_image": "1604.00727_page_0001_mask_img_8.jpg",
            "section_column": 0,
            "section_im_bbox": [
                86,
                621,
                273,
                643
            ]
        },
        {
            "section_name": "Text_714",
            "section_text": "Fader et al., 2013). Semantic parsing frameworks for large-scale knowledge bases have to be able to successfully gen erate queries for the millions of entities and thou sands of predicates in the KB, many of which are unseen during training. To address this issue, recent work relies on producing embeddings for predicates and entities in a KB based on their textual descrip tions (Bordes et al., 2014a; Bordes et al., 2015; Yih et al., 2014; Yih et al., 2015). A general interaction function can then be used to measure the semantic relevance of these embedded KB entries to the ques",
            "section_annotation": "Text",
            "section_page": 2,
            "section_id": 8,
            "section_image": "1604.00727_page_0002_mask_img_8.jpg",
            "section_column": 1,
            "section_im_bbox": [
                296,
                549,
                533,
                703
            ]
        },
        {
            "section_name": "Text_715",
            "section_text": "As illustrated in Figure 1, our model consists of three components:",
            "section_annotation": "Text",
            "section_page": 3,
            "section_id": 8,
            "section_image": "1604.00727_page_0003_mask_img_8.jpg",
            "section_column": 1,
            "section_im_bbox": [
                296,
                245,
                528,
                277
            ]
        },
        {
            "section_name": "Text_716",
            "section_text": "where λ is a constant, h1, h2 are the hidden states of the LSTM at times t = 1 and t = 2, e1, ..., en are the entity embeddings, and p1, ..., pm are the predicate embeddings. A similar likelihood function was used to train the semantic similarity modules proposed in Yih et al. (2014) and Yih et al. (2015). During inference e1 e and p1 p are the",
            "section_annotation": "Text",
            "section_page": 5,
            "section_id": 8,
            "section_image": "1604.00727_page_0005_mask_img_8.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                396,
                298,
                484
            ]
        },
        {
            "section_name": "Section-header_091",
            "section_text": "\n## 5.2 Ablation and Embedding Experiments ##\n",
            "section_annotation": "Section-header",
            "section_page": 6,
            "section_id": 8,
            "section_image": "1604.00727_page_0006_mask_img_8.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                181,
                508,
                203
            ]
        },
        {
            "section_name": "Text_717",
            "section_text": "In order to further understand how the model per forms question answering, we visualize the attention distribution over question characters in the decoding process. In each sub-ﬁgure of Figure 2, the x-axis is the character sequence of the question, and the y axis is the attention weight distribution {αi}. The blue curve is the attention distribution when gener ating the entity, and green curve is the attention dis tribution when generating the predicate. Interestingly as the examples show the attention",
            "section_annotation": "Text",
            "section_page": 7,
            "section_id": 8,
            "section_image": "1604.00727_page_0007_mask_img_8.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                366,
                293,
                493
            ]
        },
        {
            "section_name": "Text_718",
            "section_text": "However, automatically mapping a natural language question such as “Where was Barack Obama born?” to its corresponding KB query remains a challenging task.",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 9,
            "section_image": "1604.00727_page_0001_mask_img_9.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                646,
                293,
                703
            ]
        },
        {
            "section_name": "List-item_135",
            "section_text": "1. A character-level LSTM-based encoder for the question which produces a sequence of embed ding vectors, one for each character (Figure 1a). 2. A character-level convolutional neural net work (CNN)-based encoder for the predi cates/entities in a knowledge base which pro duces a single embedding vector for each pred icate or entity (Figure 1c). 3. An LSTM-based decoder with an attention mechanism and a relevance function for gener ating the topic entity and predicate to form the KB query (Figure 1b).",
            "section_annotation": "List-item",
            "section_page": 3,
            "section_id": 9,
            "section_image": "1604.00727_page_0003_mask_img_9.jpg",
            "section_column": 1,
            "section_im_bbox": [
                306,
                276,
                528,
                473
            ]
        },
        {
            "section_name": "Text_719",
            "section_text": "In the following two sections, we will ﬁrst de scribe the LSTM decoder with attention, followed by the semantic relevance function.",
            "section_annotation": "Text",
            "section_page": 4,
            "section_id": 9,
            "section_image": "1604.00727_page_0004_mask_img_9.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                66,
                528,
                113
            ]
        },
        {
            "section_name": "Text_720",
            "section_text": "During inference, e1, ..., en and p1, ..., pm are the embeddings of candidate entities and predicates. During training e1, ..., en, p1, ..., pm are the embed dings of the true entity and 50 randomly-sampled entities, and the true predicate and 50 randomly sampled predicates, respectively.",
            "section_annotation": "Text",
            "section_page": 5,
            "section_id": 9,
            "section_image": "1604.00727_page_0005_mask_img_9.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                477,
                293,
                558
            ]
        },
        {
            "section_name": "Text_721",
            "section_text": "g p We carry out ablation studies in Sections 5.2.1 and 5.2.2 through a set of random-sampling experi ments. In these experiments, for each question, we randomly sample 200 entities and predicates from the test set as noise samples. We then mix the gold entity and predicate into these negative samples, and evaluate the accuracy of our model in predicting the gold predicate or entity from this mixed set.",
            "section_annotation": "Text",
            "section_page": 6,
            "section_id": 9,
            "section_image": "1604.00727_page_0006_mask_img_9.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                196,
                528,
                313
            ]
        },
        {
            "section_name": "Text_722",
            "section_text": "g g p Interestingly, as the examples show, the attention distribution typically peaks at empty spaces. This indicates that the character-level model learns that a space deﬁnes an ending point of a complete linguis tic unit. That is, the hidden state of the LSTM en coder at a space likely summarizes content about the character sequence before that space, and therefore contains important semantic information that the de coder needs to attend to. Also we observe that entity attention distribu",
            "section_annotation": "Text",
            "section_page": 7,
            "section_id": 9,
            "section_image": "1604.00727_page_0007_mask_img_9.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                486,
                298,
                612
            ]
        },
        {
            "section_name": "Text_723",
            "section_text": "Xiaodong He Microsoft Researc",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 10,
            "section_image": "1604.00727_page_0001_mask_img_10.jpg",
            "section_column": 1,
            "section_im_bbox": [
                353,
                126,
                433,
                150
            ]
        },
        {
            "section_name": "Text_724",
            "section_text": "The details of each component are described in the following sections.",
            "section_annotation": "Text",
            "section_page": 3,
            "section_id": 10,
            "section_image": "1604.00727_page_0003_mask_img_10.jpg",
            "section_column": 1,
            "section_im_bbox": [
                296,
                476,
                528,
                508
            ]
        },
        {
            "section_name": "Section-header_092",
            "section_text": "\n## 3.3.1 LSTM-based Decoder with Attention ##\n",
            "section_annotation": "Section-header",
            "section_page": 4,
            "section_id": 10,
            "section_image": "1604.00727_page_0004_mask_img_10.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                113,
                508,
                133
            ]
        },
        {
            "section_name": "Section-header_093",
            "section_text": "\n## 3.4 Inference ##\n",
            "section_annotation": "Section-header",
            "section_page": 5,
            "section_id": 10,
            "section_image": "1604.00727_page_0005_mask_img_10.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                561,
                143,
                578
            ]
        },
        {
            "section_name": "Section-header_094",
            "section_text": "\n## 5.2.1 Character-Level vs. Word-Level Models ##\n",
            "section_annotation": "Section-header",
            "section_page": 6,
            "section_id": 10,
            "section_image": "1604.00727_page_0006_mask_img_10.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                311,
                523,
                328
            ]
        },
        {
            "section_name": "Text_725",
            "section_text": "Also, we observe that entity attention distribu tions are usually less sharp and span longer portions of words, such as “john” or “rutters”, than predicate attention distributions (e.g., Figure 2a). For enti ties, semantic information may accumulate gradu ally when seeing more and more characters, while for predicates, semantic information will become",
            "section_annotation": "Text",
            "section_page": 7,
            "section_id": 10,
            "section_image": "1604.00727_page_0007_mask_img_10.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                605,
                293,
                703
            ]
        },
        {
            "section_name": "Text_726",
            "section_text": "Xiaodong He Microsoft Research",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 11,
            "section_image": "1604.00727_page_0001_mask_img_11.jpg",
            "section_column": 1,
            "section_im_bbox": [
                346,
                141,
                448,
                158
            ]
        },
        {
            "section_name": "Section-header_095",
            "section_text": "\n## 3.1 Encoding the Question ##\n",
            "section_annotation": "Section-header",
            "section_page": 3,
            "section_id": 11,
            "section_image": "1604.00727_page_0003_mask_img_11.jpg",
            "section_column": 1,
            "section_im_bbox": [
                296,
                511,
                438,
                528
            ]
        },
        {
            "section_name": "Text_727",
            "section_text": "The attention-based LSTM decoder uses a similar architecture as the one described in Bahdanau et al. (2015). At each time step i, we feed in a context vector ci and an input vector vi into the LSTM. At time i = 1 we feed a special input vector v<S> = ⃗0 into the LSTM. At time i = 2, during training, the input vector is the embedding of the true entity, while during testing, it is the embedding of the most likely entity as determined at the previous time step. We now describe how we produce the context",
            "section_annotation": "Text",
            "section_page": 4,
            "section_id": 11,
            "section_image": "1604.00727_page_0004_mask_img_11.jpg",
            "section_column": 1,
            "section_im_bbox": [
                296,
                131,
                533,
                257
            ]
        },
        {
            "section_name": "Text_728",
            "section_text": "For each question q, we generate a candidate set of entities and predicates, {e} and {p}, and feed it through the model f(q, {e}, {p}). We then decode the most likely (entity, predicate) pair:",
            "section_annotation": "Text",
            "section_page": 5,
            "section_id": 11,
            "section_image": "1604.00727_page_0005_mask_img_11.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                576,
                293,
                638
            ]
        },
        {
            "section_name": "Text_729",
            "section_text": "We ﬁrst explore using word-level models as an al ternative to character-level models to construct em beddings for questions, entities and predicates. B h d l l d h l l d l",
            "section_annotation": "Text",
            "section_page": 6,
            "section_id": 11,
            "section_image": "1604.00727_page_0006_mask_img_11.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                326,
                528,
                373
            ]
        },
        {
            "section_name": "Text_730",
            "section_text": "clear only after seeing the complete word. For ex ample, it may only be clear that characters such as “song by” refer to a predicate after a space, as op posed to the name of a song such as “song bye bye love” (Figures 2a, 2b). In contrast, a sequence of characters starts to become a likely entity after see ing an incomplete name such as “joh” or “rutt”.",
            "section_annotation": "Text",
            "section_page": 7,
            "section_id": 11,
            "section_image": "1604.00727_page_0007_mask_img_11.jpg",
            "section_column": 1,
            "section_im_bbox": [
                296,
                276,
                528,
                372
            ]
        },
        {
            "section_name": "Text_731",
            "section_text": "aohe@microsoft.c",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 12,
            "section_image": "1604.00727_page_0001_mask_img_12.jpg",
            "section_column": 1,
            "section_im_bbox": [
                342,
                156,
                452,
                173
            ]
        },
        {
            "section_name": "Text_732",
            "section_text": "To encode the question, we take two steps:",
            "section_annotation": "Text",
            "section_page": 3,
            "section_id": 12,
            "section_image": "1604.00727_page_0003_mask_img_12.jpg",
            "section_column": 1,
            "section_im_bbox": [
                296,
                526,
                488,
                548
            ]
        },
        {
            "section_name": "Text_733",
            "section_text": "y y p p We now describe how we produce the context vector ci. Let hi−1 be the hidden state of the LSTM at time i−1, sj be the jth question character embed ding, n be the number of characters in the question, r be the size of sj, and m be a hyperparameter. Then the context vector ci, which represents the attention weighted content of the question, is recomputed at each time step i as follows:",
            "section_annotation": "Text",
            "section_page": 4,
            "section_id": 12,
            "section_image": "1604.00727_page_0004_mask_img_12.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                250,
                528,
                361
            ]
        },
        {
            "section_name": "Text_734",
            "section_text": "(e∗, p∗) = argmaxei,pj(P(ei) ∗ P(pj))",
            "section_annotation": "Text",
            "section_page": 5,
            "section_id": 12,
            "section_image": "1604.00727_page_0005_mask_img_12.jpg",
            "section_column": 0,
            "section_im_bbox": [
                91,
                636,
                268,
                658
            ]
        },
        {
            "section_name": "Text_735",
            "section_text": "g q , p Both word-level and character-level models per form comparably well when predicting the predi cate, reaching an accuracy of around 80% (Table 3). However, the word-level model has considerable difﬁculty generalizing to unseen entities, and is only able to predict 45% of the entities accurately from the mixed set. These results clearly demonstrate that the OOV issue is much more severe for entities than predicates, and the difﬁculty word-level models have when generalizing to new entities.",
            "section_annotation": "Text",
            "section_page": 6,
            "section_id": 12,
            "section_image": "1604.00727_page_0006_mask_img_12.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                367,
                528,
                502
            ]
        },
        {
            "section_name": "Text_736",
            "section_text": "ing an incomplete name such as joh or rutt . In addition, a character-level model can identify entities whose English aliases were never seen in training, such as “phrenology” (Figure 2d). The model apparently learns that words ending with the sufﬁx “nology” are likely entity mentions, which is interesting because it reads in the input one character at a time.",
            "section_annotation": "Text",
            "section_page": 7,
            "section_id": 12,
            "section_image": "1604.00727_page_0007_mask_img_12.jpg",
            "section_column": 1,
            "section_im_bbox": [
                296,
                366,
                528,
                462
            ]
        },
        {
            "section_name": "Text_737",
            "section_text": "There are three key issues that make learning this mapping non-trivial. First, there are many para phrases of the same question. Second, many of the KB entries are unseen during training time; however, we still need to correctly predict them at test time. Third, a KB such as Freebase typically contains mil lions of entities and thousands of predicates, mak ing it difﬁcult for a system to predict these entities at scale (Yih et al., 2014; Fader et al., 2014; Bordes et al., 2015). In this paper, we address all three of these issues with a character-level encoder-decoder frame work that signiﬁcantly improves performance over state-of-the-art word-level neural models, while also providing a much more compact model that can be learned from less data.",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 13,
            "section_image": "1604.00727_page_0001_mask_img_13.jpg",
            "section_column": 1,
            "section_im_bbox": [
                296,
                243,
                533,
                443
            ]
        },
        {
            "section_name": "List-item_136",
            "section_text": "1. We ﬁrst extract one-hot encoding vectors for characters in the question, x1, ..., xn, where xi represents the one-hot encoding vector for the ith character in the question. We keep the space, punctuation and original cases without tokenization. 1",
            "section_annotation": "List-item",
            "section_page": 3,
            "section_id": 13,
            "section_image": "1604.00727_page_0003_mask_img_13.jpg",
            "section_column": 1,
            "section_im_bbox": [
                306,
                551,
                528,
                633
            ]
        },
        {
            "section_name": "Picture_021",
            "section_text": "ci = n � j=1 αijsj, αij = exp (eij) �Tx k=1 exp (eik) eij =v⊤ a tanh (Wahi−1 + Uasj) ,",
            "section_annotation": "Picture",
            "section_page": 4,
            "section_id": 13,
            "section_image": "1604.00727_page_0004_mask_img_13.jpg",
            "section_column": 1,
            "section_im_bbox": [
                336,
                361,
                488,
                488
            ]
        },
        {
            "section_name": "Text_738",
            "section_text": "which becomes our semantic parse.",
            "section_annotation": "Text",
            "section_page": 5,
            "section_id": 13,
            "section_image": "1604.00727_page_0005_mask_img_13.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                661,
                223,
                678
            ]
        },
        {
            "section_name": "Text_739",
            "section_text": "when generalizing to new entities. In contrast, character-level models have no such issues, and achieve a 96.6% accuracy in predicting the correct entity on the mixed set. This demon strates that character-level models encode the se mantic representation of entities and can match en tity aliases in a KB with their mentions in natural language questions.",
            "section_annotation": "Text",
            "section_page": 6,
            "section_id": 13,
            "section_image": "1604.00727_page_0006_mask_img_13.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                495,
                528,
                598
            ]
        },
        {
            "section_name": "Text_740",
            "section_text": "at a time. Furthermore, as observed in Figure 2d, the atten tion model is capable of attending disjoint regions of the question and capture the mention of a predicate that is interrupted by entity mentions. We also note that predicate attention often peaks at the padding symbols after the last character of the question, pos sibly because sentence endings carry extra informa tion that further help disambiguate predicate men tions. In certain scenarios, the network may only have sufﬁcient information to build a semantic rep resentation of the predicate after being ensured that it reached the end of a sentence. Finally certain words in the question help identify",
            "section_annotation": "Text",
            "section_page": 7,
            "section_id": 13,
            "section_image": "1604.00727_page_0007_mask_img_13.jpg",
            "section_column": 1,
            "section_im_bbox": [
                296,
                456,
                528,
                625
            ]
        },
        {
            "section_name": "Text_741",
            "section_text": "First, we use a long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) encoder to em bed the question. Second, to make our model ro bust to unseen KB entries, we extract embeddings for questions, predicates and entities purely from their character-level representations. Character level modeling has been previously shown to gen eralize well to new words not seen during training (Ljubeˇsi´c et al., 2014; Chung et al., 2016), which makes it ideal for this task. Third, to scale our model to handle the millions of entities and thousands of predicates in the KB, instead of using a large out put layer in the decoder to directly predict the entity and predicate, we use a general interaction function between the question embeddings and KB embed dings that measures their semantic relevance to de termine the output. The combined use of character level modeling and a semantic relevance function allows us to successfully produce likelihood scores",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 14,
            "section_image": "1604.00727_page_0001_mask_img_14.jpg",
            "section_column": 1,
            "section_im_bbox": [
                296,
                451,
                528,
                708
            ]
        },
        {
            "section_name": "List-item_137",
            "section_text": "1We notice that some entity mentions in the question are capitalized and some are not. Also, some questions end with a ’?’ and some do not.",
            "section_annotation": "List-item",
            "section_page": 3,
            "section_id": 14,
            "section_image": "1604.00727_page_0003_mask_img_14.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                631,
                528,
                668
            ]
        },
        {
            "section_name": "Text_742",
            "section_text": "where {α} is the attention distribution that is ap plied over each hidden unit sj, Wa ∈ Rm×N, Ua ∈ Rm×r, and va ∈ R1×m.",
            "section_annotation": "Text",
            "section_page": 4,
            "section_id": 14,
            "section_image": "1604.00727_page_0004_mask_img_14.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                491,
                528,
                538
            ]
        },
        {
            "section_name": "Text_743",
            "section_text": "which becomes our semantic parse. We use a similar procedure as the one described in Bordes et al. (2015) to generate candidate entities",
            "section_annotation": "Text",
            "section_page": 5,
            "section_id": 14,
            "section_image": "1604.00727_page_0005_mask_img_14.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                671,
                293,
                708
            ]
        },
        {
            "section_name": "Section-header_096",
            "section_text": "\n## 5.2.2 Depth Ablation Study ##\n",
            "section_annotation": "Section-header",
            "section_page": 6,
            "section_id": 14,
            "section_image": "1604.00727_page_0006_mask_img_14.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                596,
                438,
                617
            ]
        },
        {
            "section_name": "Text_744",
            "section_text": "Finally, certain words in the question help identify both the entity and the predicate. For example, con sider the word “university” in the question “What type of educational institution is eastern new mexico university” (Figure 2c). Although it is a part of the entity mention, it also helps disambiguate the predi",
            "section_annotation": "Text",
            "section_page": 7,
            "section_id": 14,
            "section_image": "1604.00727_page_0007_mask_img_14.jpg",
            "section_column": 1,
            "section_im_bbox": [
                296,
                618,
                528,
                703
            ]
        },
        {
            "section_name": "Section-header_097",
            "section_text": "\n## 3.3.2 Semantic Relevance Function ##\n",
            "section_annotation": "Section-header",
            "section_page": 4,
            "section_id": 15,
            "section_image": "1604.00727_page_0004_mask_img_15.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                537,
                473,
                558
            ]
        },
        {
            "section_name": "Text_745",
            "section_text": "{e} and predicates {p}. Namely, we take all entities whose English alias is a substring of the question, and remove all entities whose alias is a substring of another entity. For each English alias, we sort each entity with this alias by the number of facts that it has in the KB, and append the top 10 entities from this list to our set of candidate entities. All predi cates pj for each entity in our candidate entity set become the set of candidate predicates.",
            "section_annotation": "Text",
            "section_page": 5,
            "section_id": 15,
            "section_image": "1604.00727_page_0005_mask_img_15.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                261,
                528,
                387
            ]
        },
        {
            "section_name": "Text_746",
            "section_text": "p y We also study the impact of the depth of neural networks in our model. The results are presented in Table 2. In the ablation experiments we compare the performance of a single-layer LSTM to a two layer LSTM to encode the question, and a single layer vs. two-layer CNN to encode the KB entries.",
            "section_annotation": "Text",
            "section_page": 6,
            "section_id": 15,
            "section_image": "1604.00727_page_0006_mask_img_15.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                611,
                528,
                698
            ]
        },
        {
            "section_name": "Text_747",
            "section_text": "Unlike machine translation and language model ing where the vocabulary is relatively small, there are millions of entries in the KB. If we try to di rectly predict the KB entries, the decoder will need an output layer with millions of nodes, which is computationally prohibitive. Therefore, we resort to a relevance function that measures the semantic similarity between the decoder’s hidden state and the embeddings of KB entries. Our semantic rel evance function takes two vectors x1, x2 and re turns a distance measure of how similar they are to",
            "section_annotation": "Text",
            "section_page": 4,
            "section_id": 16,
            "section_image": "1604.00727_page_0004_mask_img_16.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                556,
                528,
                708
            ]
        },
        {
            "section_name": "Text_748",
            "section_text": "\n## 3.5 Learning ##\n",
            "section_annotation": "Text",
            "section_page": 5,
            "section_id": 16,
            "section_image": "1604.00727_page_0005_mask_img_16.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                386,
                373,
                408
            ]
        },
        {
            "section_name": "Text_749",
            "section_text": "Our goal in learning is to maximize the joint likeli hood P(ec)·P(pc) of predicting the correct entity ec and predicate pc pair from a set of randomly sampled entities and predicates. We use back-propagation to learn all of the weights in our model.",
            "section_annotation": "Text",
            "section_page": 5,
            "section_id": 17,
            "section_image": "1604.00727_page_0005_mask_img_17.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                403,
                528,
                475
            ]
        },
        {
            "section_name": "Text_750",
            "section_text": "learn all of the weights in our model. All the parameters of our model are learned jointly without pre-training. These parameters in clude the weights of the character-level embeddings, CNNs, and LSTMs. Weights are randomly initial ized before training. For the ith layer in our network, each weight is sampled from a uniform distribution between − 1 |li| and 1 |li|, where |li| is the number of weights in layer i.",
            "section_annotation": "Text",
            "section_page": 5,
            "section_id": 18,
            "section_image": "1604.00727_page_0005_mask_img_18.jpg",
            "section_column": 1,
            "section_im_bbox": [
                296,
                468,
                528,
                583
            ]
        },
        {
            "section_name": "Section-header_098",
            "section_text": "\n## 4 Dataset and Experimental Settings ##\n",
            "section_annotation": "Section-header",
            "section_page": 5,
            "section_id": 19,
            "section_image": "1604.00727_page_0005_mask_img_19.jpg",
            "section_column": 1,
            "section_im_bbox": [
                296,
                586,
                498,
                608
            ]
        },
        {
            "section_name": "Text_751",
            "section_text": "We evaluate the proposed model on the SimpleQues tions dataset (Bordes et al., 2015). The dataset con sists of 108,442 single-relation questions and their corresponding (topic entity, predicate, answer en tity) triples from Freebase. It is split into 75,910 train, 10,845 validation, and 21,687 test questions. Only 10,843 of the 45,335 unique words in entity",
            "section_annotation": "Text",
            "section_page": 5,
            "section_id": 20,
            "section_image": "1604.00727_page_0005_mask_img_20.jpg",
            "section_column": 1,
            "section_im_bbox": [
                296,
                606,
                528,
                707
            ]
        }
    ]
}