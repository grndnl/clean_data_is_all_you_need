{
    "paper_id": "1603.01514.pdf",
    "title": "",
    "paper_text": [
        {
            "section_name": "Title_020",
            "section_text": "\n## A Bayesian Model of Multilingual Unsupervised Semantic Role Induction ##\n",
            "section_annotation": "Title",
            "section_page": 1,
            "section_id": 0,
            "section_image": "1603.01514_page_0001_mask_img_0.jpg",
            "section_column": 0,
            "section_im_bbox": [
                171,
                61,
                413,
                103
            ]
        },
        {
            "section_name": "Section-header_028",
            "section_text": "\n## 2 Unsupervised SRL Pipeline ##\n",
            "section_annotation": "Section-header",
            "section_page": 2,
            "section_id": 0,
            "section_image": "1603.01514_page_0002_mask_img_0.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                56,
                228,
                78
            ]
        },
        {
            "section_name": "Formula_009",
            "section_text": "P(r, f|p, vc) = P(o|p, vc) � �� � � ri∈r∩PR P(fi|ri, p) � I∈o P(r(I), f(I)|I, p) (1)",
            "section_annotation": "Formula",
            "section_page": 3,
            "section_id": 0,
            "section_image": "1603.01514_page_0003_mask_img_0.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                72,
                518,
                96
            ]
        },
        {
            "section_name": "Unkown_014",
            "section_text": "",
            "section_annotation": "Unkown",
            "section_page": 4,
            "section_id": 0,
            "section_image": "1603.01514_page_0004_mask_img_0.jpg",
            "section_column": 0,
            "section_im_bbox": [
                91,
                56,
                488,
                208
            ]
        },
        {
            "section_name": "Text_335",
            "section_text": "For sampling roles in the multilingual model, we also need to consider the probabilities of roles being generated by the CLVs:",
            "section_annotation": "Text",
            "section_page": 5,
            "section_id": 0,
            "section_image": "1603.01514_page_0005_mask_img_0.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                56,
                288,
                107
            ]
        },
        {
            "section_name": "Text_336",
            "section_text": "gument identiﬁcation, which we brieﬂy describe here. The EN sentences are parsed syntactically using MaltParser (Nivre et al., 2007) and DE using LTH parser (Johansson and Nugues, 2008). All the non-auxiliary verbs are selected as predicates. In CoNLL data, this gives us about 3k EN and 500 DE predicates. The total number of predicate in stances are 3.4M in EN (89k CoNLL + 3.3M Eu roparl) and 2.62M in DE (17k CoNLL + 2.6M Eu roparl). The arguments for EN are identiﬁed us ing the heuristics proposed by Lang and Lapata (2011a). However, we get an F1 score of 85.1% for argument identiﬁcation on CoNLL 2009 EN data as opposed to 80.7% reported by Titov and Klementiev (2012b). This could be due to imple mentation differences, which unfortunately makes our EN results incomparable. For DE, the argu ments are identiﬁed using the LTH system (Jo hansson and Nugues, 2008), which gives an F1 score of 86.5% on the CoNLL 2009 DE data. The word alignments for the EN-DE parallel Eu roparl corpus are computed using GIZA++ (Och and Ney, 2003). For high-precision, only the inter secting alignments in the two directions are kept. We deﬁne two semantic arguments as aligned if their head-words are aligned. In total we get 9.3M arguments for EN (240k CoNLL + 9.1M Europarl) and 4.43M for DE (32k CoNLL + 4.4M Europarl). Out of these, 0.76M arguments are aligned.",
            "section_annotation": "Text",
            "section_page": 6,
            "section_id": 0,
            "section_image": "1603.01514_page_0006_mask_img_0.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                58,
                288,
                448
            ]
        },
        {
            "section_name": "Table_023",
            "section_text": "Dataset Model English German Training Testing PU CO F1 PU CO F1 0 CoNLL CoNLL Baseline 78.23 79.46 78.84 83.09 79.32 81.16 1 CoNLL CoNLL Monolingual 76.29 83.13 79.56* 82.54 81.94 82.24* 2 CoNLL+EP CoNLL Monolingual 76.11 83.33 79.56 83.77 81.65 82.70* 3 2×(CoNLL+EP) CoNLL Multilingual 76.23 83.25 79.59 83.81 81.79 82.79 4 EP CoNLL Monolingual 73.26 80.60 76.76 83.72 81.28 82.48 5 2×EP CoNLL Multilingual 73.07 81.24 76.94 83.59 81.50 82.54",
            "section_annotation": "Table",
            "section_page": 7,
            "section_id": 0,
            "section_image": "1603.01514_page_0007_mask_img_0.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                56,
                513,
                173
            ]
        },
        {
            "section_name": "Text_337",
            "section_text": "tion with S. We make the following observations:",
            "section_annotation": "Text",
            "section_page": 8,
            "section_id": 0,
            "section_image": "1603.01514_page_0008_mask_img_0.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                56,
                283,
                78
            ]
        },
        {
            "section_name": "Table_024",
            "section_text": "References [F¨urstenau and Lapata2009] H. F¨urstenau and M. Lap ata. 2009. Graph alignment for semi-supervised se mantic role labeling. In Proceedings of the 2009 Conference on Empirical Methods in Natural Lan guage Processing: Volume 1-Volume 1, pages 11– 20. Association for Computational Linguistics. [Garg and Henderson2012] N. Garg and J. Henderson. 2012. Unsupervised semantic role induction with global role ordering. In Proceedings of the 50th An nual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics. [Gildea and Jurafsky2002] D. Gildea and D. Jurafsky. 2002. Automatic labeling of semantic roles. Com putational Linguistics, 28(3):245–288. [Grenager and Manning2006] T. Grenager and C.D. Manning. 2006. Unsupervised discovery of a sta tistical verb lexicon. In Proceedings of the 2006 Conference on Empirical Methods in Natural Lan guage Processing, pages 1–8. Association for Com putational Linguistics. [Hajiˇc et al.2009] J. Hajiˇc, M. Ciaramita, R. Johansson, D. Kawahara, M.A. Mart´ı, L. M`arquez, A. Meyers, J. Nivre, S. Pad´o, J. ˇStˇep´anek, et al. 2009. The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the Thirteenth Conference on Computational Nat ural Language Learning: Shared Task, pages 1–18. Association for Computational Linguistics. [Johansson and Nugues2008] R. Johansson and P. Nugues. 2008. Dependency-based semantic role labeling of propbank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 69–78. Association for Computational Linguistics. [Koehn2005] P. Koehn. 2005. Europarl: A parallel cor pus for statistical machine translation. In MT sum mit, volume 5. [Lang and Lapata2011a] J. Lang and M. Lapata. 2011a. Unsupervised semantic role induction via split merge clustering. In Proceedings of the 49th Annual Meeting of the Association for Computational Lin guistics, Portland, Oregon. [Lang and Lapata2011b] J. Lang and M. Lapata. 2011b. Unsupervised semantic role induction with graph partitioning. In Proceedings of the 2011 Conference on Empirical Methods in Natural Lan guage Processing, pages 1320–1331, Edinburgh, Scotland, UK., July. Association for Computational Linguistics. [M`arquez et al.2008] L. M`arquez, X. Carreras, K.C. Litkowski, and S. Stevenson. 2008. Semantic role labeling: an introduction to the special issue. Com putational linguistics, 34(2):145–159. [Naseem et al.2009] T. Naseem, B. Snyder, J. Eise stein, and R. Barzilay. 2009. Multilingu part-of-speech tagging: Two unsupervised a proaches. Journal of Artiﬁcial Intelligence R search, 36(1):341–385. [Nivre et al.2007] J. Nivre, J. Hall, J. Nilsso A. Chanev, G. Eryigit, S. Kubler, S. Marin and E. Marsi. 2007. Maltparser: A languag independent system for data-driven dependen parsing. Natural Language Engineering, 13(2):95 [Och and Ney2003] F.J. Och and H. Ney. 2003. A sy tematic comparison of various statistical alignme models. Computational linguistics, 29(1):19–51. [Pad´o and Lapata2009] S. Pad´o and M. Lapata. 200 Cross-lingual annotation projection for seman roles. Journal of Artiﬁcial Intelligence Resear 36(1):307–340. [Palmer et al.2005] M. Palmer, D. Gildea, and P. King bury. 2005. The proposition bank: An annotat corpus of semantic roles. Computational Lingu tics, 31(1):71–106. [Pitman2002] J. Pitman. 2002. Combinatorial stoch tic processes. Technical report, Technical Rep 621, Dept. Statistics, UC Berkeley, 2002. Lectu notes for St. Flour course. [Pradhan et al.2005] S. Pradhan, K. Haciog V. Krugler, W. Ward, J.H. Martin, and D. J rafsky. 2005. Support vector learning for seman argument classiﬁcation. Machine Learnin 60(1):11–39. [Punyakanok et al.2004] V. Punyakanok, D. Ro W. Yih, and D. Zimak. 2004. Semantic role lab ing via integer linear programming inference. Proceedings of the 20th international conference Computational Linguistics, page 1346. Associati for Computational Linguistics. [Snyder et al.2009] B. Snyder, T. Naseem, and R. Bar lay. 2009. Unsupervised multilingual grammar duction. In Proceedings of the Joint Conferen of the 47th Annual Meeting of the ACL and t 4th International Joint Conference on Natural La guage Processing of the AFNLP: Volume 1-Volum 1, pages 73–81. Association for Computational L guistics. [Swier and Stevenson2004] R. Swier and S. Stevenso 2004. Unsupervised semantic role labelling. In P ceedings of the 2004 Conference on Empirical Me ods in Natural Language Processing, pages 95–10 [Titov and Klementiev2012a] I. Titov and A. Kleme tiev. 2012a. A bayesian approach to unsupervis semantic role induction. In Proceedings of the Co ference of the European Chapter of the Associati for Computational Linguistics, page 12.",
            "section_annotation": "Table",
            "section_page": 9,
            "section_id": 0,
            "section_image": "1603.01514_page_0009_mask_img_0.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                56,
                500,
                748
            ]
        },
        {
            "section_name": "Text_338",
            "section_text": "Nikhil Garg i it f G",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 1,
            "section_image": "1603.01514_page_0001_mask_img_1.jpg",
            "section_column": 0,
            "section_im_bbox": [
                156,
                111,
                228,
                133
            ]
        },
        {
            "section_name": "Text_339",
            "section_text": "As established in previous work (Gildea and Juraf sky, 2002; Pradhan et al., 2005), we use a standard unsupervised SRL setup, consisting of the follow ing steps:",
            "section_annotation": "Text",
            "section_page": 2,
            "section_id": 1,
            "section_image": "1603.01514_page_0002_mask_img_1.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                76,
                288,
                136
            ]
        },
        {
            "section_name": "Picture_009",
            "section_text": "� �� � o=ordering(r) � ri∈r∩PR � �� � Primary Roles � I∈o � �� � Intervals where P(r(I), f(I)|I, p) = � ri∈r(I) P(¬stop|I, p, adj) � �� � generate indicator P(ri|I, p) � �� � generate SR P(fi|ri, p) � �� � generate features P(stop|I, p, adj) � �� � end of the interval (2) and P(fi|ri, p) = T � t=1 P(fi,t|ri, p) (3) P(f|p, vc) = � r P(r, f|p, vc) (4) (a) Probability equations for the monolingual model. Bold-faced variables denote a sequence of values. r denotes the complete sequence of roles, and f denotes the complete sequence of features. p and vc denote the predicate and its voice respectively. o denotes the ordering of PRs in the sequence r and ordering(r) is a function for computing this ordering. ri and fi denote the role and features at position i respectively, and r(I) and f(I) respectively denote the SR sequence and feature sequence in interval I. fi,t denotes the value of feature t at position i. adj = 0 for generating the ﬁrst SR, and 1 for a subsequent one. Equation 1 gives the joint probability of the model and equation 4 gives the marginal probability of the observed features. P(rl1, fl1, rl2, fl2, z|pl1, vcl1, pl2, vcl2) = P(z) � l∈{l1,l2} P(rl, fl|z, pl, vcl) (5) ≈ P(z) � l∈{l1,l2} P(rl, fl|pl, vcl) � i,k:zk→rl i P(rl i|zk) (6) (a) Probability equations for the multilingual model. The superscript l denotes the variable for language l. z denotes the common crosslingual latent variables for both languages. zk → rl i denotes that the argument at position i in language l is t d t th li l l t t i bl #k",
            "section_annotation": "Picture",
            "section_page": 3,
            "section_id": 1,
            "section_image": "1603.01514_page_0003_mask_img_1.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                89,
                518,
                378
            ]
        },
        {
            "section_name": "Caption_026",
            "section_text": "Figure 4: Multilingual model. The CLVs and their associated parameters are drawn in bold. PR3 in language 1 is aligned to PR3 in language 2 with the corresponding CLV taking the value c2, and SRM is aligned to PR2 with the CLV taking the value c7.",
            "section_annotation": "Caption",
            "section_page": 4,
            "section_id": 1,
            "section_image": "1603.01514_page_0004_mask_img_1.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                211,
                518,
                258
            ]
        },
        {
            "section_name": "Formula_010",
            "section_text": "P(ri|r−i,f,p,vc,z,D−)∝P(ri,r−i,f|z,p,vc,D−) = � P(ri,r−i,f|θ,z,p,vc)P(θ|D−)dθ = � P(ri,r−i,f|θ,p,vc)(� P(rj|θ,zk) j,k:zk→rj )P(θ|D−)dθ",
            "section_annotation": "Formula",
            "section_page": 5,
            "section_id": 1,
            "section_image": "1603.01514_page_0005_mask_img_1.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                101,
                248,
                163
            ]
        },
        {
            "section_name": "Section-header_029",
            "section_text": "\n## 6.5 Main Results ##\n",
            "section_annotation": "Section-header",
            "section_page": 6,
            "section_id": 1,
            "section_image": "1603.01514_page_0006_mask_img_1.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                456,
                158,
                473
            ]
        },
        {
            "section_name": "Caption_027",
            "section_text": "Table 1: Main Results. A * denotes a signiﬁcant improvement in the F1 score over the the previous line. We compute the signiﬁcance using stratiﬁed shufﬂing and consider it signiﬁcant if the p-value < 0.05.",
            "section_annotation": "Caption",
            "section_page": 7,
            "section_id": 1,
            "section_image": "1603.01514_page_0007_mask_img_1.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                167,
                513,
                203
            ]
        },
        {
            "section_name": "List-item_074",
            "section_text": "• In both languages, at around S = 10, the su pervised baseline starts outperforming the semi supervised model, which suggests that manu ally labeling about 10% of the sentences is a good enough alternative to our training proce dure. Note that 10% amounts to about 3.6k sen tences in German and 4k in English. We noticed that the proportion of seen predicates increases dramatically as we increase the proportion of supervised sentences. At 10% supervised sen tences, the model has already seen 63% of pred icates in German and 44% in English. This explains to some extent why only 10% labeled sentences are enough. • For German, it takes about 3.5% or 1260 super vised sentences to have the same performance increase as 1.5M unlabeled sentences (Line 1 to Line 2 in Table 1). Adding about 180 more su pervised sentences also covers the beneﬁt ob tained by alignments in the multilingual model (Line 2 to Line 3 in Table 1). There is no no ticeable performance difference in English.",
            "section_annotation": "List-item",
            "section_page": 8,
            "section_id": 1,
            "section_image": "1603.01514_page_0008_mask_img_1.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                81,
                288,
                388
            ]
        },
        {
            "section_name": "Text_340",
            "section_text": "\n## References ##\n",
            "section_annotation": "Text",
            "section_page": 9,
            "section_id": 1,
            "section_image": "1603.01514_page_0009_mask_img_1.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                57,
                128,
                78
            ]
        },
        {
            "section_name": "List-item_075",
            "section_text": "[Titov and Klementiev2012b] I. Titov and A. Klemen tiev. 2012b. Crosslingual induction of semantic roles. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics. As sociation for Computational Linguistics. [Toutanova et al.2008] K. Toutanova, A. Haghighi, and C.D. Manning. 2008. A global joint model for semantic role labeling. Computational Linguistics, 34(2):161–191.",
            "section_annotation": "List-item",
            "section_page": 10,
            "section_id": 1,
            "section_image": "1603.01514_page_0010_mask_img_1.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                58,
                288,
                173
            ]
        },
        {
            "section_name": "Text_341",
            "section_text": "ing steps: 1. Syntactic Parsing Off-the-shelf parsers can be used to syntactically parse a given sentence. We use a dependency parse because of its simplicity and easier comparison with the previous work in unsupervised SRL. 2. Predicate Identiﬁcation We select all the non auxiliary verbs in a sentence as predicates. 3. Argument Identiﬁcation For a given predi cate, this step classiﬁes each constituent of the parse tree as a semantic argument or a non argument. Heuristics based on syntactic features such as the dependency relation of a constituent to its head, path from the constituent to the pred icate, etc. have been used in unsupervised SRL. 4. Argument Classiﬁcation Without access to semantic role labels, unsupervised SRL systems cast the problem as a clustering problem. Argu ments of a predicate in all the sentences are di vided into clusters such that each cluster corre sponds to a single semantic role. The better this clustering is, the easier it becomes for a human to give it an actual semantic role label like A0, A1, etc. Our model assigns a role variable to ev ery identiﬁed argument. This variable can take any value from 1 to N, where N is the number of semantic roles that we want to induce. The task we model, unsupervised semantic role in duction is the step 4 of this pipeline",
            "section_annotation": "Text",
            "section_page": 2,
            "section_id": 2,
            "section_image": "1603.01514_page_0002_mask_img_2.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                129,
                288,
                522
            ]
        },
        {
            "section_name": "Text_342",
            "section_text": "ample, a value c for the CLV z might give high probabilities to S3 and S8 in language 1, and to S1 in language 2. If c is the only value for z that gives high probability to S3 in language 1, and the monolingual model in language 1 decides to as sign S3 to the role for z, then z will predict S1 in language 2, with high probability. We generate the CLVs via a Chinese Restaurant Process (Pitman, 2002), a non-parametric Bayesian model, which allows us to induce the number of CLVs for every predicate-tuple from the data. We continue to train on the non-parallel sentences using the respective monolingual models.",
            "section_annotation": "Text",
            "section_page": 4,
            "section_id": 2,
            "section_image": "1603.01514_page_0004_mask_img_2.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                261,
                288,
                440
            ]
        },
        {
            "section_name": "Text_343",
            "section_text": "For sampling CLVs, we need to consider three factors: two corresponding to probabilities of gen erating the aligned roles, and the third one corre sponding to selecting the CLV according to CRP.",
            "section_annotation": "Text",
            "section_page": 5,
            "section_id": 2,
            "section_image": "1603.01514_page_0005_mask_img_2.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                166,
                288,
                223
            ]
        },
        {
            "section_name": "Text_344",
            "section_text": "Since the CoNLL annotations have 21 semantic roles in total, we use 21 roles in our model as well as the baseline. Following Garg and Henderson (2012), we set the number of PRs to 2 (excluding START, END and PRED), and SRs to 21-2=19. Table 1 shows the results.",
            "section_annotation": "Text",
            "section_page": 6,
            "section_id": 2,
            "section_image": "1603.01514_page_0006_mask_img_2.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                476,
                288,
                562
            ]
        },
        {
            "section_name": "Table_025",
            "section_text": "Source Target English German PU CO F1 PU CO F1 1 Multilingual Model 76.23 83.25 79.59 83.81 81.79 82.79 2 English German NA 83.83 81.83 82.82 3 German English 76.26 83.37 79.66 NA",
            "section_annotation": "Table",
            "section_page": 7,
            "section_id": 2,
            "section_image": "1603.01514_page_0007_mask_img_2.jpg",
            "section_column": 0,
            "section_im_bbox": [
                121,
                203,
                458,
                279
            ]
        },
        {
            "section_name": "Text_345",
            "section_text": "We also evaluated the performance variation on a completely unseen CoNLL test set. Since the test set is very small compared to the training set, the clustering evaluation is not as reliable. Nonethe less, we broadly obtained the same pattern.",
            "section_annotation": "Text",
            "section_page": 8,
            "section_id": 2,
            "section_image": "1603.01514_page_0008_mask_img_2.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                391,
                288,
                467
            ]
        },
        {
            "section_name": "Text_346",
            "section_text": "N Ga g University of Geneva Switzerland nikgarg@gmail com",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 3,
            "section_image": "1603.01514_page_0001_mask_img_3.jpg",
            "section_column": 0,
            "section_im_bbox": [
                136,
                126,
                248,
                163
            ]
        },
        {
            "section_name": "Text_347",
            "section_text": "duction, is the step 4 of this pipeline.",
            "section_annotation": "Text",
            "section_page": 2,
            "section_id": 3,
            "section_image": "1603.01514_page_0002_mask_img_3.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                515,
                233,
                533
            ]
        },
        {
            "section_name": "Text_348",
            "section_text": "g The multilingual model is deﬁcient, since the aligned roles are being generated twice. Ideally, we would like to add the CLV as additional condi tioning variables in the monolingual models. The new joint probability can be written as equation 5 (Figure 2a), which can be further decomposed following the decomposition of the monolingual model in Figure 1a. However, having this addi tional conditioning variable breaks the Dirichlet multinomial conjugacy, which makes it intractable to marginalize out the parameters during infer ence. Hence, we use an approximation where we treat each of the aligned roles as being generated twice, once by the monolingual model and once by the corresponding CLV (equation 6).",
            "section_annotation": "Text",
            "section_page": 4,
            "section_id": 3,
            "section_image": "1603.01514_page_0004_mask_img_3.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                436,
                288,
                642
            ]
        },
        {
            "section_name": "Formula_011",
            "section_text": "\n## P(zk|rl1 i ,rl2 j ,D−,k)∝P(rl1 i |zk,D−,k)P(rl2 j |zk,D−,k)P(zk|D−,k) ##\n",
            "section_annotation": "Formula",
            "section_page": 5,
            "section_id": 3,
            "section_image": "1603.01514_page_0005_mask_img_3.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                236,
                296,
                258
            ]
        },
        {
            "section_name": "Text_349",
            "section_text": "In the ﬁrst setting (Line 1), we train and test the monolingual model on the CoNLL data. We ob serve signiﬁcant improvements in F1 score over the Baseline (Line 0) in both languages. Using the CoNLL 2009 dataset alone, Titov and Klementiev (2012b) report an F1 score of 80.9% (PU=86.8%, CO=75.7%) for German. Thus, our monolingual model outperforms their monolingual model in German. For English, they report an F1 score of 83.6% (PU=87.5%, CO=80.1%), but note that our English results are not directly comparable to theirs due to differences argument identiﬁcation, as discussed in section 6.4. As their argument identiﬁcation score is lower, perhaps their system",
            "section_annotation": "Text",
            "section_page": 6,
            "section_id": 3,
            "section_image": "1603.01514_page_0006_mask_img_3.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                556,
                288,
                748
            ]
        },
        {
            "section_name": "Section-header_030",
            "section_text": "\n## 7 Related Work ##\n",
            "section_annotation": "Section-header",
            "section_page": 8,
            "section_id": 3,
            "section_image": "1603.01514_page_0008_mask_img_3.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                466,
                163,
                488
            ]
        },
        {
            "section_name": "Text_350",
            "section_text": "nikgarg@gmail.com",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 4,
            "section_image": "1603.01514_page_0001_mask_img_4.jpg",
            "section_column": 0,
            "section_im_bbox": [
                131,
                156,
                258,
                173
            ]
        },
        {
            "section_name": "Section-header_031",
            "section_text": "\n## 3 Monolingual Model ##\n",
            "section_annotation": "Section-header",
            "section_page": 2,
            "section_id": 4,
            "section_image": "1603.01514_page_0002_mask_img_4.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                532,
                188,
                553
            ]
        },
        {
            "section_name": "Text_351",
            "section_text": "common crosslingual latent variables for both la connected to the crosslingual latent variable #k.",
            "section_annotation": "Text",
            "section_page": 3,
            "section_id": 4,
            "section_image": "1603.01514_page_0003_mask_img_4.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                371,
                243,
                388
            ]
        },
        {
            "section_name": "Text_352",
            "section_text": "y p g q This is the ﬁrst work to incorporate the cou pling of aligned arguments directly in a Bayesian SRL model. This makes it easier to see how to extend this model in a principled way to incor porate additional sources of information. First, the model scales gracefully to more than two lan guages. If there are a total of n languages, and there is an aligned argument in m of them, the",
            "section_annotation": "Text",
            "section_page": 4,
            "section_id": 4,
            "section_image": "1603.01514_page_0004_mask_img_4.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                636,
                288,
                748
            ]
        },
        {
            "section_name": "Text_353",
            "section_text": "where the aligned roles rl1 i and rl2 j are connected to zk, and D−,k refers to all the variables except zk, rl1 i , and rl2 j .",
            "section_annotation": "Text",
            "section_page": 5,
            "section_id": 4,
            "section_image": "1603.01514_page_0005_mask_img_4.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                256,
                288,
                304
            ]
        },
        {
            "section_name": "Text_354",
            "section_text": "is discarding “difﬁcult” arguments which leads to a higher clustering score. I h d i (Li 2) h d",
            "section_annotation": "Text",
            "section_page": 6,
            "section_id": 4,
            "section_image": "1603.01514_page_0006_mask_img_4.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                56,
                518,
                93
            ]
        },
        {
            "section_name": "Caption_028",
            "section_text": "Table 2: Results for the Multilingual Model with using labeled data for the source language.",
            "section_annotation": "Caption",
            "section_page": 7,
            "section_id": 4,
            "section_image": "1603.01514_page_0007_mask_img_4.jpg",
            "section_column": 0,
            "section_im_bbox": [
                91,
                276,
                488,
                297
            ]
        },
        {
            "section_name": "Text_355",
            "section_text": "As discussed in section 6.3, our work is closely re lated to the crosslingual unsupervised SRL work of Titov and Klementiev (2012b). The idea of us ing superlingual latent variables to capture cross lingual information was proposed for POS tagging by Naseem et al. (2009), which we use here for SRL. In a semi-supervised setting, Pad´o and Lap ata (2009) used a graph based approach to trans fer semantic role annotations from English to Ger man. F¨urstenau and Lapata (2009) used a graph alignment method to measure the semantic and syntactic similarity between dependency tree ar guments of known and unknown verbs.",
            "section_annotation": "Text",
            "section_page": 8,
            "section_id": 4,
            "section_image": "1603.01514_page_0008_mask_img_4.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                491,
                288,
                668
            ]
        },
        {
            "section_name": "Table_026",
            "section_text": "ata. 2009. Graph alignment for semi-supervised se mantic role labeling. In Proceedings of the 2009 Conference on Empirical Methods in Natural Lan guage Processing: Volume 1-Volume 1, pages 11– 20. Association for Computational Linguistics. [Garg and Henderson2012] N. Garg and J. Henderson. 2012. Unsupervised semantic role induction with global role ordering. In Proceedings of the 50th An nual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics. [Gildea and Jurafsky2002] D. Gildea and D. Jurafsky. 2002. Automatic labeling of semantic roles. Com putational Linguistics, 28(3):245–288. [Grenager and Manning2006] T. Grenager and C.D. Manning. 2006. Unsupervised discovery of a sta tistical verb lexicon. In Proceedings of the 2006 Conference on Empirical Methods in Natural Lan guage Processing, pages 1–8. Association for Com putational Linguistics. [Hajiˇc et al.2009] J. Hajiˇc, M. Ciaramita, R. Johansson, D. Kawahara, M.A. Mart´ı, L. M`arquez, A. Meyers, J. Nivre, S. Pad´o, J. ˇStˇep´anek, et al. 2009. The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the Thirteenth Conference on Computational Nat ural Language Learning: Shared Task, pages 1–18. Association for Computational Linguistics. [Johansson and Nugues2008] R. Johansson and P. Nugues. 2008. Dependency-based semantic role labeling of propbank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 69–78. Association for Computational Linguistics. [Koehn2005] P. Koehn. 2005. Europarl: A parallel cor pus for statistical machine translation. In MT sum mit, volume 5. [Lang and Lapata2011a] J. Lang and M. Lapata. 2011a. Unsupervised semantic role induction via split merge clustering. In Proceedings of the 49th Annual Meeting of the Association for Computational Lin guistics, Portland, Oregon. [Lang and Lapata2011b] J. Lang and M. Lapata. 2011b. Unsupervised semantic role induction with graph partitioning. In Proceedings of the 2011 Conference on Empirical Methods in Natural Lan guage Processing, pages 1320–1331, Edinburgh, Scotland, UK., July. Association for Computational Linguistics. [M`arquez et al.2008] L. M`arquez, X. Carreras, K.C. Litkowski, and S. Stevenson. 2008. Semantic role labeling: an introduction to the special issue. Com putational linguistics, 34(2):145–159.",
            "section_annotation": "Table",
            "section_page": 9,
            "section_id": 4,
            "section_image": "1603.01514_page_0009_mask_img_4.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                98,
                288,
                744
            ]
        },
        {
            "section_name": "Section-header_032",
            "section_text": "\n## Abstract ##\n",
            "section_annotation": "Section-header",
            "section_page": 1,
            "section_id": 5,
            "section_image": "1603.01514_page_0001_mask_img_5.jpg",
            "section_column": 0,
            "section_im_bbox": [
                146,
                196,
                203,
                213
            ]
        },
        {
            "section_name": "Caption_029",
            "section_text": "Figure 3: Probability equations for the (a) monolingual and (b) multilingual model.",
            "section_annotation": "Caption",
            "section_page": 3,
            "section_id": 5,
            "section_image": "1603.01514_page_0003_mask_img_5.jpg",
            "section_column": 0,
            "section_im_bbox": [
                106,
                386,
                473,
                403
            ]
        },
        {
            "section_name": "Text_356",
            "section_text": "multilingual latent variable is connected to only those m aligned arguments.",
            "section_annotation": "Text",
            "section_page": 4,
            "section_id": 5,
            "section_image": "1603.01514_page_0004_mask_img_5.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                261,
                518,
                294
            ]
        },
        {
            "section_name": "Text_357",
            "section_text": "zk, ri , and rj . We use the trained parameters to parse the monolingual data using the monolingual model. The crosslingual parameters are ignored even if they were used during training. Thus, the infor mation coming from the CLVs acts as a regularizer for the monolingual models.",
            "section_annotation": "Text",
            "section_page": 5,
            "section_id": 5,
            "section_image": "1603.01514_page_0005_mask_img_5.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                297,
                288,
                388
            ]
        },
        {
            "section_name": "Text_358",
            "section_text": "from a resource rich language to a resource poor language. We evaluated our model in a very general annotation transfer scenario, where we have a small labeled dataset for one language (source), and a large parallel unlabeled dataset for the source and another (target) language. We in vestigate whether this setting improves the param eter estimates for the target language. To this end, we clamp the role annotations of the source lan guage in the CoNLL dataset using a predeﬁned mapping1, and do not sample them during train ing. This data gives us good parameters for the source language, which are used to sample the roles of the source language in the unlabeled Eu roparl data. The CLVs aim to capture this im provement and thereby improve sampling and pa rameter estimates for the target language. Table 2 shows the results of this experiment. We obtain small improvements in the target languages. As in the unsupervised setting, the small percentage of aligned roles probably limits the impact of the cross-lingual information.",
            "section_annotation": "Text",
            "section_page": 7,
            "section_id": 5,
            "section_image": "1603.01514_page_0007_mask_img_5.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                306,
                288,
                602
            ]
        },
        {
            "section_name": "List-item_076",
            "section_text": "g For monolingual unsupervised SRL, Swier and Stevenson (2004) presented the ﬁrst work on a domain-general corpus, the British National Cor pus, using 54 verbs taken from VerbNet. Garg and Henderson (2012) proposed a Bayesian model for this problem that we use here. Titov and",
            "section_annotation": "List-item",
            "section_page": 8,
            "section_id": 5,
            "section_image": "1603.01514_page_0008_mask_img_5.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                663,
                288,
                750
            ]
        },
        {
            "section_name": "Text_359",
            "section_text": "We propose a Bayesian model of unsuper vised semantic role induction in multiple languages, and use it to explore the useful ness of parallel corpora for this task. Our joint Bayesian model consists of individ ual models for each language plus addi tional latent variables that capture align ments between roles across languages. Because it is a generative Bayesian model, we can do evaluations in a variety of sce narios just by varying the inference proce dure, without changing the model, thereby comparing the scenarios directly. We com pare using only monolingual data, using a parallel corpus, using a parallel corpus with annotations in the other language, and using small amounts of annotation in the target language. We ﬁnd that the biggest impact of adding a parallel cor pus to training is actually the increase in mono-lingual data, with the alignments to another language resulting in small im provements, even with labeled data for the other language.",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 6,
            "section_image": "1603.01514_page_0001_mask_img_6.jpg",
            "section_column": 0,
            "section_im_bbox": [
                81,
                216,
                273,
                538
            ]
        },
        {
            "section_name": "Text_360",
            "section_text": "We use the Bayesian model of Garg and Hender son (2012) as our base monolingual model. The semantic roles are predicate-speciﬁc. To model the role ordering and repetition preferences, the role inventory for each predicate is divided into Primary and Secondary roles as follows:",
            "section_annotation": "Text",
            "section_page": 2,
            "section_id": 6,
            "section_image": "1603.01514_page_0002_mask_img_6.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                551,
                288,
                638
            ]
        },
        {
            "section_name": "Text_361",
            "section_text": "for the monolingual model. This formulation models the global role ordering and repetition preferences using PRs, and limited context for SRs using intervals. Ordering and repetition informa tion was found to be helpful in supervised SRL as well (Punyakanok et al., 2004; Pradhan et al., 2005; Toutanova et al., 2008). More details, in cluding the motivations behind this model, are in (Garg and Henderson, 2012).",
            "section_annotation": "Text",
            "section_page": 3,
            "section_id": 6,
            "section_image": "1603.01514_page_0003_mask_img_6.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                406,
                288,
                533
            ]
        },
        {
            "section_name": "Text_362",
            "section_text": "those m aligned arguments. Second, having one joint Bayesian model al lows us to use the same model in various semi supervised learning settings, just by ﬁxing the annotated variables during training. Section 6.6 evaluates a setting where we have some labeled data in one language (called source), while no la beled data in the second language (called target). Note that this is different from a classic annotation projection setting (e.g. (Pad´o and Lapata, 2009)), where the role labels are mapped from source con stituents to aligned target constituents.",
            "section_annotation": "Text",
            "section_page": 4,
            "section_id": 6,
            "section_image": "1603.01514_page_0004_mask_img_6.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                287,
                518,
                443
            ]
        },
        {
            "section_name": "Section-header_033",
            "section_text": "\n## 6 Experiments ##\n",
            "section_annotation": "Section-header",
            "section_page": 5,
            "section_id": 6,
            "section_image": "1603.01514_page_0005_mask_img_6.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                386,
                153,
                408
            ]
        },
        {
            "section_name": "Text_363",
            "section_text": "g g In the second setting (Line 2), we use the ad ditional monolingual Europarl (EP) data for train ing. We get equivalent results in English and a signiﬁcant improvement in German compared to our previous setting (Line 1). The German dataset in CoNLL is quite small and beneﬁts from the ad ditional EP training data. In contrast, the English model is already quite good due to a relatively big dataset from CoNLL, and good accuracy syntac tic parsers. Unfortunately, Titov and Klementiev (2012b) do not report results with this setting. The third setting (Line 3) gives the results of our",
            "section_annotation": "Text",
            "section_page": 6,
            "section_id": 6,
            "section_image": "1603.01514_page_0006_mask_img_6.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                87,
                518,
                241
            ]
        },
        {
            "section_name": "Section-header_034",
            "section_text": "\n## 6.7 Labeled Data in Monolingual Model ##\n",
            "section_annotation": "Section-header",
            "section_page": 7,
            "section_id": 6,
            "section_image": "1603.01514_page_0007_mask_img_6.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                602,
                263,
                623
            ]
        },
        {
            "section_name": "Text_364",
            "section_text": "Klementiev (2012a) also proposed a closely re lated Bayesian model. Grenager and Manning (2006) proposed a generative model but their pa rameter space consisted of all possible linkings of syntactic constituents and semantic roles, which made unsupervised learning difﬁcult and a sep arate language-speciﬁc rule based method had to be used to constrain this space. Other pro posed models include an iterative split-merge al gorithm (Lang and Lapata, 2011a) and a graph partitioning based approach (Lang and Lapata, 2011b). M`arquez et al. (2008) provide a good overview of the supervised SRL systems.",
            "section_annotation": "Text",
            "section_page": 8,
            "section_id": 6,
            "section_image": "1603.01514_page_0008_mask_img_6.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                56,
                518,
                238
            ]
        },
        {
            "section_name": "Section-header_035",
            "section_text": "\n## 1 Introduction ##\n",
            "section_annotation": "Section-header",
            "section_page": 1,
            "section_id": 7,
            "section_image": "1603.01514_page_0001_mask_img_7.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                541,
                153,
                558
            ]
        },
        {
            "section_name": "Section-header_036",
            "section_text": "\n## 4 Multilingual Model ##\n",
            "section_annotation": "Section-header",
            "section_page": 3,
            "section_id": 7,
            "section_image": "1603.01514_page_0003_mask_img_7.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                536,
                188,
                558
            ]
        },
        {
            "section_name": "Section-header_037",
            "section_text": "\n## 5 Inference and Training ##\n",
            "section_annotation": "Section-header",
            "section_page": 4,
            "section_id": 7,
            "section_image": "1603.01514_page_0004_mask_img_7.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                446,
                438,
                468
            ]
        },
        {
            "section_name": "Section-header_038",
            "section_text": "\n## 6.1 Evaluation ##\n",
            "section_annotation": "Section-header",
            "section_page": 5,
            "section_id": 7,
            "section_image": "1603.01514_page_0005_mask_img_7.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                407,
                148,
                428
            ]
        },
        {
            "section_name": "Text_365",
            "section_text": "The third setting (Line 3) gives the results of our multilingual model, which adds the word align ments in the EP data. Comparing with Line 2, we get non-signiﬁcant improvements in both lan guages. Titov and Klementiev (2012b) obtain an F1 score of 82.7% (PU=85.0%, CO=80.6%) for German, and 83.7% (PU=86.8%, CO=80.7%) for English. Thus, for German, our multilingual Bayesian model is able to capture the cross-lingual patterns at least as well as the external penalty term in (Titov and Klementiev, 2012b). We can not compare the English results unfortunately due to differences in argument identiﬁcation.",
            "section_annotation": "Text",
            "section_page": 6,
            "section_id": 7,
            "section_image": "1603.01514_page_0006_mask_img_7.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                234,
                518,
                409
            ]
        },
        {
            "section_name": "Text_366",
            "section_text": "We explored the improvement in the monolingual model in a semi-supervised setting. To this end, we randomly selected S% of the sentences in the CoNLL dataset as “supervised sentences” and the rest (100−S)% were kept unsupervised. Next, we clamped the role labels of the supervised sentences",
            "section_annotation": "Text",
            "section_page": 7,
            "section_id": 7,
            "section_image": "1603.01514_page_0007_mask_img_7.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                621,
                288,
                708
            ]
        },
        {
            "section_name": "Text_367",
            "section_text": "\n## 8 Conclusions ##\n",
            "section_annotation": "Text",
            "section_page": 8,
            "section_id": 7,
            "section_image": "1603.01514_page_0008_mask_img_7.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                241,
                383,
                258
            ]
        },
        {
            "section_name": "Text_368",
            "section_text": "Semantic Role Labeling (SRL) has emerged as an important task in Natural Language Processing (NLP) due to its applicability in information ex traction, question answering, and other NLP tasks. SRL is the problem of ﬁnding predicate-argument structure in a sentence, as illustrated below: [A0 Mike ] has [PRED written ][A1 a book ] (S1) Here the predicate WRITE has two arguments:",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 8,
            "section_image": "1603.01514_page_0001_mask_img_8.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                561,
                288,
                675
            ]
        },
        {
            "section_name": "Text_369",
            "section_text": "Primary Role (PR) Let there be a total of N roles (or clusters) for each predicate. Assign K of them as PRs {P1, P2, ..., PK}. Further, create 3 additional PRs: START denoting the start of the role sequence, END denoting its end, and PRED denoting the predicate. These (K + 3) PRs are not allowed to repeat in a frame and their order ing deﬁnes the global role ordering.",
            "section_annotation": "Text",
            "section_page": 2,
            "section_id": 8,
            "section_image": "1603.01514_page_0002_mask_img_8.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                636,
                288,
                748
            ]
        },
        {
            "section_name": "Text_370",
            "section_text": "The multilingual model uses word alignments be tween sentences in a parallel corpus to exploit role correspondences across languages. We make copies of the monolingual model for each lan guage and add additional crosslingual latent vari ables (CLVs) to couple the monolingual mod els, capturing crosslingual semantic role patterns. Concretely, when training on parallel sentences, whenever the head words of the arguments are aligned, we add a CLV as a parent of the two cor responding role variables. Figure 4 illustrates this model. The generative process, as explained be low, remains the same as the monolingual model for the most part, with the exception of aligned",
            "section_annotation": "Text",
            "section_page": 3,
            "section_id": 8,
            "section_image": "1603.01514_page_0003_mask_img_8.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                556,
                288,
                748
            ]
        },
        {
            "section_name": "Text_371",
            "section_text": "The inference problem consists of predicting the role labels and CLVs (the hidden variables) given the predicate, its voice, and syntactic features of all the identiﬁed arguments (the visible variables). We use a collapsed Gibbs-sampling based ap proach to generate samples for the hidden vari ables (model parameters are integrated out). The sample counts and the priors are then used to cal culate the MAP estimate of the model parameters.",
            "section_annotation": "Text",
            "section_page": 4,
            "section_id": 8,
            "section_image": "1603.01514_page_0004_mask_img_8.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                466,
                518,
                593
            ]
        },
        {
            "section_name": "Text_372",
            "section_text": "Following the setting of Titov and Klementiev (2012b), we evaluate only on the arguments that were correctly identiﬁed, as the incorrectly iden tiﬁed arguments do not have any gold semantic labels. Evaluation is done using the metric pro posed by Lang and Lapata (2011a), which has 3 components: (i) Purity (PU) measures how well an induced cluster corresponds to a single gold role, (ii) Collocation (CO) measures how well a gold role corresponds to a single induced cluster, and (iii) F1 is the harmonic mean of PU and CO. For each predicate, let N denote the total num ber of argument instances, Ci the instances in the induced cluster i, and Gj the instances having la bel j in gold annotations. PU= 1 N � i maxj|Ci∩Gj| , CO= 1 N � j maxi|Ci∩Gj| , and F1= 2·P U·CO P U+CO . The score for each predicate is weighted by the number of its argument instances, and a weighted average is computed over all the predicates.",
            "section_annotation": "Text",
            "section_page": 5,
            "section_id": 8,
            "section_image": "1603.01514_page_0005_mask_img_8.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                426,
                288,
                683
            ]
        },
        {
            "section_name": "Text_373",
            "section_text": "g We also compared monolingual and bilingual training data using a setting that emulates the stan dard supervised setup of separate training and test data sets. We train only on the EP dataset and test on the CoNLL dataset. Lines 4 and 5 of Ta ble 1 give the results. The multilingual model obtains small improvements in both languages, which conﬁrms the results from the standard un supervised setup, comparing lines 2 to 3.",
            "section_annotation": "Text",
            "section_page": 6,
            "section_id": 8,
            "section_image": "1603.01514_page_0006_mask_img_8.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                406,
                518,
                529
            ]
        },
        {
            "section_name": "Text_374",
            "section_text": "1A0 was mapped to the primary role P1, A1 to P2, and the rest were mapped to the secondary roles (S1, ..., S19) in the order of their decreasing frequency.",
            "section_annotation": "Text",
            "section_page": 7,
            "section_id": 8,
            "section_image": "1603.01514_page_0007_mask_img_8.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                709,
                288,
                748
            ]
        },
        {
            "section_name": "Text_375",
            "section_text": "We propose a Bayesian model of semantic role induction (SRI) that uses crosslingual latent vari ables to capture role alignments in parallel cor pora. The crosslingual latent variables capture cor relations between roles in different languages, and regularize the parameter estimates of the mono lingual models. Because this is a joint Bayesian model of multilingual SRI, we can apply the same model to a variety of training scenarios just by changing the inference procedure appropriately. We evaluate monolingual SRI with a large unla beled dataset, bilingual SRI with a parallel cor pus, bilingual SRI with annotations available for the source language, and monolingual SRI with a small labeled dataset. Increasing the amount of monolingual unlabeled data signiﬁcantly im proves SRI in German but not in English. Adding word alignments in parallel sentences results in small, non signiﬁcant improvements, even if there is some labeled data available in the source lan guage. This difﬁculty in showing the usefulness of parallel corpora for SRI may be due to the current assumptions about role alignments, which mean that only a small percentage of roles are aligned. Further analyses reveals that annotating small amounts of data can easily outperform the performance gains obtained by adding large unla beled dataset as well as adding parallel corpora. Future work includes training on different lan",
            "section_annotation": "Text",
            "section_page": 8,
            "section_id": 8,
            "section_image": "1603.01514_page_0008_mask_img_8.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                261,
                518,
                643
            ]
        },
        {
            "section_name": "Text_376",
            "section_text": "Secondary Role (SR) The rest of the (N − K) roles are called SRs {S1, S2, ..., SN−K}. Unlike PRs, they are not constrained to occur only once and only their ordering w.r.t. PRs is used in the probability model.",
            "section_annotation": "Text",
            "section_page": 2,
            "section_id": 9,
            "section_image": "1603.01514_page_0002_mask_img_9.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                56,
                513,
                132
            ]
        },
        {
            "section_name": "Text_377",
            "section_text": "roles which are now generated by both the mono lingual process as well as the CLV.",
            "section_annotation": "Text",
            "section_page": 3,
            "section_id": 9,
            "section_image": "1603.01514_page_0003_mask_img_9.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                406,
                518,
                441
            ]
        },
        {
            "section_name": "Text_378",
            "section_text": "culate the MAP estimate of the model parameters. For the monolingual model, the role at a given position is sampled as:",
            "section_annotation": "Text",
            "section_page": 4,
            "section_id": 9,
            "section_image": "1603.01514_page_0004_mask_img_9.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                587,
                513,
                621
            ]
        },
        {
            "section_name": "Section-header_039",
            "section_text": "\n## 6.2 Baseline ##\n",
            "section_annotation": "Section-header",
            "section_page": 5,
            "section_id": 9,
            "section_image": "1603.01514_page_0005_mask_img_9.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                686,
                138,
                703
            ]
        },
        {
            "section_name": "Text_379",
            "section_text": "p p p g These results indicate that little information can be learned about semantic roles from this parallel data setup. One possible explanation for this re sult is that the setup itself is inadequate. Given the deﬁnition of aligned arguments, only 8% of English arguments and 17% of German arguments are aligned. This plus our experiments suggest that improving the alignment model is a necessary step to making effective use of parallel data in multilin gual SRI, for example by joint modeling with SRI. We leave this exploration to future work.",
            "section_annotation": "Text",
            "section_page": 6,
            "section_id": 9,
            "section_image": "1603.01514_page_0006_mask_img_9.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                526,
                518,
                678
            ]
        },
        {
            "section_name": "Picture_010",
            "section_text": "80 82 84 86 88 90 92 0 20 40 60 80 100 F1 Score % supervised sentences Model Supervised Baseline Baseline (a) German 76 78 80 82 84 86 88 90 92 94 96 0 20 40 60 80 100 F1 Score % supervised sentences Model Supervised Baseline Baseline (b) English",
            "section_annotation": "Picture",
            "section_page": 7,
            "section_id": 9,
            "section_image": "1603.01514_page_0007_mask_img_9.jpg",
            "section_column": 1,
            "section_im_bbox": [
                311,
                301,
                527,
                533
            ]
        },
        {
            "section_name": "Text_380",
            "section_text": "Future work includes training on different lan guage pairs, on more than two languages, and with more inclusive models of role alignment.",
            "section_annotation": "Text",
            "section_page": 8,
            "section_id": 9,
            "section_image": "1603.01514_page_0008_mask_img_9.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                636,
                518,
                677
            ]
        },
        {
            "section_name": "Text_381",
            "section_text": "Here, the predicate WRITE has two arguments: ‘Mike’ as A0 or the writer, and ‘a book’ as A1 or the thing written. The labels A0 and A1 correspond to the PropBank annotations (Palmer et al., 2005).",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 10,
            "section_image": "1603.01514_page_0001_mask_img_10.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                668,
                288,
                720
            ]
        },
        {
            "section_name": "List-item_077",
            "section_text": "1. Monolingual Data Given a parallel frame with the predicate pair p1, p2, generate two separate monolingual frames as in section 3. 2. Aligned Arguments For each aligned argu ment, ﬁrst generate a crosslingual latent variable from a Chinese Restaurant Process (CRP). Then generate the two aligned roles: for aligned arguments i, j: draw a crosslingual latent variable: z ∼ CRP(αCRP p1,p2) draw role for language l1: ri ∼ Multinomial(θ align p1,p2,z,l1) draw role for language l2: rj ∼ Multinomial(θ align p1,p2,z,l2)",
            "section_annotation": "List-item",
            "section_page": 3,
            "section_id": 10,
            "section_image": "1603.01514_page_0003_mask_img_10.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                441,
                518,
                638
            ]
        },
        {
            "section_name": "Text_382",
            "section_text": "We use the same baseline as used by Lang and La pata (2011a) which has been shown to be difﬁcult to outperform. This baseline assigns a semantic",
            "section_annotation": "Text",
            "section_page": 5,
            "section_id": 10,
            "section_image": "1603.01514_page_0005_mask_img_10.jpg",
            "section_column": 0,
            "section_im_bbox": [
                61,
                701,
                288,
                748
            ]
        },
        {
            "section_name": "Section-header_040",
            "section_text": "\n## 6.6 Multilingual Training with Labeled Data for One Language ##\n",
            "section_annotation": "Section-header",
            "section_page": 6,
            "section_id": 10,
            "section_image": "1603.01514_page_0006_mask_img_10.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                681,
                513,
                718
            ]
        },
        {
            "section_name": "Caption_030",
            "section_text": "g Figure 5: F1 with a portion of the data labeled.",
            "section_annotation": "Caption",
            "section_page": 7,
            "section_id": 10,
            "section_image": "1603.01514_page_0007_mask_img_10.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                531,
                508,
                548
            ]
        },
        {
            "section_name": "Text_383",
            "section_text": "\n## Acknowledgments ##\n",
            "section_annotation": "Text",
            "section_page": 8,
            "section_id": 10,
            "section_image": "1603.01514_page_0008_mask_img_10.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                676,
                393,
                698
            ]
        },
        {
            "section_name": "Text_384",
            "section_text": "putational linguistics, 34(2):1",
            "section_annotation": "Text",
            "section_page": 9,
            "section_id": 10,
            "section_image": "1603.01514_page_0009_mask_img_10.jpg",
            "section_column": 0,
            "section_im_bbox": [
                76,
                737,
                193,
                748
            ]
        },
        {
            "section_name": "Text_385",
            "section_text": "to the PropBank annotations (Palmer et al., 2005). As the need for SRL arises in different domains and languages, the existing manually annotated",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 11,
            "section_image": "1603.01514_page_0001_mask_img_11.jpg",
            "section_column": 0,
            "section_im_bbox": [
                66,
                713,
                288,
                748
            ]
        },
        {
            "section_name": "Text_386",
            "section_text": "For example, the complete role sequence in a frame could be: (START, P3, S1, S1, PRED, P2, S5, END). The ordering is deﬁned as the sequence of PRs, (START, P3, PRED, P2, END). Each pair of consecutive PRs in an ordering is called an inter val. Thus, (P3, PRED) is an interval that contains two SRs, S1 and S1. An interval could also be empty, for instance (START, P3) contains no SRs. When we evaluate, these roles get mapped to gold roles. For instance, the PR P2 could get mapped to a core role like A0, A1, etc. or to a modiﬁer role like AM −TMP, AM −MOD, etc. Garg and Henderson (2012) reported that, in practice, PRs mostly get mapped to core roles and SRs to modiﬁer roles, which conforms to the linguistic motivations for this distinction. Figure 4 illustrates two copies of the monolin",
            "section_annotation": "Text",
            "section_page": 2,
            "section_id": 11,
            "section_image": "1603.01514_page_0002_mask_img_11.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                136,
                518,
                355
            ]
        },
        {
            "section_name": "Formula_012",
            "section_text": "\n## P(ri|r−i,f,p,vc,D−)∝P(ri,r−i,f|p,vc,D−) = � P(ri,r−i,f|θ,p,vc)P(θ|D−)dθ ##\n",
            "section_annotation": "Formula",
            "section_page": 4,
            "section_id": 11,
            "section_image": "1603.01514_page_0004_mask_img_11.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                621,
                453,
                658
            ]
        },
        {
            "section_name": "Text_387",
            "section_text": "role to a constituent based on its syntactic func tion, i.e. the dependency relation to its head. If there is a total of N clusters, (N − 1) most fre quent syntactic functions get a cluster each, and the rest are assigned to the Nth cluster.",
            "section_annotation": "Text",
            "section_page": 5,
            "section_id": 11,
            "section_image": "1603.01514_page_0005_mask_img_11.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                56,
                518,
                133
            ]
        },
        {
            "section_name": "Text_388",
            "section_text": "Another motivation for jointly modeling SRL in multiple languages is the transfer of information",
            "section_annotation": "Text",
            "section_page": 6,
            "section_id": 11,
            "section_image": "1603.01514_page_0006_mask_img_11.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                716,
                518,
                748
            ]
        },
        {
            "section_name": "Text_389",
            "section_text": "using the predeﬁned mapping from Section 6.6. Sampling was done on the unsupervised sentences as usual. We then measured the clustering perfor mance using the trained parameters.2 T th t ib ti f ti l i i",
            "section_annotation": "Text",
            "section_page": 7,
            "section_id": 11,
            "section_image": "1603.01514_page_0007_mask_img_11.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                556,
                518,
                617
            ]
        },
        {
            "section_name": "Text_390",
            "section_text": "This work was funded by the Swiss NSF grant 200021 125137 and EC FP7 grant PARLANCE.",
            "section_annotation": "Text",
            "section_page": 8,
            "section_id": 11,
            "section_image": "1603.01514_page_0008_mask_img_11.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                701,
                518,
                733
            ]
        },
        {
            "section_name": "Text_391",
            "section_text": "isen ngual",
            "section_annotation": "Text",
            "section_page": 9,
            "section_id": 11,
            "section_image": "1603.01514_page_0009_mask_img_11.jpg",
            "section_column": 1,
            "section_im_bbox": [
                493,
                61,
                513,
                83
            ]
        },
        {
            "section_name": "Text_392",
            "section_text": "James Henderson U i it f G",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 12,
            "section_image": "1603.01514_page_0001_mask_img_12.jpg",
            "section_column": 1,
            "section_im_bbox": [
                336,
                111,
                438,
                133
            ]
        },
        {
            "section_name": "Text_393",
            "section_text": "Figure 4 illustrates two copies of the monolin gual model, on either side of the crosslingual latent variables. The generative process is as follows:",
            "section_annotation": "Text",
            "section_page": 2,
            "section_id": 12,
            "section_image": "1603.01514_page_0002_mask_img_12.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                348,
                518,
                393
            ]
        },
        {
            "section_name": "Text_394",
            "section_text": "Every predicate-tuple has its own inventory of CLVs speciﬁc to that tuple. Each CLV z is a multi valued variable where each value deﬁnes a distri bution over role labels for each language (denoted by θ align p1,p2,z,l above). These distributions over la bels are trained to be peaky, so that each value c for a CLV represents a correlation between the la bels that c predicts in the two languages. For ex",
            "section_annotation": "Text",
            "section_page": 3,
            "section_id": 12,
            "section_image": "1603.01514_page_0003_mask_img_12.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                636,
                518,
                748
            ]
        },
        {
            "section_name": "Section-header_041",
            "section_text": "\n## 6.3 Closest Previous Work ##\n",
            "section_annotation": "Section-header",
            "section_page": 5,
            "section_id": 12,
            "section_image": "1603.01514_page_0005_mask_img_12.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                131,
                428,
                152
            ]
        },
        {
            "section_name": "Text_395",
            "section_text": "g p To access the contribution of partial supervision better, we constructed a “supervised baseline” as follows. For predicates seen in the supervised sen tences, a MAP estimate of the parameters was cal culated using the predeﬁned mapping. For the un seen predicates, the standard baseline was used. Fi 5 d 5b h h f i",
            "section_annotation": "Text",
            "section_page": 7,
            "section_id": 12,
            "section_image": "1603.01514_page_0007_mask_img_12.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                610,
                518,
                696
            ]
        },
        {
            "section_name": "List-item_078",
            "section_text": "1. Predicate, Voice The predicate p and its voice vc are treated as top-level visible variables. 2. Ordering (Generate PRs) Select an ordered set of PRs from a multinomial distribution. o ∼ Multinomial(θorder p,vc ) 3. Generate SRs For each interval in the ordering o, a sequence of SRs is generated as: for each interval I ∈ o: draw an indicator s ∼ Binomial(θST OP p,I,0 ) while s ̸= STOP: choose a SR r ∼ Multinomial(θSR p,I) draw an indicator s ∼ Binomial(θST OP p,I,1 ) 4. Generate Features For each PR and SR, the features for that constituent are generated inde pendently. To keep the model simple and com parable to previous unsupervised work, we only use three features: (i) dependency relation of the argument to its head, (ii) head word of the argu ment, and (iii) POS tag of the head word: for each generated role r: for each feature type f: choose a value vf ∼ Multinomial(θF p,r,f)",
            "section_annotation": "List-item",
            "section_page": 2,
            "section_id": 13,
            "section_image": "1603.01514_page_0002_mask_img_13.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                388,
                518,
                703
            ]
        },
        {
            "section_name": "Text_396",
            "section_text": "where the subscript −i refers to all the variables except at position i, D− refers to the variables in all the training instances except the current one, and θ refers to all the model parameters. The above integral has a closed form solution due to Dirichlet-multinomial conjugacy.",
            "section_annotation": "Text",
            "section_page": 4,
            "section_id": 13,
            "section_image": "1603.01514_page_0004_mask_img_13.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                661,
                518,
                748
            ]
        },
        {
            "section_name": "Text_397",
            "section_text": "k) This work is closely related to the cross-lingual unsupervised SRL work of Titov and Klemen tiev (2012b). Their model has separate mono lingual models for each language and an extra penalty term which tries to maximize P(rl2|rl1) and P(rl1|rl2) i.e. for all the aligned arguments with role label rl1 in language 1, it tries to ﬁnd a role label rl2 in language 2 such that the given proportion is maximized and vice verse. However, there is no efﬁcient way to optimize the objec tive with this penalty term and the authors used an inference method similar to annotation projec tion. Further, the method does not scale naturally to more than two languages. Their algorithm ﬁrst does monolingual inference in one language ignor ing the penalty and then does the inference in the second language taking into account the penalty term. In contrast, our model adds the latent vari ables as a part of the model itself, and not an exter nal penalty, which enables us to use the standard Bayesian learning methods such as sampling.",
            "section_annotation": "Text",
            "section_page": 5,
            "section_id": 13,
            "section_image": "1603.01514_page_0005_mask_img_13.jpg",
            "section_column": 1,
            "section_im_bbox": [
                289,
                151,
                518,
                428
            ]
        },
        {
            "section_name": "Text_398",
            "section_text": "p , Figures 5a and 5b show the performance varia",
            "section_annotation": "Text",
            "section_page": 7,
            "section_id": 13,
            "section_image": "1603.01514_page_0007_mask_img_13.jpg",
            "section_column": 1,
            "section_im_bbox": [
                306,
                689,
                518,
                713
            ]
        },
        {
            "section_name": "Picture_011",
            "section_text": "9  – n. h  al n y.  D. a 6  m n, s, e c s t 8. d c of al r r  a. t al  a. h 1  h, al C. e stein, and R. Barzilay. 2009. Multilingual part-of-speech tagging: Two unsupervised ap proaches. Journal of Artiﬁcial Intelligence Re search, 36(1):341–385. [Nivre et al.2007] J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. Kubler, S. Marinov, and E. Marsi. 2007. Maltparser: A language independent system for data-driven dependency parsing. Natural Language Engineering, 13(2):95. [Och and Ney2003] F.J. Och and H. Ney. 2003. A sys tematic comparison of various statistical alignment models. Computational linguistics, 29(1):19–51. [Pad´o and Lapata2009] S. Pad´o and M. Lapata. 2009. Cross-lingual annotation projection for semantic roles. Journal of Artiﬁcial Intelligence Research, 36(1):307–340. [Palmer et al.2005] M. Palmer, D. Gildea, and P. Kings bury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguis tics, 31(1):71–106. [Pitman2002] J. Pitman. 2002. Combinatorial stochas tic processes. Technical report, Technical Report 621, Dept. Statistics, UC Berkeley, 2002. Lecture notes for St. Flour course. [Pradhan et al.2005] S. Pradhan, K. Hacioglu, V. Krugler, W. Ward, J.H. Martin, and D. Ju rafsky. 2005. Support vector learning for semantic argument classiﬁcation. Machine Learning, 60(1):11–39. [Punyakanok et al.2004] V. Punyakanok, D. Roth, W. Yih, and D. Zimak. 2004. Semantic role label ing via integer linear programming inference. In Proceedings of the 20th international conference on Computational Linguistics, page 1346. Association for Computational Linguistics. [Snyder et al.2009] B. Snyder, T. Naseem, and R. Barzi lay. 2009. Unsupervised multilingual grammar in duction. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Lan guage Processing of the AFNLP: Volume 1-Volume 1, pages 73–81. Association for Computational Lin guistics. [Swier and Stevenson2004] R. Swier and S. Stevenson. 2004. Unsupervised semantic role labelling. In Pro ceedings of the 2004 Conference on Empirical Meth ods in Natural Language Processing, pages 95–102. [Titov and Klementiev2012a] I. Titov and A. Klemen tiev. 2012a. A bayesian approach to unsupervised semantic role induction. In Proceedings of the Con",
            "section_annotation": "Picture",
            "section_page": 9,
            "section_id": 13,
            "section_image": "1603.01514_page_0009_mask_img_13.jpg",
            "section_column": 1,
            "section_im_bbox": [
                279,
                76,
                518,
                723
            ]
        },
        {
            "section_name": "Text_399",
            "section_text": "Ja es e de so University of Geneva Switzerland s henderson@unig",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 14,
            "section_image": "1603.01514_page_0001_mask_img_14.jpg",
            "section_column": 1,
            "section_im_bbox": [
                331,
                126,
                443,
                162
            ]
        },
        {
            "section_name": "Text_400",
            "section_text": "Bayesian learning methods such as sampling. The monolingual model we use (Garg and Hen derson, 2012) also has two main advantages over Titov and Klementiev (2012b). First, the former incorporates a global role ordering probability that is missing in the latter. Secondly, the latter deﬁnes argument-keys as a tuple of four syntactic features and all the arguments having the same argument keys are assigned the same role. This kind of hard clustering is avoided in the former model where two constituents having the same set of features might get assigned different roles if they appear in different contexts.",
            "section_annotation": "Text",
            "section_page": 5,
            "section_id": 14,
            "section_image": "1603.01514_page_0005_mask_img_14.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                421,
                518,
                588
            ]
        },
        {
            "section_name": "Footnote_062",
            "section_text": "2To account for the randomness in selecting the super vised sentences, the experiment was repeated 10 times and average of the performance numbers was taken.",
            "section_annotation": "Footnote",
            "section_page": 7,
            "section_id": 14,
            "section_image": "1603.01514_page_0007_mask_img_14.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                709,
                518,
                748
            ]
        },
        {
            "section_name": "Text_401",
            "section_text": "Switzerland james.henderson@unige.ch",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 15,
            "section_image": "1603.01514_page_0001_mask_img_15.jpg",
            "section_column": 1,
            "section_im_bbox": [
                301,
                151,
                473,
                173
            ]
        },
        {
            "section_name": "List-item_079",
            "section_text": "\n## 6.4 Data ##\n",
            "section_annotation": "List-item",
            "section_page": 5,
            "section_id": 15,
            "section_image": "1603.01514_page_0005_mask_img_15.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                591,
                348,
                613
            ]
        },
        {
            "section_name": "Text_402",
            "section_text": "a become insufﬁcient to build supervised",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 16,
            "section_image": "1603.01514_page_0001_mask_img_16.jpg",
            "section_column": 1,
            "section_im_bbox": [
                331,
                196,
                513,
                206
            ]
        },
        {
            "section_name": "Text_403",
            "section_text": "Following Titov and Klementiev (2012b), we run our experiments on the English (EN) and Ger man (DE) sections of the CoNLL 2009 corpus (Hajiˇc et al., 2009), and EN-DE section of the Eu roparl corpus (Koehn, 2005). We get about 40k EN and 36k DE sentences from the CoNLL 2009 training set, and about 1.5M parallel EN-DE sen tences from Europarl. For appropriate compari son, we keep the same setting as in (Titov and Klementiev, 2012b) for automatic parses and ar",
            "section_annotation": "Text",
            "section_page": 5,
            "section_id": 16,
            "section_image": "1603.01514_page_0005_mask_img_16.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                611,
                518,
                749
            ]
        },
        {
            "section_name": "Text_404",
            "section_text": "corpora become insufﬁcient to build supervised systems. This has motivated work on unsuper vised SRL (Lang and Lapata, 2011b; Titov and Klementiev, 2012a; Garg and Henderson, 2012). Previous work has indicated that unsupervised systems could beneﬁt from the word alignment in formation in parallel text in two or more languages (Naseem et al., 2009; Snyder et al., 2009; Titov and Klementiev, 2012b). For example, consider the German translation of sentence S1: [A0Mike] hat [A1ein Buch][PREDgeschrieben] (S2)",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 17,
            "section_image": "1603.01514_page_0001_mask_img_17.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                199,
                526,
                348
            ]
        },
        {
            "section_name": "Text_405",
            "section_text": "C ation",
            "section_annotation": "Text",
            "section_page": 9,
            "section_id": 17,
            "section_image": "1603.01514_page_0009_mask_img_17.jpg",
            "section_column": 1,
            "section_im_bbox": [
                493,
                721,
                513,
                738
            ]
        },
        {
            "section_name": "Text_406",
            "section_text": "All the multinomial and binomial distributions have symmetric Dirichlet and beta priors respec tively. Figure 1a gives the probability equations",
            "section_annotation": "Text",
            "section_page": 2,
            "section_id": 18,
            "section_image": "1603.01514_page_0002_mask_img_18.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                701,
                518,
                750
            ]
        },
        {
            "section_name": "Text_407",
            "section_text": "and Klementiev, 2012b). For example, consider the German translation of sentence S1: [A0Mike] hat [A1ein Buch][PREDgeschrieben] (S2) If sentences S1 and S2 have the word alignments: Mike-Mike, written-geschrieben, and book-Buch, the system might be able to predict A1 for Buch, even if there is insufﬁcient information in the monolingual German data to learn this assign ment. Thus, in languages where the resources are sparse or not good enough, or the distributions are not informative, SRL systems could be made more accurate by using parallel data with resource rich or more amenable languages.",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 19,
            "section_image": "1603.01514_page_0001_mask_img_19.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                308,
                523,
                481
            ]
        },
        {
            "section_name": "Text_408",
            "section_text": "nguages",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 20,
            "section_image": "1603.01514_page_0001_mask_img_20.jpg",
            "section_column": 1,
            "section_im_bbox": [
                386,
                474,
                418,
                483
            ]
        },
        {
            "section_name": "Text_409",
            "section_text": "or more amenable languages. In this paper, we propose a joint Bayesian model for unsupervised semantic role induction in multiple languages. The model consists of individual Bayesian models for each language (Garg and Henderson, 2012), and crosslingual la tent variables to incorporate soft role agreement between aligned constituents. This latent vari able approach has been demonstrated to increase the performance in a multilingual unsupervised part-of-speech tagging model based on HMMs (Naseem et al., 2009). We investigate the appli cation of this approach to unsupervised SRL, pre senting the performance improvements obtained in different settings involving labeled and unla beled data, and analyzing the annotation effort re quired to obtain similar gains using labeled data. We begin by brieﬂy describing the unsupervised",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 21,
            "section_image": "1603.01514_page_0001_mask_img_21.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                476,
                518,
                698
            ]
        },
        {
            "section_name": "Text_410",
            "section_text": "q g g We begin by brieﬂy describing the unsupervised SRL pipeline and the monolingual semantic role induction model we use, and then describe our multilingual model.",
            "section_annotation": "Text",
            "section_page": 1,
            "section_id": 22,
            "section_image": "1603.01514_page_0001_mask_img_22.jpg",
            "section_column": 1,
            "section_im_bbox": [
                291,
                691,
                518,
                748
            ]
        }
    ]
}