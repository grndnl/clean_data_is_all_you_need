{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import fitz  # PyMuPDF\n",
    "from os.path import join\n",
    "from ast import literal_eval\n",
    "from dla.src.dla_pipeline_support_functions import reset_directory, list_files_with_extensions, get_filename_without_extension, find_files_recursively\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 999)\n",
    "pd.set_option(\"display.max_columns\", 999)\n",
    "pd.set_option(\"display.width\", 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory: /mnt/HDD_1/w210/clean_data_is_all_you_need/app/data/s4_json_text_output, was found.\n",
      "All contents in \"/mnt/HDD_1/w210/clean_data_is_all_you_need/app/data/s4_json_text_output\" have been deleted successfully.\n"
     ]
    }
   ],
   "source": [
    "APP_DIRECTORY = \"/mnt/HDD_1/w210/clean_data_is_all_you_need/app\"\n",
    "\n",
    "DATA_DIRECTORY = join(APP_DIRECTORY, \"data\")\n",
    "S1_INPUT_PDFS_DIR = join(DATA_DIRECTORY, \"s1_input_pdfs\")\n",
    "S2_DLA_INPUTS_DIR = join(DATA_DIRECTORY, \"s2_dla_inputs\")\n",
    "S3_OUTPUTS_DIR = join(DATA_DIRECTORY, \"s3_outputs\")\n",
    "S4_JSON_TEXT_OUTPUTS_DIR = join(DATA_DIRECTORY, \"s4_json_text_output\")\n",
    "PAGE_MASK_DIR = join(S3_OUTPUTS_DIR, \"page_masks\")\n",
    "\n",
    "reset_directory(S4_JSON_TEXT_OUTPUTS_DIR,erase_contents=True, verbose=True)\n",
    "\n",
    "for p in [S1_INPUT_PDFS_DIR,S2_DLA_INPUTS_DIR,S3_OUTPUTS_DIR,S4_JSON_TEXT_OUTPUTS_DIR, PAGE_MASK_DIR]:\n",
    "    assert os.path.exists(p), f\"Directory {p}, does not exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug, to ensure that the the contents on the individual csvs match the registry one\n",
    "def valadiate_results_csvs(page_mask_directory):\n",
    "    try:\n",
    "        agg_csvs = pd.DataFrame()\n",
    "\n",
    "        for file in list_files_with_extensions(page_mask_directory, [\"csv\"]):\n",
    "            file_name = get_filename_without_extension(file)\n",
    "\n",
    "            if file_name == \"mask_registry\":\n",
    "                mask_registry = pd.read_csv(file)\n",
    "\n",
    "            else:\n",
    "                agg_csvs = pd.concat([agg_csvs, pd.read_csv(file)], axis=0)\n",
    "\n",
    "        agg_csvs.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "        mask_registry.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "        agg_csvs.sort_values([\"document\", \"page_no\", \"mask_id\"], inplace=True)\n",
    "        mask_registry.sort_values([\"document\", \"page_no\", \"mask_id\"], inplace=True)\n",
    "\n",
    "        # Check that they are the same\n",
    "        assert len(mask_registry) == len(\n",
    "            agg_csvs\n",
    "        ), \"The aggregated content of all the individual page csvs and the mask_registry, do not have the same length\"\n",
    "        assert np.array_equal(\n",
    "            agg_csvs, mask_registry\n",
    "        ), \"The aggregated content of all the individual page csvs and the mask_registry, do not match\"\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Match error: {e}\")\n",
    "\n",
    "\n",
    "def load_mask_registry(page_mask_directory, validate_csvs=False):\n",
    "    valadiate_results_csvs(page_mask_directory)\n",
    "    mask_registry = pd.read_csv(join(page_mask_directory, \"mask_registry.csv\"))\n",
    "\n",
    "    mask_registry.sort_values([\"document\", \"page_no\", \"mask_id\"], inplace=True)\n",
    "\n",
    "    mask_registry = mask_registry[\n",
    "        [\n",
    "            \"document\",\n",
    "            \"page_no\",\n",
    "            \"mask_id\",\n",
    "            \"category\",\n",
    "            \"category_lbl\",\n",
    "            \"score\",\n",
    "            \"x0\",\n",
    "            \"x1\",\n",
    "            \"y0\",\n",
    "            \"y1\",\n",
    "            \"xcf\",\n",
    "            \"ycf\",\n",
    "            \"column\",\n",
    "            \"mask_shape\",\n",
    "            \"is_primary\",\n",
    "            \"mask_img_file_names\",\n",
    "            \"mask_file_names\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    mask_registry[\"mask_shape\"] = mask_registry[\"mask_shape\"].apply(\n",
    "        lambda var: literal_eval(var)\n",
    "    )\n",
    "\n",
    "    return mask_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40397/2677786001.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  doc_mask_registry.sort_values(by=[\"mask_id\",\"page_no\"], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_scaled_pdf(pdf_path, page_number, coords, new_dimensions):\n",
    "    \"\"\"Extract text from specified coordinates in a scaled PDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    scaled_doc = fitz.open()  # Create a new empty PDF for scaled pages\n",
    "\n",
    "    # Scale the specific page\n",
    "    page = doc.load_page(page_number)\n",
    "    new_page = scaled_doc.new_page(width=int(new_dimensions[1]), height=int(new_dimensions[0]))\n",
    "    new_page.show_pdf_page(new_page.rect, doc, page.number)\n",
    "\n",
    "    # Extract text from the scaled page\n",
    "    scaled_page = scaled_doc.load_page(0)  # As we have only one page in scaled_doc\n",
    "    extracted_text = scaled_page.get_text(\"text\", clip=fitz.Rect(coords))\n",
    "    #extracted_text = ' '.join(extracted_text.split())\n",
    "\n",
    "    # Clean up\n",
    "    doc.close()\n",
    "    scaled_doc.close()\n",
    "    return extracted_text\n",
    "\n",
    "def process_pdfs_local():\n",
    "    \n",
    "    # Look for results\n",
    "    mask_registry = load_mask_registry(PAGE_MASK_DIR, validate_csvs=True)\n",
    "    pdf_list = np.unique(mask_registry['document'].values)\n",
    "    model_support_images = list_files_with_extensions(join(S3_OUTPUTS_DIR, \"model_outputs\"), ['jpg'])  \n",
    "\n",
    "    for pdf_file in pdf_list:\n",
    "        pdf_name = get_filename_without_extension(pdf_file)\n",
    "\n",
    "        pdf_file_path = join(S1_INPUT_PDFS_DIR, pdf_file)      \n",
    "        assert os.path.exists(pdf_file_path), f\"PDF File {pdf_file_path}, Not Found\"\n",
    "\n",
    "        doc_mask_registry = mask_registry.query(f\"document=='{pdf_file}' & is_primary==True\")\n",
    "        assert len(doc_mask_registry) != 0, f\"No results found for {pdf_file}\"\n",
    "\n",
    "        doc_mask_registry.sort_values(by=[\"mask_id\",\"page_no\"], inplace=True)     \n",
    "\n",
    "        # SETUP OUTPUT DIR\n",
    "        doc_output_dir = join(S4_JSON_TEXT_OUTPUTS_DIR,pdf_name)\n",
    "        reset_directory(doc_output_dir, erase_contents=True)   \n",
    "        \n",
    "        concatenated_text = \"\"\n",
    "        json_structure = {\"paper_id\": pdf_file, \"title\": \"\", \"paper_text\": []}\n",
    "\n",
    "        for i,row in doc_mask_registry.iterrows():\n",
    "            coords = (row['x0'], row['y0'], row['x1'], row['y1'])\n",
    "            page_number = row['page_no'] - 1\n",
    "            category = row['category_lbl']\n",
    "            numbers = row['mask_shape']\n",
    "            \n",
    "            ## TEXT\n",
    "            extracted_text = extract_text_from_scaled_pdf(pdf_file_path, page_number, coords, numbers)\n",
    "\n",
    "            lines = extracted_text.split('\\n')\n",
    "            processed_lines = [line[:-1] if line.endswith('-') else line for line in lines]\n",
    "            single_line_text = ''.join(processed_lines).strip()\n",
    "            #single_line_text = textwrap.fill(single_line_text, width=80)\n",
    "\n",
    "            if category == 'title':\n",
    "                single_line_text = \"\\n## \" + single_line_text + \" ##\\n\"\n",
    "\n",
    "            concatenated_text += \"\\n\" + single_line_text\n",
    "\n",
    "            ## JSON\n",
    "            decoded_text = json.dumps(single_line_text, ensure_ascii=False)\n",
    "\n",
    "            section_dict = {\n",
    "                \"section_name\": \"\",  # Set based on your data/logic\n",
    "                \"section_text\": decoded_text,\n",
    "                \"section_annotation\": category,\n",
    "                \"section_page\": page_number + 1,\n",
    "                \"section_column\": 0,  # Set based on your data/logic\n",
    "                \"section_location\": [coords]\n",
    "            }\n",
    "\n",
    "            json_structure[\"paper_text\"].append(section_dict)\n",
    "\n",
    "            ## IMAGES\n",
    "            mask_img_f_name = row['mask_img_file_names']\n",
    "            shutil.copy2(join(PAGE_MASK_DIR, mask_img_f_name), doc_output_dir)\n",
    "\n",
    "\n",
    "        # Save TEXT FILE\n",
    "        output_file_path = join(doc_output_dir, pdf_name+\".txt\")\n",
    "        with open(output_file_path, 'w') as file:\n",
    "            file.write(concatenated_text)        \n",
    "\n",
    "        # Save JSON FILE\n",
    "        json_output = json.dumps(json_structure, indent=4)\n",
    "        json_output_file_path = join(doc_output_dir, pdf_name+\".json\")\n",
    "        with open(json_output_file_path, 'w') as json_file:\n",
    "           json_file.write(json_output)\n",
    "\n",
    "        # COPY SUPPORTING FILES\n",
    "        doc_mask_registry_path = join(doc_output_dir, f\"{pdf_name}_mask_registry.csv\")\n",
    "        doc_mask_registry.to_csv(doc_mask_registry_path, index=False)\n",
    "\n",
    "        # Useful Images\n",
    "        for im_path in model_support_images:\n",
    "            im_name = get_filename_without_extension(im_path)\n",
    "            if im_name.startswith(pdf_name) and im_name.endswith(\"_base_dla_result\"):\n",
    "                shutil.copy2(im_path, doc_output_dir)\n",
    "\n",
    "\n",
    "process_pdfs_local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p38-web",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
