{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from dla_pipeline_support_functions import load_mask_registry\n",
    "\n",
    "from transformers import (\n",
    "    LayoutLMv3FeatureExtractor,\n",
    "    LayoutLMv3Tokenizer,\n",
    "    LayoutLMv3Processor,\n",
    "    LayoutLMv3ForTokenClassification,\n",
    ")\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from PIL.JpegImagePlugin import JpegImageFile\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 999)\n",
    "pd.set_option(\"display.max_columns\", 999)\n",
    "pd.set_option(\"display.width\", 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"/data\"):\n",
    "    os.symlink(\"/user/w210/clean_data_is_all_you_need/app/data\", \"/data\")\n",
    "\n",
    "DATA_DIRECTORY = \"/data\"\n",
    "\n",
    "S1_INPUT_PDFS_DIR = join(DATA_DIRECTORY, \"s1_input_pdfs\")\n",
    "S2_DLA_INPUTS_DIR = join(DATA_DIRECTORY, \"s2_dla_inputs\")\n",
    "S3_OUTPUTS_DIR = join(DATA_DIRECTORY, \"s3_outputs\")\n",
    "S4_JSON_TEXT_OUTPUTS_DIR = join(DATA_DIRECTORY, \"s4_json_text_output\")\n",
    "PAGE_MASK_DIR = join(S3_OUTPUTS_DIR, \"page_masks\")\n",
    "\n",
    "PRETRAINED_MODEL_DIR = \"/user/w210/large_file_repo/models_pretrained\"\n",
    "MODEL_TAG = \"layoutlmv3-finetuned-DocLayNet_large_sci_23_12_02-15_50_34/checkpoint-5946\"\n",
    "MODEL_WEIGHTS = join(PRETRAINED_MODEL_DIR, MODEL_TAG)\n",
    "MODEL_PROCESSOR = join(PRETRAINED_MODEL_DIR, \"microsoft-layoutlmv3-base-processor\")\n",
    "MODEL_CATEGORIES_JSON = join(DATA_DIRECTORY, \"dla_categories_doclaynet.json\")\n",
    "\n",
    "GLOBAL_BATCH_SIZE = 1\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "for pth in [\n",
    "    DATA_DIRECTORY,\n",
    "    S1_INPUT_PDFS_DIR,\n",
    "    S2_DLA_INPUTS_DIR,\n",
    "    S3_OUTPUTS_DIR,\n",
    "    S4_JSON_TEXT_OUTPUTS_DIR,\n",
    "    PAGE_MASK_DIR,\n",
    "    MODEL_WEIGHTS,\n",
    "    MODEL_PROCESSOR,\n",
    "    MODEL_CATEGORIES_JSON,\n",
    "]:\n",
    "    assert os.path.exists(pth), f\"PATH NOT FOUND: {pth}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize box diamentions to range 0 to 1000\n",
    "def normalized_box(box, image_width, image_height):\n",
    "    return [\n",
    "        int(1000 * (box[0] / image_width)),\n",
    "        int(1000 * (box[1] / image_height)),\n",
    "        int(1000 * (box[2] / image_width)),\n",
    "        int(1000 * (box[3] / image_height)),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_registry = load_mask_registry(PAGE_MASK_DIR, validate_csvs=False)\n",
    "mask_registry[\"umask_id\"] = np.arange(len(mask_registry))\n",
    "\n",
    "page_image_registry = pd.read_csv(join(S3_OUTPUTS_DIR, \"page_images_list.csv\"))\n",
    "\n",
    "doc_text_registry = pd.read_csv(\n",
    "    join(S4_JSON_TEXT_OUTPUTS_DIR, \"text_extract.csv\")\n",
    ")\n",
    "\n",
    "doc_text_registry[\"json_path\"] = doc_text_registry.apply(\n",
    "    lambda var: var[\"pdf_file\"].replace(\".pdf\", \".json\"),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "print(mask_registry.columns)\n",
    "print(\"\")\n",
    "print(page_image_registry.columns)\n",
    "print(\"\")\n",
    "print(doc_text_registry.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_doclaylet_dataset(\n",
    "    page_image_registry: pd.DataFrame,\n",
    "    doc_text_registry: pd.DataFrame,\n",
    "    mask_registry: pd.DataFrame,\n",
    "    max_text_length: int\n",
    "):\n",
    "    dataset_dict = {\n",
    "        \"document_id\": [],\n",
    "        \"page_no\": [],\n",
    "        \"images\": [],\n",
    "        \"words\": [],\n",
    "        \"bboxes\": [],\n",
    "        \"umask_id\": [],\n",
    "        \"dummy_label\": []\n",
    "    }\n",
    "    for i, row in doc_text_registry.iterrows():\n",
    "\n",
    "        # DOCUMENT SPECIFIC VALUES ########################################\n",
    "        ###################################################################\n",
    "\n",
    "        doc_id = row[\"pdf_file\"]\n",
    "\n",
    "        doc_json_path = join(row[\"output_directory\"], row[\"json_path\"])\n",
    "        with open(doc_json_path, \"r\") as json_file:\n",
    "            doc_json = json.load(json_file)\n",
    "\n",
    "        doc_json_df = pd.DataFrame(doc_json[\"paper_text\"])\n",
    "\n",
    "        # Ensure the box is read as numbers\n",
    "        doc_json_df[\"section_im_bbox\"] = doc_json_df[\"section_im_bbox\"].apply(\n",
    "            lambda var: literal_eval(str(var))\n",
    "        )        \n",
    "        doc_json_df.sort_values(by=[\"section_page\", \"section_id\"], inplace=True)\n",
    "\n",
    "\n",
    "        # PAGE SPECIFIC VALUES ############################################\n",
    "        ###################################################################\n",
    "        doc_image_df = page_image_registry.query(f\"document=='{doc_id}'\")\n",
    "        \n",
    "        for ii, im_row in doc_image_df.iterrows():\n",
    "            page_no = im_row[\"page_no\"]\n",
    "            page_img_path = join(S2_DLA_INPUTS_DIR, im_row[\"file_name\"])\n",
    "            page_img = JpegImageFile(page_img_path)\n",
    "\n",
    "            # Dataset Doc Info\n",
    "            dataset_dict[\"document_id\"].append(doc_id)\n",
    "            dataset_dict[\"page_no\"].append(page_no)\n",
    "\n",
    "            # Dataset Images\n",
    "            dataset_dict[\"images\"].append(page_img)\n",
    "\n",
    "            # MASK SPECIFIC VALUES ########################################\n",
    "            ###############################################################\n",
    "            \n",
    "            doc_page_json_df = doc_json_df.query(f\"section_page=={page_no}\")\n",
    "            doc_page_json_df[\"mask_id\"] = doc_page_json_df[\"section_id\"]  \n",
    "\n",
    "            # Dataset Words\n",
    "            #   NOTE: We need to ensure that per page there are less tokens \n",
    "            #   generated than the maximum number of tokens allowed. The code \n",
    "            #   below, ensures that each section gets tokens.\n",
    "\n",
    "            page_text = doc_page_json_df[\"section_text\"].to_list()\n",
    "\n",
    "            no_masks = len(page_text)\n",
    "            words_per_mask = int((max_text_length / no_masks) * 0.60)\n",
    "\n",
    "            short_page_text = []\n",
    "            for section_text in page_text:\n",
    "                short_txt = section_text.split(\" \")[0:words_per_mask]\n",
    "                short_txt = \" \".join(short_txt)\n",
    "                short_page_text.append(short_txt)\n",
    "\n",
    "            dataset_dict[\"words\"].append(short_page_text)\n",
    "\n",
    "            # Dataset bboxes\n",
    "            bboxes = doc_page_json_df[\"section_im_bbox\"].to_list()\n",
    "            dataset_dict[\"bboxes\"].append(bboxes)    \n",
    "                    \n",
    "\n",
    "            # Unique Mask ID:\n",
    "            page_mask_df = mask_registry.query(f\"document=='{doc_id}' & page_no=={page_no}\")\n",
    "            page_mask_df = pd.merge(doc_page_json_df, page_mask_df, on=\"mask_id\", how=\"inner\")\n",
    "            umask_ids = page_mask_df['umask_id'].to_list()\n",
    "\n",
    "            assert len(short_page_text) == len(bboxes) == len(umask_ids)\n",
    "\n",
    "            dataset_dict['umask_id'].append(umask_ids)\n",
    "            dataset_dict['dummy_label'].append(np.zeros(len(umask_ids)))\n",
    "\n",
    "\n",
    "\n",
    "    return Dataset.from_dict(dataset_dict)\n",
    "\n",
    "\n",
    "dataset = generate_doclaylet_dataset(page_image_registry, doc_text_registry, mask_registry, max_text_length=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MODEL_CATEGORIES_JSON, \"r\") as json_file:\n",
    "    categories_dict = json.load(json_file)\n",
    "\n",
    "categories_dict\n",
    "\n",
    "id2label = {int(k): v for k, v in categories_dict.items()}\n",
    "label2id = {v: int(k) for k, v in categories_dict.items()}\n",
    "\n",
    "print(id2label)\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model and Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processor = LayoutLMv3Processor.from_pretrained(MODEL_PROCESSOR, apply_ocr=False)\n",
    "model = LayoutLMv3ForTokenClassification.from_pretrained(\n",
    "    MODEL_WEIGHTS, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset for inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_examples(examples):\n",
    "    images = examples[\"images\"]\n",
    "    words = examples[\"words\"]\n",
    "    bboxes = examples[\"bboxes\"]\n",
    "    word_labels = examples[\"dummy_label\"]\n",
    "\n",
    "    unique_boxes_list = []\n",
    "    for img, boxs in zip(images, bboxes):\n",
    "        width, height = img.size\n",
    "        original_bboxes_list = [normalized_box(box, width, height) for box in boxs]\n",
    "\n",
    "        unique_boxes_list.append(original_bboxes_list)\n",
    "\n",
    "    # https://github.com/huggingface/transformers/issues/19190\n",
    "    encoding = processor(\n",
    "        images,\n",
    "        words,\n",
    "        boxes=unique_boxes_list,\n",
    "        word_labels=word_labels,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        stride=128,\n",
    "        truncation=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "    )\n",
    "    offset_mapping = encoding.pop(\"offset_mapping\")\n",
    "\n",
    "    overflow_to_sample_mapping = encoding.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    return encoding\n",
    "\n",
    "\n",
    "from datasets import Features, Sequence, ClassLabel, Value, Array2D, Array3D\n",
    "\n",
    "# we need to define custom features for `set_format` (used later on) to work properly\n",
    "features = Features(\n",
    "    {\n",
    "        \"pixel_values\": Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "        \"input_ids\": Sequence(feature=Value(dtype=\"int64\")),\n",
    "        \"attention_mask\": Sequence(Value(dtype=\"int64\")),\n",
    "        \"bbox\": Array2D(dtype=\"int64\", shape=(512, 4)),\n",
    "        \"labels\": Sequence(feature=Value(dtype=\"int64\")),\n",
    "    }\n",
    ")\n",
    "\n",
    "eval_dataset = dataset.map(\n",
    "    prepare_examples,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.features.keys(),\n",
    "    features=features,\n",
    "    batch_size=GLOBAL_BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.data.data_collator import default_data_collator\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=processor,\n",
    "    data_collator=default_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_dataset = eval_dataset\n",
    "\n",
    "predictions_full = trainer.predict(report_dataset)\n",
    "\n",
    "predictions = predictions_full.predictions.argmax(axis=2)\n",
    "labels = report_dataset[\"labels\"]\n",
    "\n",
    "filtered_predictions = [\n",
    "    [p for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "filtered_labels = [\n",
    "    [l for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "\n",
    "assert len(filtered_predictions) == len(filtered_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mask_registry_with_new_labels(mask_registry: pd.DataFrame, dataset:Dataset, filtered_predictions:list, id2label: dict):\n",
    "\n",
    "    unknown_id = -1\n",
    "\n",
    "    id2label[unknown_id] = \"Unknown\"\n",
    "\n",
    "    mask_registry[\"new_category\"] = np.full(len(mask_registry), unknown_id)\n",
    "\n",
    "   \n",
    "    umask_id_list = []\n",
    "    predictions_list = []\n",
    "\n",
    "    for um_list,p_list in zip(dataset['umask_id'], filtered_predictions):\n",
    "        assert len(um_list) == len(p_list)\n",
    "\n",
    "        for umask_id, pred in zip(um_list,p_list):\n",
    "            umask_id_list.append(umask_id)\n",
    "            predictions_list.append(pred)\n",
    "\n",
    "            matching_index = mask_registry.index[mask_registry[\"umask_id\"] == umask_id].tolist()[0]\n",
    "\n",
    "\n",
    "            mask_registry.at[matching_index, \"new_category\"] = pred\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "update_mask_registry_with_new_labels(mask_registry, dataset, filtered_predictions, id2label)\n",
    "\n",
    "mask_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(filtered_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "layoutlmv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
